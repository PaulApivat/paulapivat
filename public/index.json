[{"authors":["admin"],"categories":null,"content":"Enabling capital, coordination and culture on Web3 through on- and off-chain data.\nCurrent\nCore contributor at Bankless DAO ( POAPs). We\u0026rsquo;re creating infrastructure with DAO Dash to enable data insights for Guilds and Projects at Bankless DAO. I\u0026rsquo;m also part of the Bounty Board team working to coordinate talent and capital.\nI work primarily in the Analytics and Developer\u0026rsquo;s Guild to support other guilds including Education, Treasury, Marketing, Writing and Operations (see our Notion).\nOn-Chain Data\n  Bankless DAO Membership Categories Dashboard  Bankless DAO Membership Over Time  Bankless DAO Treasury  Bankless DAO Treasury Start \u0026amp; End Period  $BANK Trading Volume  Staking $AAVE  Bankless DAO Community Calls POAPs Claimed Data  Bankless DAO Tipping Economy  ForeFront Tipping Economy  Learn Foundational Ethereum Topics with SQL  Grants Committee Dashboard  Off-Chain Data\n  Bankless DAO Community Growth, Season 2 Launch  DAOs: The New Coordination Frontier (Gitcoin \u0026amp; BanklessDAO)  Bankless DAO Twitter Analytics  Exploring Discord Data: Bankless DAO Engagement  Bankless DAO Onboard Survey Pt 1  Bankless DAO Onboard Survey Pt 2  I also enjoy visualizing data.\nPast writings can be found on Medium, Quora, dev.to, Hashnode and Notion.\nCheck out my NFTs\nFind me on Twitter.\n","date":1620691200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1620691200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/paul-apivat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/paul-apivat/","section":"authors","summary":"Enabling capital, coordination and culture on Web3 through on- and off-chain data.\nCurrent\nCore contributor at Bankless DAO ( POAPs). We\u0026rsquo;re creating infrastructure with DAO Dash to enable data insights for Guilds and Projects at Bankless DAO.","tags":null,"title":"Paul Apivat","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1461110400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1555459200,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Courses Overview","type":"docs"},{"authors":null,"categories":null,"content":"Flexibility Add Technical Notes.\nThis feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"c432be3f6e1e5e026e053659322b73ca","permalink":"/technical_notes/example_tech/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/technical_notes/example_tech/","section":"technical_notes","summary":"Learn how to use Academic's docs layout for publishing technical notes and tutorials.","tags":null,"title":"Technical Notes Overview","type":"docs"},{"authors":null,"categories":null,"content":"Overview FIRST steps for any project are:\n Go to the directory\u0026rsquo;s root and create .gitignore file. Add .env (dot env) to .gitignore to hide from GitHub. note: Use one .env file per project, you can store multiple API Keys, passwords and other sensitive info. Best practice to store .env on same root directory as .gitignore.  Setting up dotenv Files Use Case: We have sensitive API Keys and Secret Keys we don\u0026rsquo;t want to put into version control, but we need to access.\nThe best practice is to create a .env file at the root of your project and store the keys in there, and most importantly, making sure to include .env in a .gitignore file so it does not get included in versioning. source\nThe challenge is to install the dotenv module to access the load_dotenv function in order to access the data in the environment variable (.env).\nFirst, here\u0026rsquo;s how the structure of the directory could look like:\n. ├── .env └── settings.py └── .gitignore └── project_directory_1 └── file.py └── project_directory_2 └── another_file.py  The key things to pay attention here are .env, your_python_script.py and .gitignore.\nFor the present hypothetical project, here\u0026rsquo;s what needs to be in the .gitignore file:\n#.gitignore # Environents .env  Here\u0026rsquo;s what needs to be in .env. The example here is accessing Twitter API. The first time you\u0026rsquo;re setting this up, I\u0026rsquo;d recommend using a \u0026ldquo;fake\u0026rdquo; CONSUMER_KEY (aka API Key) and CONSUMER_SECRET (API Secret Key) just to test it out and make sure that it doesn\u0026rsquo;t get inadvertently added to version control (note: see setting up .gitignore above).\n# you'll put your actual API Keys and Secrete Keys here TWITTER_CONSUMER_KEY=fakeconsumerkey TWITTER_CONSUMER_SECRET=fakeconsumersecret  Within the your_python_script.py file, you\u0026rsquo;ll have:\nfrom dotenv import load_dotenv #for python-dotenv method load_dotenv() #for python-dotenv method import os user_name = os.environ.get('USER') password = os.environ.get('password') print(user_name, password) # output username password  From there you\u0026rsquo;ll go into file.py and use the dotenv module to access environment variables (sensitive API Keys).\nThe example below accesses Twitter API.\n#file.py # for python-dotenv method of access environment variables from twython import Twython import webbrowser import os import dotenv from dotenv import load_dotenv load_dotenv() ###### TWITTER API ###### # IMPORTANT: PLUG YOUR KEY AND SECRET IN DIRECTLY (without os.environ.get()) CONSUMER_KEY = os.environ.get(\u0026quot;TWITTER_CONSUMER_KEY\u0026quot;) # API Key CONSUMER_SECRET = os.environ.get(\u0026quot;TWITTER_CONSUMER_SECRET\u0026quot;) # API Secret Key  Assuming everything is setup properly, you should be able to run the following code within IPython and have CONSUMER_KEY return a string:\nimport os import dotenv from dotenv import load_dotenv load_dotenv() # True CONSUMER_KEY = os.environ.get(\u0026quot;TWITTER_CONSUMER_KEY\u0026quot;) CONSUMER_KEY # 'fakeconsumerkey'  Installing dotenv module This is the challenging part. I had installed dotenv but was unable to access it within settings.py and file.py.\nFirst, because we\u0026rsquo;re using Python3, anything that involves pip install should be pip3 install.\nI had to uninstall, then re-install dotenv, and what worked was following this Stack Overflow answer:\n# update Nov 17, 2021, this works $ pip3 install -U python-dotenv # old installation conda install -c conda-forge python-dotenv  Sometimes having both dotenv and python-dotenv install can cause conflict. In which case, in your virtual environment, try below. Based on this answer.\n# update Nov 17, 2021 - use python-dotenv method pip3 uninstall dotenv pip3 uninstall python-dotenv pip3 install python-dotenv  You\u0026rsquo;ll know you have something working when you run the following code in the command line:\ndotenv --version # dotenv, version 0.19.2 (Nov 17, 2021)  ","date":1637103600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637103600,"objectID":"91d021e00299ab974a90a6847479f570","permalink":"/technical_notes/example_tech/python_dotenv/","publishdate":"2021-11-17T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_dotenv/","section":"technical_notes","summary":"Overview FIRST steps for any project are:\n Go to the directory\u0026rsquo;s root and create .gitignore file. Add .env (dot env) to .gitignore to hide from GitHub. note: Use one .","tags":null,"title":"Setting .env to store sensitive info without pushing to GitHub","type":"docs"},{"authors":null,"categories":null,"content":"Picking columns, rows in Pandas Note: To identify a specific cell, it\u0026rsquo;s good to have at least a unique id column to reference.\nThis part grabs the specific row: df.loc[df['id'] == number]\nThis part grabs the specific column: ['column_name']\n# generic df.loc[df['id'] == number]['column_name'] # specific concat_frames_4.loc[concat_frames_4['id'] == 1983]['choice']  For more content on Data and DAOs find me on Twitter.\n","date":1636844400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636844400,"objectID":"e756959dc2212e990695650cd46d1715","permalink":"/technical_notes/example_tech/pandas_pick_df_rows_columns/","publishdate":"2021-11-14T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/pandas_pick_df_rows_columns/","section":"technical_notes","summary":"Picking columns, rows in Pandas Note: To identify a specific cell, it\u0026rsquo;s good to have at least a unique id column to reference.\nThis part grabs the specific row: df.loc[df['id'] == number]","tags":null,"title":"Picking columns, rows in Pandas","type":"docs"},{"authors":null,"categories":null,"content":"Use Python\u0026rsquo;s string interpolation to query GraphQL Situation: For this project, I had previously grabbed the latest timestamp in a data table, assigned it to a variable and now want to use it as input for a GraphQL query:\n# Run separate request to GraphQL endpoint # use max_tx_timestamp in parameter 'where: {timestamp_gte: max_tx_timestamp}' # this will return on-chain tx since latest timestamp (i.e., max_tx_timestamp) variables = {'input': max_tx_timestamp} query = f\u0026quot;\u0026quot;\u0026quot; {{ transferBanks(first: 1000, where: {{timestamp_gte:{max_tx_timestamp}}}, orderBy: timestamp, orderDirection: asc, subgraphError: allow) {{ id from_address to_address amount amount_display timestamp timestamp_display }} }} \u0026quot;\u0026quot;\u0026quot;  Then to make sure this string interpolation actually works, we need to make a post request to the GraphQL API endpoint, query it, save that query into a data frame.\n(NOTE: This requies toggling back and forth between the database client like pgAdmin and your ipython environment)\n# note: 'variables' defined above def run_query(q): request = requests.post('https://api.studio.thegraph.com/query/1121/bankv1/v0.0.5' '', json={'query': query, 'variables': variables}) if request.status_code == 200: return request.json() else: raise Exception('Query failed. return code is {}. {}'.format( request.status_code, query)) result = run_query(query) # print results print('Print Bank Subgraph Result - {}'.format(result))  For more content on data science, R, and Python find me on Twitter.\n","date":1636239600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636239600,"objectID":"e86e285571beb8bf44d500eb18d2a4df","permalink":"/technical_notes/example_tech/python_string_interpolation_graphql/","publishdate":"2021-11-07T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_string_interpolation_graphql/","section":"technical_notes","summary":"Use Python\u0026rsquo;s string interpolation to query GraphQL Situation: For this project, I had previously grabbed the latest timestamp in a data table, assigned it to a variable and now want to use it as input for a GraphQL query:","tags":null,"title":"Use string interpolation to query GraphQL","type":"docs"},{"authors":null,"categories":null,"content":"Using Python to query GraphQL Situation: We need to query GraphQL, in JSON format, to convert to dataframe.\nThere are several ways to go about this. The simplest way is to use the requests library to make HTTP requests to the API endpoint, then the json library to convert those requests into JSON format:\nimport requests import json import pandas as pd def run_query(q): request = requests.post('https://api-endpoint' '', json={'query': query}) if request.status_code == 200: return request.json() else: raise Exception('Query failed. return code is {}. {}'.format( request.status_code, query)) # basic query first query = \u0026quot;\u0026quot;\u0026quot; { transferBanks(first: 1000, orderBy: timestamp, orderDirection: asc, subgraphError: allow) { id from_address to_address amount amount_display timestamp timestamp_display } } \u0026quot;\u0026quot;\u0026quot; # returns JSON result = run_query(query)  An alternative way is to use the gql and the gql.transport.aiohttp libraries:\nfrom gql import gql, Client from gql.transport.aiohttp import AIOHTTPTransport # Select transport with url endpoint transport = AIOHTTPTransport( url=\u0026quot;https://api.studio.thegraph.com/query/1121/bankv1/v0.0.5\u0026quot;) # create GraphQL client using defined transport client = Client(transport=transport, fetch_schema_from_transport=True) # GraphQL query query = gql(\u0026quot;\u0026quot;\u0026quot; { transferBanks(first: 1000, where: {timestamp_gte: \u0026quot;1635403557\u0026quot;}, orderBy: timestamp, orderDirection: asc, subgraphError: allow) { id from_address to_address amount amount_display timestamp timestamp_display } } \u0026quot;\u0026quot;\u0026quot;) # run query on transport result = client.execute(query) print(result)  For more content on data science, R, and Python find me on Twitter.\n","date":1635807600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635807600,"objectID":"f72b44298906e6c3313a3e46807a7eb0","permalink":"/technical_notes/example_tech/python_query_graphql/","publishdate":"2021-11-02T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_query_graphql/","section":"technical_notes","summary":"Using Python to query GraphQL Situation: We need to query GraphQL, in JSON format, to convert to dataframe.\nThere are several ways to go about this. The simplest way is to use the requests library to make HTTP requests to the API endpoint, then the json library to convert those requests into JSON format:","tags":null,"title":"Using Python to query GraphQL","type":"docs"},{"authors":null,"categories":null,"content":"Using Pandas to Convert JSON to Dataframes Situation: We are pulling in JSON data and need to convert it to a dataframe to load to a relational database or for further analysis.\nIt\u0026rsquo;s likely the JSON is nested with at least two levels. Here the requests library makes an http request to an API endpoint. A function run_query(q) is written to return the request in JSON data.\nimport requests import json import pandas as pd def run_query(q): request = requests.post('https://api-endpoint' '', json={'query': query}) if request.status_code == 200: return request.json() else: raise Exception('Query failed. return code is {}. {}'.format( request.status_code, query))  Then we need to unnest the JSON into a list of dictionaries before converting into dataframe. THe flow is JSON -\u0026gt; get Items -\u0026gt; turn into List -\u0026gt; dig down into List of Dictionaries -\u0026gt; convert to dataframe:\n# returns JSON result = run_query(query) # get Items result_items = result.items() # turn into List result_list = list(result_items) # dig down into List of Dictionaries (2 levels) lst_of_dict = result_list[0][1].get('transferBanks') # convert to data frame df = pd.json_normalize(lst_of_dict)  For more content on data science, R, and Python find me on Twitter.\n","date":1635721200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635721200,"objectID":"3d04527165435e8a3a2ef854f0f71a97","permalink":"/technical_notes/example_tech/python_json_to_df/","publishdate":"2021-11-01T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_json_to_df/","section":"technical_notes","summary":"Using Pandas to Convert JSON to Dataframes Situation: We are pulling in JSON data and need to convert it to a dataframe to load to a relational database or for further analysis.","tags":null,"title":"Using Pandas to Convert JSON to Dataframes","type":"docs"},{"authors":null,"categories":null,"content":"Guide for Creating a GitHub Repo and Cloning Locally This guide can help you setup a fresh new GitHub Repo, then clone it locally to begin working in your local environment, then pushing changes up to your GitHub Repo.\nThis guide assumes you have a Text Editor like VSCode installed and that you can use the Terminal to navigate through various folders.\nCreating Repo  Sign-up for a GitHub Account Navigate to Your Profile Click on Repositories Click on green button \u0026ldquo;New\u0026rdquo; (for New Repo) In the Repository Name field, give your new repo a name*  Names should have no spaces, use underscore _ to separate words (ie., special_project)\n Description is optional\n  Default is Public (a Private Repo requires a monthly fee)\n  Tick Add a README and Choose license (if applicable, if not just go with a README)\n  A .gitignore file is recommended if your project has sensitive info associated with it that you\u0026rsquo;d like to keep OUT of GitHub\n  Click \u0026ldquo;Create Repository\u0026rdquo;\n  Cloning Locally Navigate to your newly created repository (still on GitHub) Click on the Green Button \u0026ldquo;Code\u0026rdquo; Select the HTTPS (default) option and copy the https://github.com/YourName/special_project  Once you\u0026rsquo;ve copied the https address, you\u0026rsquo;ll use the terminal in your text editor (i.e., VSCode) to navigate to a folder on your local machine (desktop or laptop) where you\u0026rsquo;ve already created a folder just for coding projects (i.e., coding_projects)\nOpen the text editor (i.e., VS Code), click on Terminal in the menu bar, \u0026ldquo;New Terminal\u0026rdquo;, this will open your terminal to have you start at a level above the Desktop. Suppose you have a folder on your Desktop that says \u0026ldquo;coding_projects\u0026rdquo; you\u0026rsquo;ll want to clone your newly-created github repo in this folder. But you have to navigate to coding_projects folder first. Here\u0026rsquo;s the command to navigate in Terminal:  cd Desktop/coding_projects\n cd means change directory; here, you\u0026rsquo;ll change directory into your Desktop, then into coding_projects folder (two steps) to clone your github repository (special_project)\n  (in Terminal), you\u0026rsquo;re now inside the coding_projects folder, you\u0026rsquo;ll run this command, pasting the https address that you copied from your GitHub Repo:\n  git clone https://github.com/YourName/special_project\nThis will initialize your repo locally. From here, try making a minor change to the README file.\n Make a small change to your README file on your Text Editor, then save the changes.\n  Back in Terminal, you\u0026rsquo;ll add the changes, then commit, then push back up to GitHub. You\u0026rsquo;ll run these commands:\n  git status (this should indicate in red that you have new changes) git add . (you\u0026rsquo;ll add those changes) git push (you\u0026rsquo;re pushing those changes to your GitHub Repo)\nNOTE: you might see a prompt to do git push --upstream origin, which happens if this is the first push you\u0026rsquo;re making to the repo.\nNow you\u0026rsquo;ve cloned your repo locally, made some changes, then pushed it back up to GitHub. Its important to note that the process described here is to allow you to track changes for your personal coding projects, NOT any code for production or development environment.\nIn those cases, the team may have it\u0026rsquo;s own protocol for version control and you\u0026rsquo;ll want to get familiar and follow those protocols.\n","date":1626822000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626822000,"objectID":"bc236934c8deb417985afa93cfe5cc5e","permalink":"/technical_notes/example_tech/github_make_repo/","publishdate":"2021-07-21T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/github_make_repo/","section":"technical_notes","summary":"Guide for Creating a GitHub Repo and Cloning Locally This guide can help you setup a fresh new GitHub Repo, then clone it locally to begin working in your local environment, then pushing changes up to your GitHub Repo.","tags":null,"title":"Creating a Github Repo","type":"docs"},{"authors":null,"categories":null,"content":"Guide for contributing to another code base by making a pull request Public Repo  Identify a project you want to contribute to on GitHub Fork that project Clone it to your local machine*  note : Do not clone in another directory that was already cloned from another repo. For example, when I cloned pytalentsolution on my local machine, I initially cloned it inside Saku directory, which was itself a clone of an already-existing-repo. The correct way was to create a new directory for pytalentsolution.\nMake a new branch   4a: cd into directory of the cloned repo 4b: use git branch to confirm you\u0026rsquo;re on the *master branch 4c: use git checkout -b name_of_new_branch 4d: use git branch to check that you\u0026rsquo;re on name_of_new_branch   Make changes\n  Push it back to your repo\n  note : You\u0026rsquo;ll get a message git push remote -- use this instead of the traditional git push\n(Go back to the forked repo in GitHub) Click the Compare \u0026amp; Pull Request button.   add a description to changes made in the pull request  Click Create pull request to open a new pull request  Private Repo 1-3. You cannot fork a private repo\n Clone directly to local machine\n  git branch -a to see all branches in the project\n  git branch to confirm you\u0026rsquo;re on master branch\n  [important] git status to check if ahead or behind (if behind, use git pull to fetch and download content from a remote repository)\n  [from git master] create a new branch use git checkout -b name_of_new_branch\n  Use git branch to make sure you\u0026rsquo;re on new branch\n  Make changes, then normal: git add, git commit -m \u0026ldquo;message\u0026rdquo;, git push\n  note : You\u0026rsquo;ll get a message git push remote -- use this instead of the traditional git push\n(Go back to the forked repo in GitHub) Click the Compare \u0026amp; Pull Request button.   Add good PR description, see here  Click Create pull request to open a new pull request  Production vs Development Ongoing projects with several contributors will generally separate the main from develop branch. Making a PR in this context is slighty different:\n  In the command line, switch to development branch git checkout develop (even if you don\u0026rsquo;t see the develop branch locally, you may just see main or a new branch you created new_branch).\n  Now that you\u0026rsquo;re starting on develop branch, do git pull origin develop to make sure that any prior changes from the develop branch is pulled in locally, and you\u0026rsquo;re up-to-date.\n  Then from develop, create a new feature branch like so: git checkout feature/new-branch. (note: feature/new-branch is a naming convention that explicitly says you\u0026rsquo;re creating a new feature branch).\n  Then make your changes or add new code.\n  Then, git push origin (in this case, origin will be develop)\n  Once you go back to github, if you see compare and pull request, make sure it is being merged into develop and not main.\n  When Feature branch goes out of sync with Development Sometimes you\u0026rsquo;ve already pushed a pull request, but the development branch goes ahead of your proposed changes. Here\u0026rsquo;s how to handle:\n Switch to a specific feature branch (no need to go back to develop): git checkout branch_name Run git pull origin develop Then git push origin  3a. If there\u0026rsquo;s an Index Error, run\n git reset --hard * git clean -df   git reset is to take the current branch and reset it to point somewhere else, also bringing the index and working tree along. Here\u0026rsquo;s a more visual explanation ( source)  If your main branch is C and you want to point your current branch somewhere else:\n- A - B - C (HEAD, main branch)  and you want to point to B, not C, then you use git reset B to move it there:\n- A - B (HEAD, main branch) # - C is still here, but there's no branch pointing to it anymore  VIM If for whatever reason you find yourself on VIM, you can escape by:\n Press esc (escape) Press : (colon) Press wq (write and quit)  Or one-line command to get out of VIM\nPress :wq! (colon, write and quit)  ","date":1626390000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626390000,"objectID":"1b75d214807339f17480086258120866","permalink":"/technical_notes/example_tech/github_make_pr/","publishdate":"2021-07-16T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/github_make_pr/","section":"technical_notes","summary":"Guide for contributing to another code base by making a pull request Public Repo  Identify a project you want to contribute to on GitHub Fork that project Clone it to your local machine*  note : Do not clone in another directory that was already cloned from another repo.","tags":null,"title":"Making a Pull Request on a GitHub Repo","type":"docs"},{"authors":null,"categories":null,"content":"Creating mock data in Python with the Faker library If you need to simulate mock data, the Faker library is a great resource. Shout out to @joke2k for maintaining this project.\nThe full code for a recent project is below, with breakdown of each section to follow. Project context: Utilizing MongoDB as our NoSQL database. I\u0026rsquo;ve created a basic document (json) and now need to replicate multiple documents to simulate data once it\u0026rsquo;s populated with data from the frontend (or a Bot for this project specifically).\nfrom json import dumps from faker import Faker import collections database = [] filename = 'testing_bounty' length = 5 fake = Faker() # fake.word(ext_word_list=) random_currencies = ['BANK', 'ETH', 'BTC'] random_guilds = [\u0026quot;Marketing Guild\u0026quot;, \u0026quot;Treasury Guild\u0026quot;, \u0026quot;Developer's Guild\u0026quot;, \u0026quot;Analytics Guild\u0026quot;, \u0026quot;Writer's Guild\u0026quot;] discord_handle = [\u0026quot;@bob#8888\u0026quot;, \u0026quot;@alice#1234\u0026quot;, \u0026quot;@carol#5555\u0026quot;, \u0026quot;@delta#2222\u0026quot;, \u0026quot;@lambda#3333\u0026quot;] bounty_status = ['Open', 'Draft', 'In-Progress', 'In-Review', 'Completed', 'Deleted'] skills = [\u0026quot;writing\u0026quot;, \u0026quot;design\u0026quot;, \u0026quot;software development\u0026quot;, \u0026quot;strategic planning\u0026quot;, \u0026quot;data analysis\u0026quot;, \u0026quot;grant writing\u0026quot;, \u0026quot;proposal development\u0026quot;, \u0026quot;team building\u0026quot;, \u0026quot;marketing\u0026quot;] for x in range(length): database.append(collections.OrderedDict([ ('season', fake.random_int(0, 10)), ('bounty', fake.sentence()), ('bountyDescription', fake.sentence()), ('doneCriteria', fake.sentence()), ('bountyReward', collections.OrderedDict([ ('currency', fake.word(ext_word_list=random_currencies)), ('amount', fake.random_int(0, 50000)) ])), # list of dictionaries ('applicableGuilds', [collections.OrderedDict( [('guildName', fake.word(ext_word_list=random_guilds))]), collections.OrderedDict([('guildName', fake.word(ext_word_list=random_guilds))])]), ('bountyCreatedBy', collections.OrderedDict([ ('isDaoMember', fake.pybool()), ('guildName', fake.word(ext_word_list=random_guilds)), ('discordHandle', fake.word(ext_word_list=discord_handle)), ('publicAddress', \u0026quot;0x2d94aa3e47d9d5024503ca8\u0026quot; + fake.pystr()) ])), ('bountyCreatedAt', fake.date_between(start_date='today', end_date='+3m')), ('bountyDueAt', fake.date_between(start_date='today', end_date='+1y')), ('bountyActivatedAt', fake.date_between( start_date='today', end_date='+6m')), ('bountyClaimedBy', collections.OrderedDict([ ('guildName', fake.word(ext_word_list=random_guilds)), ('discordHandle', fake.word(ext_word_list=discord_handle)), ('publicAddress', \u0026quot;0x2d94aa3e47d9d5024503ca8\u0026quot; + fake.pystr()) ])), ('bountyClaimedAt', fake.date_between(start_date='today', end_date='+4m')), ('bountySubmittedBy', fake.word(ext_word_list=random_guilds)), ('bountySubmittedAt', fake.date_between( start_date='today', end_date='+11m')), ('bountySubmissionLink', \u0026quot;www.\u0026quot; + fake.safe_domain_name()), # list of dictionaries ('bountyStatus', [collections.OrderedDict([ ('status', fake.word(ext_word_list=bounty_status)), ('bountyStatusTime', fake.date_between( start_date='today', end_date='+1y')) ])]), ('bountyHash', fake.md5(raw_output=False)), # list of words ('skillsRequired', [fake.word(ext_word_list=skills), fake.word(ext_word_list=skills)]) ])) with open('%s.json' % filename, 'w') as output: # turns date_between into string, circumvent json serialization output.write(dumps(database, indent=4, sort_keys=False, default=str)) print(\u0026quot;Done.\u0026quot;)  Here\u0026rsquo;s a breakdown, section-by-section:\nSince we\u0026rsquo;ll be creating mock json data, the json built-in library is imported (specifically dumps), the Faker library is the main tool and we\u0026rsquo;ll also be using OrderedDict from collections to create document objects.\nDatabase is set to an empty array which will store the json object (OrderedDict). We\u0026rsquo;ll save the file that we eventually write as testing_bounty and keep it a short length (5), while we\u0026rsquo;re still testing.\nFinally, we initialize the Faker library by calling the Faker function and setting to fake.\nfrom json import dumps from faker import Faker import collections database = [] filename = 'testing_bounty' length = 5 fake = Faker() \u0026lt;--- important  This next section save a list of words that pertinent to our project in lists. As we create mock json objects, for certain fields, we\u0026rsquo;ll want to populate from a sample of keywords that\u0026rsquo;s important for our project.\nWithout these keywords, we\u0026rsquo;ll need to rely on random words/sentences that comes with the Faker library and that may not always be appropriate for our context.\n# fake.word(ext_word_list=) random_currencies = ['BANK', 'ETH', 'BTC'] random_guilds = [\u0026quot;Marketing Guild\u0026quot;, \u0026quot;Treasury Guild\u0026quot;, \u0026quot;Developer's Guild\u0026quot;, \u0026quot;Analytics Guild\u0026quot;, \u0026quot;Writer's Guild\u0026quot;] discord_handle = [\u0026quot;@bob#8888\u0026quot;, \u0026quot;@alice#1234\u0026quot;, \u0026quot;@carol#5555\u0026quot;, \u0026quot;@delta#2222\u0026quot;, \u0026quot;@lambda#3333\u0026quot;] bounty_status = ['Open', 'Draft', 'In-Progress', 'In-Review', 'Completed', 'Deleted'] skills = [\u0026quot;writing\u0026quot;, \u0026quot;design\u0026quot;, \u0026quot;software development\u0026quot;, \u0026quot;strategic planning\u0026quot;, \u0026quot;data analysis\u0026quot;, \u0026quot;grant writing\u0026quot;, \u0026quot;proposal development\u0026quot;, \u0026quot;team building\u0026quot;, \u0026quot;marketing\u0026quot;]  This next section is where we create a number of json objects, specified by length. Everything is pushed into an collections.OrderedDict and in some cases, we have nested data.\nMost of the fields are typical key-value pairs. There are variations in the values, including:\n strings (fake.sentence(), fake.word(), fake.pystr()); you can sample from list of words created above integers (fake.random_int()); you can randomize within a range of integers dates (fake.date_between) boolean (fake.pybool) dictionaries / json object (collections.OrderedDict) list of dictionaries, enclose within [] list of strings   for x in range(length): database.append(collections.OrderedDict([ ('season', fake.random_int(0, 10)), ('bounty', fake.sentence()), ('bountyDescription', fake.sentence()), ('doneCriteria', fake.sentence()), ('bountyReward', collections.OrderedDict([ ('currency', fake.word(ext_word_list=random_currencies)), ('amount', fake.random_int(0, 50000)) ])), # list of dictionaries ('applicableGuilds', [collections.OrderedDict( [('guildName', fake.word(ext_word_list=random_guilds))]), collections.OrderedDict([('guildName', fake.word(ext_word_list=random_guilds))])]), ('bountyCreatedBy', collections.OrderedDict([ ('isDaoMember', fake.pybool()), ('guildName', fake.word(ext_word_list=random_guilds)), ('discordHandle', fake.word(ext_word_list=discord_handle)), ('publicAddress', \u0026quot;0x2d94aa3e47d9d5024503ca8\u0026quot; + fake.pystr()) ])), ('bountyCreatedAt', fake.date_between(start_date='today', end_date='+3m')), ('bountyDueAt', fake.date_between(start_date='today', end_date='+1y')), ('bountyActivatedAt', fake.date_between( start_date='today', end_date='+6m')), ('bountyClaimedBy', collections.OrderedDict([ ('guildName', fake.word(ext_word_list=random_guilds)), ('discordHandle', fake.word(ext_word_list=discord_handle)), ('publicAddress', \u0026quot;0x2d94aa3e47d9d5024503ca8\u0026quot; + fake.pystr()) ])), ('bountyClaimedAt', fake.date_between(start_date='today', end_date='+4m')), ('bountySubmittedBy', fake.word(ext_word_list=random_guilds)), ('bountySubmittedAt', fake.date_between( start_date='today', end_date='+11m')), ('bountySubmissionLink', \u0026quot;www.\u0026quot; + fake.safe_domain_name()), # list of dictionaries ('bountyStatus', [collections.OrderedDict([ ('status', fake.word(ext_word_list=bounty_status)), ('bountyStatusTime', fake.date_between( start_date='today', end_date='+1y')) ])]), ('bountyHash', fake.md5(raw_output=False)), # list of words ('skillsRequired', [fake.word(ext_word_list=skills), fake.word(ext_word_list=skills)]) ]))  Once we run our script, we\u0026rsquo;ll want to save and serialize the OrderedDict in a json file. We also want to enclose our dates in strings to avoid getting a json serialization error; this is done by setting the parameters of output.write() to default=str.\nwith open('%s.json' % filename, 'w') as output: # turns date_between into string, circumvent json serialization output.write(dumps(database, indent=4, sort_keys=False, default=str)) print(\u0026quot;Done.\u0026quot;)  ","date":1625958000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625958000,"objectID":"693a956da21aaf33e27d23b7691e88fb","permalink":"/technical_notes/example_tech/python_faker/","publishdate":"2021-07-11T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_faker/","section":"technical_notes","summary":"Creating mock data in Python with the Faker library If you need to simulate mock data, the Faker library is a great resource. Shout out to @joke2k for maintaining this project.","tags":null,"title":"Using Faker to simulate fake data with Python","type":"docs"},{"authors":null,"categories":null,"content":"Setting up Poetry for Dependency Management  Poetry is python packaging and dependency management made easy.\nPoetry Install Script $ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3  Sample Project Create Sample Project $ poetry new how-long  Creating virtual environment NOTE: This is done only once.\n$ poetry config virtualenvs.in-project true $ poetry install  Launch virtual environment $ poetry shell  Uninstall virtual environment $ poetry env remove 3.x  Existing Project  Switch to master branch (if not already on) git status git pull git checkout -b new_branch_name  NOTE: Assume that poetry config virtualenvs.in-project true has been run.\n$ poetry install $ poetry shell  Check python environment.\n","date":1608591600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608591600,"objectID":"ea04116a58a656307edcd4b75b34cb9b","permalink":"/technical_notes/example_tech/python_poetry/","publishdate":"2020-12-22T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_poetry/","section":"technical_notes","summary":"Setting up Poetry for Dependency Management  Poetry is python packaging and dependency management made easy.\nPoetry Install Script $ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3  Sample Project Create Sample Project $ poetry new how-long  Creating virtual environment NOTE: This is done only once.","tags":null,"title":"Setting up Poetry for Dependency Management","type":"docs"},{"authors":null,"categories":null,"content":"Git commit messages Context: You\u0026rsquo;ve merged a branch to master/main and you get this message. See original stackoverflow question for more context:\n Please enter a commit message to explain why this merge is necessary, especially if it merges an updated upstream into a topic branch.\n You try typing a message or hitting enter or escaping and nothing happens. This is not an error message. Git is using your default editor. Here\u0026rsquo;s what to do:\n press \u0026ldquo;i\u0026rdquo; (i for insert) write your merge message press \u0026ldquo;esc\u0026rdquo; (escape) write \u0026ldquo;:wq\u0026rdquo; (write \u0026amp; quite) then press enter  ","date":1606518000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606518000,"objectID":"c5e6368ba4ff29a918bb848c10d8393a","permalink":"/technical_notes/example_tech/git_commit_message/","publishdate":"2020-11-28T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/git_commit_message/","section":"technical_notes","summary":"Git commit messages Context: You\u0026rsquo;ve merged a branch to master/main and you get this message. See original stackoverflow question for more context:\n Please enter a commit message to explain why this merge is necessary, especially if it merges an updated upstream into a topic branch.","tags":null,"title":"Writing a git commit message","type":"docs"},{"authors":null,"categories":null,"content":"Virtual Environment Best Practices NOTE: This is from chapter 2 of Joel Grus' \u0026lsquo;Data Science from Scratch\u0026rsquo;.\nJoel\u0026rsquo;s a known opponent of notebooks and recommends operating in IPython instead.\nI was pleasantly surprised that the process of setting up a virtual environment and IPython was relatively painless. Here\u0026rsquo;s my process, taken from the book with some tweaks:\n# create a Python 3.6 environment named 'dsfs' conda create -n dsfs python=3.6 # update conda to latest version (4.9.0) conda update -n base -c defaults conda # to activate virtual environment (named it 'dsfs' to keep it simple) source activate dsfs # install pip (note: currently using Python 3.8.5) python3 get-pip.py # install IPython python3 -m pip install ipython # save IPython session # save lines 1-21 in session to file initial_ipython_session.py %save initial_ipython_session 1-21 # exit IPython ctrl + D # exit conda virtual environment conda deactivate  Pulling up a saved IPython session in VSCode note: I am using VSCode as my main python IDE outside of jupyter notebooks.\nAfter you\u0026rsquo;ve saved an IPython session (see above), you may want to pull up the .py file for further edits at a later time. To do this, you\u0026rsquo;ll need to ensure that the code command for VSCode is installed.\nAssuming you\u0026rsquo;re already in VSCode, press (I\u0026rsquo;m using macOS):\nCommand + Shift + P  Then select Shell Command: Install code in PATH. That\u0026rsquo;s it.\nTo open a previously saved IPython session in VSCode from the VSCode terminal, type:\n% code name_of_file.py  Note that this can be done from (base) or from a previously configured virtual environment session, for example:\n(base) paulapivat@Pauls-MacBook dsfs % code function_session.py (base) paulapivat@Pauls-MacBook dsfs % source activate dsfs (dsfs) paulapivat@Pauls-MacBook dsfs % code function_session.py  ","date":1603321200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603321200,"objectID":"fbd245bc481bd9e0d8985298f919118a","permalink":"/technical_notes/example_tech/python_virtualenv/","publishdate":"2020-10-22T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_virtualenv/","section":"technical_notes","summary":"Virtual Environment Best Practices NOTE: This is from chapter 2 of Joel Grus' \u0026lsquo;Data Science from Scratch\u0026rsquo;.\nJoel\u0026rsquo;s a known opponent of notebooks and recommends operating in IPython instead.\nI was pleasantly surprised that the process of setting up a virtual environment and IPython was relatively painless.","tags":null,"title":"Setting up Conda Virtual Env and IPython","type":"docs"},{"authors":null,"categories":null,"content":"Creating and Looping through List of Tuples If you come to Python from R, it\u0026rsquo;s not immediately obvious how Lists, Dictionaries, Tuples, Series, then Loops help you do the things you can do in R.\nYou can begin to connect the dots when you see that Lists of Tuples are the building blocks of DataFrames - available in both languages to handle tidy (tabular) data.\nLists Lists are ordered and mutable collection of data. Below are lists of strings and integers.\nname_list = ['paul', 'apivat', 'marvin', 'pim', 'milin'] int_list = [3,4,5,2,5,6,7,5]  Tuples Tuples, also collections, are ordered and immutable. But more related to the handling of data, tuples can be converted to DataFrames (using the Pandas library). Below, the List of Tuples (data) is converted into a DataFrame.\nimport pandas as pd data = [ ('r1', 'c1', 11, 11), ('r1', 'c2', 12, 12), ('r2', 'c1', 21, 21), ('r2', 'c2', 22, 22) ] df = pd.DataFrame(data, columns=['R_Number', 'C_Number', 'Avg', 'Std'])  Loops You can loop through lists of strings and integers.\nint_list = [3,4,5,2,5,6,7,5] for num in int_list: print(num) name_list = ['paul', 'apivat', 'marvin', 'pim', 'milin'] for name in name_list: print(name)  Looping through List of Tuples (DataFrame) Just like you can loop through any collection, you can loop through a list of tuples - which means you can loop through DataFrames.\n# Looping through column names df = pd.DataFrame(data, columns=['R_Number', 'C_Number', 'Avg', 'Std']) for col_names in df: print(col_names) # Looping through a specific column for items in df['R_Number']: print(items) # Looping through a specific row for items in df.iloc[1]: print(items)  That\u0026rsquo;s the basic connection between python fundamental data structures and for-loop operations and data science.\n","date":1600815600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600815600,"objectID":"10f37798ee6e8eb8161738f38853e3d8","permalink":"/technical_notes/example_tech/python_tip2/","publishdate":"2020-09-23T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_tip2/","section":"technical_notes","summary":"Creating and Looping through List of Tuples If you come to Python from R, it\u0026rsquo;s not immediately obvious how Lists, Dictionaries, Tuples, Series, then Loops help you do the things you can do in R.","tags":null,"title":"Creating and Looping through DataFrames","type":"docs"},{"authors":null,"categories":null,"content":"Random Numbers with Numpy Numpy has a sub-module called random. Technically both are of the \u0026lsquo;module\u0026rsquo; class. numpy.random contains other methods like: seed, set_state, standard_t etc.\n# Submodules import numpy print(\u0026quot;numpy.random is a\u0026quot;, type(numpy.random)) print(\u0026quot;numpy is a\u0026quot;, type(numpy)) print(\u0026quot;it contains names such as...\u0026quot;, dir(numpy.random)[-15:])  Reproducibility When using numpy.random, you can ensure reproducibility by accessing numpy.random.seed(30), which mirrors #Rstats' set.seed(30) behavior.\nimport random numpy.random.seed(30) rolls = numpy.random.randint(low=1, high=6, size=10) rolls  ","date":1599692400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599692400,"objectID":"684176c32c2cfcd15b0d67ffcf04af31","permalink":"/technical_notes/example_tech/python_reproducibility/","publishdate":"2020-09-10T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_reproducibility/","section":"technical_notes","summary":"Random Numbers with Numpy Numpy has a sub-module called random. Technically both are of the \u0026lsquo;module\u0026rsquo; class. numpy.random contains other methods like: seed, set_state, standard_t etc.\n# Submodules import numpy print(\u0026quot;numpy.","tags":null,"title":"Random Numbers \u0026 Reproducibility in Python","type":"docs"},{"authors":null,"categories":null,"content":"Steps for Connecting BigQuery to Data Studio This note outlines the basic steps required to generate charts in Google Data Studio, specifically pulling data from BigQuery.\nBigQuery  The starting point is to generate a query in BigQuery Once a query is created, click Save Results In the pop-up window, a prompt: \u0026ldquo;choose where to save the results data from the query\u0026rdquo;, save result as BigQuery Table Set project name (i.e., jobsbot) Set dataset name (i.e., internalmongo) Create table name, for the specific query (i.e., jobfieldname_ranking)  Google Data Studio Click Add Data Find BigQuery in Google Connectors Locate saved query table (see above) (i.e., My Projects \u0026gt; jobsbot (project) \u0026gt; internalmongo (dataset) \u0026gt; jobfieldname_ranking (table/specific query)) Click Add Select \u0026lsquo;Add a Chart\u0026rsquo; (note: could be Table or Chart style) Optional: copy/paste Table to create a companion Chart for table Select Table; in Data Menu, select Metric, \u0026lsquo;Add Metric\u0026rsquo; to swap out generic default Report Count (for more informative data generated from the query)  ","date":1599260400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599260400,"objectID":"3aadd358dc22bab05cb5eaf1acd29262","permalink":"/technical_notes/example_tech/google_cloud_tip1/","publishdate":"2020-09-05T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/google_cloud_tip1/","section":"technical_notes","summary":"Steps for Connecting BigQuery to Data Studio This note outlines the basic steps required to generate charts in Google Data Studio, specifically pulling data from BigQuery.\nBigQuery  The starting point is to generate a query in BigQuery Once a query is created, click Save Results In the pop-up window, a prompt: \u0026ldquo;choose where to save the results data from the query\u0026rdquo;, save result as BigQuery Table Set project name (i.","tags":null,"title":"Connecting BigQuery to Google Data Studio [Basic Setup]","type":"docs"},{"authors":null,"categories":null,"content":"Setting up Python for R Users I\u0026rsquo;ve recently started #66DaysOfData and will be using this opportunity to make some headway into the world of Python. It\u0026rsquo;s reputation for having a complex, at times frustrating, setup process precedes itself and is probably warranted. That said, here are some tips to minimize that.\nHere\u0026rsquo;s my current OS environment. Mac users will have an older version of Python that comes with the computer, you can type python --version into your terminal to find out. Here\u0026rsquo;s mine:\nmacOS Catalina version 10.15.5 Python 2.7.16  Python 2 vs Python 3 There appears to be general consensus for anyone starting out in Python that you\u0026rsquo;ll want Python 3. There\u0026rsquo;s no debate here. Just get Python 3. I found the easiest way to go to Python Release for Mac OS X, which as of this writing is Python 3.8.5 and use the macOS 64-bit installer.\nOnce installed, you\u0026rsquo;ll want to check.\nInstead of python --version, which checks Python 2, you\u0026rsquo;ll use python3 --version. This implies that Python 3 isn\u0026rsquo;t merely a \u0026ldquo;newer\u0026rdquo; version of Python, but that they are completely different categories.\nAnaconda While this isn\u0026rsquo;t my first choice of development environment, it is the first option that allowed me to get coding in Python the fastest.\nYou\u0026rsquo;ll download the Individual Edition of the Anaconda, open-source platform. You\u0026rsquo;ll download the application for your desktop and you\u0026rsquo;ll find Anaconda-Navigator in your list of applications (or where ever you chose to place your newly installed application).\nNOTE: Shortly after installing and using, the Desktop version of Anaconda froze and I had a difficult time even \u0026ldquo;Force Quitting\u0026rdquo; it, so my preferred method of launching Anaconda Navigator is to open the mac terminal and type in the command anaconda-navigator.\nThe navigator supports Jupyter Notebooks, PyCharm and even RStudio among other environments.\nI will be using Jupyter Notebooks while I get acclimated to Python, but ultimately i\u0026rsquo;m looking for interoperability with #Rstats.\nReticulate This is an R package that allows you to run Python code in R environments. The feature I am looking forward to using is the R Markdown document that allows me to run chunks of python code.\nWork-in-Progress: TBD\nVSCode This is another popular IDE with widely used Python Extension.\nWork-in-Progress: TBD\nPyCharm I\u0026rsquo;ve heard this IDE most closely resembles RStudio in ease of use.\nWork-in-Progress: TBD\nSpyder This appears to be close approximation of the functionality in RStudio.\nWork-in-Progress: TBD\n","date":1599174000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599174000,"objectID":"7e9e9cc140ad4f3aa593032d2899ac12","permalink":"/technical_notes/example_tech/python_tip1/","publishdate":"2020-09-04T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/python_tip1/","section":"technical_notes","summary":"Setting up Python for R Users I\u0026rsquo;ve recently started #66DaysOfData and will be using this opportunity to make some headway into the world of Python. It\u0026rsquo;s reputation for having a complex, at times frustrating, setup process precedes itself and is probably warranted.","tags":null,"title":"Python Setup Options","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTechnical Tip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTechnical Tip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"14dcb8c9da95e5d25a957b90a714d7bd","permalink":"/technical_notes/example_tech/technical_notes1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/technical_notes1/","section":"technical_notes","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTechnical Tip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat.","tags":null,"title":"Technical Notes Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Using Projection to Limit Set of Fields Retrieved Situation: You want to query a list of discordHandles or customerNames from a collection of documents (list of objects) and you don\u0026rsquo;t want every field.\nReturning only one field in a document # list only customer names without their id db.customers.find({}, {customerName: 1, _id: 0}).pretty() # list discord handles without the id db.bounties.find({}, {\u0026quot;createdBy.discordHandle\u0026quot;: 1, _id: 0}).pretty()  Using comparison operators to limit number of documents retrieved # list all bounties with reward amount greater than 100 db.bounties.find({\u0026quot;reward.amount\u0026quot;: {$gt: 100}}).pretty() # list all bounties with reward less than 100 db.bounties.find({\u0026quot;reward.amount\u0026quot;: {$lt: 100}}).pretty()  For more content on data science, R, and Python find me on Twitter.\n","date":1637622000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637622000,"objectID":"00fceb8633d04105df3e9367ef970bb8","permalink":"/technical_notes/example_tech/mongodb_data_retrieval/","publishdate":"2021-11-23T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/mongodb_data_retrieval/","section":"technical_notes","summary":"Using Projection to Limit Set of Fields Retrieved Situation: You want to query a list of discordHandles or customerNames from a collection of documents (list of objects) and you don\u0026rsquo;t want every field.","tags":null,"title":"Retrieving data in MongoDB","type":"docs"},{"authors":null,"categories":null,"content":"Comparison Operators with GraphQL Situation: When querying a GraphQL endpoint, you might need to query from the latest timestamp. You\u0026rsquo;ll need a comparison operator for that.\nHere\u0026rsquo;s are the operators, taken from this stackoverflow answer.\nnote: You can add these operators to any key, for example if the column name was \u0026ldquo;created\u0026rdquo; (i.e., created at 123456789 timestamp), a query might include created_gt to mean \u0026ldquo;timestamp greater than\u0026rdquo;:\n# sample query votes(first: 1000, where: {created_gt: 123456789}) { id vote_id voter created proposal { id } } # Other comparison operators _gt (greater than) _lt (less than) _gte (greater than or equal to) _lte (less than or equal to) _in (equal to) _not_in (not equal to)  For more content on data science, R, and Python find me on Twitter.\n","date":1637103600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637103600,"objectID":"c62c62b0863afcd440eadd0eeec62a8e","permalink":"/technical_notes/example_tech/graphql_comparison_operators/","publishdate":"2021-11-17T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/graphql_comparison_operators/","section":"technical_notes","summary":"Comparison Operators with GraphQL Situation: When querying a GraphQL endpoint, you might need to query from the latest timestamp. You\u0026rsquo;ll need a comparison operator for that.\nHere\u0026rsquo;s are the operators, taken from this stackoverflow answer.","tags":null,"title":"Comparison Operators with GraphQL","type":"docs"},{"authors":null,"categories":null,"content":"Create, Read, Update, Delete in Mongo (Shell) Situation: This is a summary of basic Mongo operations in shell. These commands can be used in create, seed test databases outside of the production database.\nBasic commands # show available databases show dbs # use a database use db_name # show collections within database show collections  Delete an entire collection db.collectionName.drop()  Insert one document into a collection Create a collection and insert one document. Copy a document (i.e., an object or python dictionary) and paste in argument of .insertOne(). What\u0026rsquo;s inserted is a single object.\ndb.newCollectionName.insertOne({})  note: MongoServerError: _id fields may not contain '$'-prefixed fields: $oid is not valid for storage. Because mongo shell automatically inserted ids:\nExample of two ObjectIds being inserted:\n{ acknowledged: true, insertedIds: { '0': ObjectId(\u0026quot;618d1fb2f5975b1a2ed10b91\u0026quot;), '1': ObjectId(\u0026quot;618d1fb2f5975b1a2ed10b92\u0026quot;) } }  Insert many documents into a collection note: to avoid MongoServerError with '$'oid is not valid for storage error, edit in VSCode before pasting in mongo shell.\nNote, parameter is an array of objects.\ndb.newCollectionName.insertMany([{}, {}])  Count document(s) inside a collection db.collection.countDocuments()  Find document where integer value \u0026gt; 9000 Using $gt (greater than) as an example from the Bounty Board project.\nAlternatives:\n $lt less than $gte greater than or equal to  db.bounties2.find({\u0026quot;reward.amount\u0026quot;: {$gt: 9000} }).pretty()  Update one document Updating season from 1 to 2:\ndb.bounties2.updateOne({_id: ObjectId(\u0026quot;618d2585f5975b1a2ed10b93\u0026quot;)}, {$set: {season: 2}})  ReplaceOne instead update Note: This is more tedious than updating just one field; with replaceOne() you have to update all fields otherwise, they get deleted. Arguably, this makes it more explicit (and safe) way to update.\nThis replaces key-value pairs in the document with a specific _id to only have 2 key-value pairs (erasing all others).\ndb.bounties2.replaceOne({_id: ObjectId(\u0026quot;618d1cddf5975b1a2ed10b8f\u0026quot;)}, {season: 2, \u0026quot;title\u0026quot;: \u0026quot;Implement Changes\u0026quot;})  Print more than just 20 documents bountyboard\u0026gt; db.bounties2.find().forEach((bounties2Data) =\u0026gt; {printjson(bounties2Data)})  Return only one field in a document Here we\u0026rsquo;re returning \u0026ldquo;title\u0026rdquo;, then \u0026ldquo;reward.amount\u0026rdquo;.\n# return title db.bounties2.find({}, {title: 1, _id: 0}).pretty() # return reward.amount db.bounties2.find({}, {\u0026quot;reward.amount\u0026quot;: 1, _id: 0}).pretty()  Joining two collections Note: Join two collections by customerId\ndb.bounties2.aggregate([{ $lookup: { from: \u0026quot;customers\u0026quot;, localField: \u0026quot;customerId\u0026quot;, foreignField: \u0026quot;customerId\u0026quot;, as: \u0026quot;customerId\u0026quot; } }])  For more content on data science, R, and Python find me on Twitter.\n","date":1636585200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636585200,"objectID":"326e483c17a59181d4cce71aef75d7a4","permalink":"/technical_notes/example_tech/mongodb_crud/","publishdate":"2021-11-11T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/mongodb_crud/","section":"technical_notes","summary":"Create, Read, Update, Delete in Mongo (Shell) Situation: This is a summary of basic Mongo operations in shell. These commands can be used in create, seed test databases outside of the production database.","tags":null,"title":"CRUD Operations in Mongo (Shell)","type":"docs"},{"authors":null,"categories":null,"content":"Create a Test Table then Insert data to establish a connection Situation: You want to create a quick \u0026lsquo;test\u0026rsquo; table via the sqlalchemy library in Python to establish a connection with your postgres db.\n# Create TEST table to confirm connection db.execute( \u0026quot;CREATE TABLE IF NOT EXISTS films (title text, director text, year text)\u0026quot;) # Insert data db.execute( \u0026quot;INSERT INTO films (title, director, year) VALUES ('Dune', 'Denis Villeneuve', '2021')\u0026quot;) # Read data result_set = db.execute(\u0026quot;SELECT * FROM films\u0026quot;) for r in result_set: print(r)  For more content on data science, R, and Python find me on Twitter.\n","date":1636412400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636412400,"objectID":"6656b9ba157009e50d33d6637fe911c0","permalink":"/technical_notes/example_tech/postgresql_create_table/","publishdate":"2021-11-09T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/postgresql_create_table/","section":"technical_notes","summary":"Create a Test Table then Insert data to establish a connection Situation: You want to create a quick \u0026lsquo;test\u0026rsquo; table via the sqlalchemy library in Python to establish a connection with your postgres db.","tags":null,"title":"Create a table to established connection with SQLAlchemy","type":"docs"},{"authors":null,"categories":null,"content":"Setting up Docker locally Situation: We need have an separate frontend-backend work environment instead of spinning up a test database in our actual database.\nWe need to separate test environment from dev/prod environment.\nFor demonstration, I\u0026rsquo;m pulling a docker container into my local machine to test a database for the bounty board project.\nEnter Docker. Assuming a dev has already setup the docker container in a specific feature branch feature/docker-compose-mongo.\n# clone repo that contains docker container (bountyboard-docker directory is installed locally) git clone https://github.com/jordaniza/bounty-board.git bountyboard-docker # change directory into that directory's root cd bountyboard-docker # create new feature branch to match branch you want to pull locally (containing the docker container) git checkout -b feature/docker-compose-mongo # pull container to your local environment git pull origin feature/docker-compose-mongo # change directory to folder with \u0026quot;Dockerfile\u0026quot; cd mongo # run command to start Docker up docker-compose up --build # start up (frontend) App # install first yarn \u0026amp;\u0026amp; yarn dev ## OPEN LOCALHOST:3000 http://localhost:3000  ENVIRONMENT VARIABLES We have to change directory into packages/react-app/.env.local to create the .env.local file (changed for security)\n# terminal $ code .env.local # paste this into the newly opened file BUILD_ENV=development MONGODB_DB=bountyboard #PROD_MONGODB_URI= MONGODB_URI=mongodb://localhost:27017/bountyboard NEXT_PUBLIC_DISCORD_SERVER_ID=8******************0 NEXT_PUBLIC_DISCORD_CHANNEL_BOUNTY_BOARD_ID=8******************0 # Public Environments NEXT_PUBLIC_DAO_CURRENT_SEASON=1 NEXT_PUBLIC_DAO_CURRENT_SEASON_END_DATE=2021-08-31T04:00:00.000Z # URLs NEXT_PUBLIC_DAO_BOUNTY_BOARD_URL=https://bountyboard.bankless.community DISCORD_BOUNTY_BOARD_WEBHOOK=  What happens when you pull a Docker container locally We are:\n Pulling a folder for MongoDB with the latest scripts, schema, and Dockerfiles Using the docker-compose utility, docker-compose up --build to fire up a Mongo container, and a temporary seeding container The seeding container runs a set of bash scripts to populate the DB you can see this in mongo/seed.sh  Opening Additional Terminal windows: Mongo Once, I ran yarn dev I got the Application to fire up, ready - started server on 0.0.0.0:3000, url: http://localhost:3000\nbut Mongo wasn\u0026rsquo;t turned on.\nFirst, open up a new terminal, then:\ndocker exec -it mongo_mongo_1 bash  NOTE: Install DOCKER in VSCode to follow along\nFind the Container that\u0026rsquo;s a GREEN TRIANGLE, right click, then attach shell.\nNOTE: This did not work, so I needed to open a 3rd Terminal to type in:\ndocker exec -it mongo_mongo_1 bash  exec executes a command -it starts an interactive terminal session mongo_mongo_1 is the name of the container and bash is the shell.\nNOTE: You should see something like root@4d5cedd1a8a7:/#, then type in the following to fire up Mongo Shell:\nmongosh  Another confirmation is Connecting to: mongodb://127.0.0.1:27017/?directConnection=true\u0026amp;serverSelectionTimeoutMS=2000\nAt this point you can do basic Mongo commands:\nshow collections  NOTE: I was on the test\u0026gt; database and needed to switch to bountyboard\u0026gt; database where the \u0026ldquo;test\u0026rdquo; data was located and ready for testing.\nOther commands to run are findOne() or find() just to see the seeded data:\ndb.bounties.findOne()  NOTE: I did not get the front end to work so for this session, we only tested the database by querying in Mongo shell.\nFor more content on data science, R, and Python find me on Twitter.\n","date":1636412400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636412400,"objectID":"37d66f479616b2c6bb5d301d396aef3d","permalink":"/technical_notes/example_tech/database_install_docker/","publishdate":"2021-11-09T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/database_install_docker/","section":"technical_notes","summary":"Setting up Docker locally Situation: We need have an separate frontend-backend work environment instead of spinning up a test database in our actual database.\nWe need to separate test environment from dev/prod environment.","tags":null,"title":"Testing databases in a Docker Container","type":"docs"},{"authors":null,"categories":null,"content":"Delete a table Situation: Sometimes when testing a pipeline, you mess up a table (e.g., append the wrong index), you just need to delete and start over.\nTRUNCATE TABLE public.name_of_table;  For more content on Data and DAOs find me on Twitter.\n","date":1636239600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636239600,"objectID":"7230c732d2a22b96a7f9a8e51ec2f47e","permalink":"/technical_notes/example_tech/postgresql_delete_table/","publishdate":"2021-11-07T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/postgresql_delete_table/","section":"technical_notes","summary":"Delete a table Situation: Sometimes when testing a pipeline, you mess up a table (e.g., append the wrong index), you just need to delete and start over.\nTRUNCATE TABLE public.name_of_table;  For more content on Data and DAOs find me on Twitter.","tags":null,"title":"Delete a table","type":"docs"},{"authors":null,"categories":null,"content":"Copy existing table to a new table Situation: Useful to create \u0026lsquo;tests\u0026rsquo; tables while testing a new data pipeline. You can preserve the original in case you need to Write to or Update a table.\nCREATE TABLE new_table AS TABLE original_table  For more content on data science, R, and Python find me on Twitter.\n","date":1636239600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636239600,"objectID":"24d848e8f4f9c288132270d6df2ef7cd","permalink":"/technical_notes/example_tech/postgresql_copy_to_new_table/","publishdate":"2021-11-07T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/postgresql_copy_to_new_table/","section":"technical_notes","summary":"Copy existing table to a new table Situation: Useful to create \u0026lsquo;tests\u0026rsquo; tables while testing a new data pipeline. You can preserve the original in case you need to Write to or Update a table.","tags":null,"title":"Copy to new table","type":"docs"},{"authors":null,"categories":null,"content":"Connecting to postgresql database with python Situation: You need to establish connection to an existing table in your postgresql database in order to build a data pipeline into it.\nI use sqlalchemy to work with existing tables in postgresql. In this project, I connected to a GraphQL API endpoint with requests and the json library is needed to work with JSON and pandas for dataframes.\nimport sqlalchemy from sqlalchemy import create_engine from sqlalchemy import text import requests import json import pandas as pd from pprint import pprint  Making a connection With sqlalchemy we use the create_engine() function. Here we\u0026rsquo;re reading a table and doing some data manipulation:\ndb_string = 'postgresql://user:password@localhost:port/mydatabase' db = create_engine(db_string) # once a database connection is established, we can select pieces of data we want from a table: # Query existing postgres table: stg_subgraph_bank # read from stg_subgraph_bank to select MAX (tx_timestamp) # then, set to variable max_tx_timestamp with db.connect() as conn: result = conn.execute( text(\u0026quot;SELECT MAX(tx_timestamp) AS max_tx_timestamp, MAX(id) AS max_id FROM stg_subgraph_bank_1\u0026quot;)) for row in result: max_tx_timestamp = row.max_tx_timestamp max_id = row.max_id print(\u0026quot;new max_tx_timestamp: \u0026quot;, max_tx_timestamp) print(\u0026quot;new max_id: \u0026quot;, max_id)  For more content on Data and DAOs find me on Twitter.\n","date":1636239600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636239600,"objectID":"f6a51eeceaa5119cb620eaae9aed53fa","permalink":"/technical_notes/example_tech/postgresql_connecting_sqlalchemy/","publishdate":"2021-11-07T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/postgresql_connecting_sqlalchemy/","section":"technical_notes","summary":"Connecting to postgresql database with python Situation: You need to establish connection to an existing table in your postgresql database in order to build a data pipeline into it.\nI use sqlalchemy to work with existing tables in postgresql.","tags":null,"title":"Connecting to postgresql database","type":"docs"},{"authors":null,"categories":null,"content":"Delete a table Situation: If you have a CSV file, you can manually load it into PostgreSQL through the pgAdmin client. This may require creating the data values in Excel first.\nFirst, you\u0026rsquo;d have to create a table with the right columns in pgAdmin before inserting data in.\nTo create in Excel, you\u0026rsquo;d need a function to copy the values from the dataframe into a tuple of string values. Data is cut short to keep the example manageable.\nNote: There needs to be a comma between columns below or else it throws an error. This example is for a table with 7 columns; your situation may differ.\n# In Excel =CONCATENATE(\u0026quot;('\u0026quot;,B2,\u0026quot;','\u0026quot;,C2,\u0026quot;','\u0026quot;,D2,\u0026quot;','\u0026quot;,E2,\u0026quot;','\u0026quot;,F2,\u0026quot;','\u0026quot;,G2,\u0026quot;','\u0026quot;,H2,\u0026quot;'),\u0026quot;) # sample tuple format ('0x7a250d5630b4cf539739df2c5dacb4c659f2488d','14897.1883870177','0x59c1349bc6f28a427e78ddb6130ec669c2f39b48','0x0f433138b2a8f2997ef387ffcebec7cd204ab2053c43f8d4a6efaa74eddc0e0c-23','1620159318','Tue, 04 May 2021 20:15:18 GMT'),  Once the data is prepped in Excel, you can manully paste into pgAdmin (note: can be error prone with 20,000+ rows). Data is cut short to keep the example manageable.\nINSERT INTO public.table_to_insert( amount_display, from_address, id, timestamp, to_address) VALUES ('14897.1883870177','0x59c1349bc6f28a427e78ddb6130ec669c2f39b48','0x0f433138b2a8f2997ef387ffcebec7cd204ab2053c43f8d4a6efaa74eddc0e0c-23','1620159318','Tue, 04 May 2021 20:15:18 GMT');  Note: this is similar to how it\u0026rsquo;s done using the sqlalchemy library in python.\n# Sending Multiple Parameters with engine.connect() as conn: conn.execute( text(\u0026quot;INSERT INTO some_table (x, y) VALUES (:x, :y)\u0026quot;), [{\u0026quot;x\u0026quot;: 11, \u0026quot;y\u0026quot;: 12}, {\u0026quot;x\u0026quot;: 13, \u0026quot;y\u0026quot;: 14}] ) conn.commit()  Demo Situation: You have to CREATE TABLE before you INSERT INTO. Here\u0026rsquo;s a full example of creating a table in Postgres. The code is truncated to save time.\nIncidentally, you have to delete the table if you mistakenly created it in Postgres (use SQL commands to Create the table.)\nNOTE: PGAdmin is the client, but this should transfer across client.\n# delete table (if needed) DROP TABLE bankless_snapshot_header_1; # create table before insert in postgres CREATE TABLE IF NOT EXISTS bankless_snapshot_header_1 ( id SERIAL, proposal_id VARCHAR(100), title VARCHAR(2000), start_date BIGINT, end_date BIGINT, PRIMARY KEY (proposal_id) ) # insert data (copied from csv) to postgres INSERT INTO bankless_snapshot_header_1( id, proposal_id, title, start_date, end_date) VALUES ('0','QmdoixPMMT76vSt6ewkE87JZJywS1piYsGC3nJJpcrPXKS','Approve the Bankless DAO Genesis¬†Proposal?','1620154800','1620414000'), ('1','QmbCCAH3WbAFJS4FAUTvpWGmtgbaeMh1zoKgocdt3ZJmdr','What charity should CMS Holdings donate 100k towards? ','1620327600','1620673200'), ('2','QmYvsZ7AU2XyhpBL8g4QRQbLRX6uU3t5CHNdFQbs5r7ypJ','Badge Distribution for Second Airdrop','1620759600','1621018800'), ('3','QmQX2DQcDTZzCpM6DTVNJutQJwWXtxJDTMpBoFjbnaM9i2','Reward Season 0 Active Members ','1623196800','1623456000'), ('4','QmXrfAHMoRcu5Vy3DsRTfokqLBTEKR6tqKVecLvkgw5NZf','Bankless DAO Season 1 ','1623985200','1624590000');  Demo 2 Context: For the Snapshot data pipeline, I had to create two pipes - one for proposals and one for votes. This is the process for votes, it\u0026rsquo;s similar, but there are differences:\nI initially set FOREIGN KEY (proposal_id), but got a syntax error, there\u0026rsquo;s a specific way to set up foreign key constraints first before explicitly define foreign key during CREATE TABLE events.\nAlso, some rows at non-explicit null values (''), so I had to manually go line-by-line to set to NULL.\n# Create Table in Postgresql CREATE TABLE IF NOT EXISTS stg_bankless_snapshot_1( id SERIAL, vote_id VARCHAR(100), voter VARCHAR(100), created BIGINT, choice REAL, __typename VARCHAR(20), proposal_id VARCHAR(100) ) # Insert data INSERT INTO stg_bankless_snapshot_1( id, vote_id, voter, created, choice, __typename, proposal_id) VALUES ('0','QmQFvHkah7w2qAcY4iECn6THDbaypto8JVF5G6YQaneZRV','0xD00dF71434Cf40b2CDb65ff73bD9789933adA44A','1620413879','1','Vote','QmdoixPMMT76vSt6ewkE87JZJywS1piYsGC3nJJpcrPXKS'), ('1','QmSS2x2xBRwTigXR5vucVp75FqCP5ns3CLYK3dLgNQonkC','0x910176D294AFA2cD017928cA92a0bf5a01152194','1620413347','1','Vote','QmdoixPMMT76vSt6ewkE87JZJywS1piYsGC3nJJpcrPXKS'), ('2','QmSa7QFD3vsV6bhsfSKGW1tUtQyJk3umMTgVkFS1H8fnXJ','0x37bf9E28E099335DCec53a8b7FadeFDE6DbF108d','1620410370','1','Vote','QmdoixPMMT76vSt6ewkE87JZJywS1piYsGC3nJJpcrPXKS'),  For more content on Data and DAOs find me on Twitter.\n","date":1636239600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636239600,"objectID":"be5128acdef0ba61fd6420c532497f58","permalink":"/technical_notes/example_tech/postgresql_insert_csv_to_db/","publishdate":"2021-11-07T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/postgresql_insert_csv_to_db/","section":"technical_notes","summary":"Delete a table Situation: If you have a CSV file, you can manually load it into PostgreSQL through the pgAdmin client. This may require creating the data values in Excel first.","tags":null,"title":"Manually insert data to database","type":"docs"},{"authors":null,"categories":null,"content":"Prep dataframe to insert to database Situation: When inserting to postgresql with sqlalchemy, the to_sql() function works, but we need to make sure we\u0026rsquo;re appending the id (primary key) column the right way \u0026ndash; incrementally.\nThis will involve manipulating the dataframe with by incremeting with the max_id before using reset_index() to create an additional column using the natural index, then setting the index=False parameter.\n# change column name # id, graph_id, amount_display, from_address, to_address, tx_timestamp, timestamp_display # use rename function to change Two column names, set inplace=False to preserve original dataframe column name df2 = df.rename(columns={'id': 'graph_id', 'timestamp': 'tx_timestamp'}, inplace=False) # reorder dataframe column using list of names # list of names (in same order as stg_subgraph_bank) list_of_col_names = ['graph_id', 'amount_display', 'from_address', 'to_address', 'tx_timestamp', 'timestamp_display'] df2 = df2.filter(list_of_col_names)  This next part is KEY:\ndf2.index += max_id # increment with max_id df2 = df2.reset_index() # reset index to later increment with max_id df3 = df2.rename(columns={'index': 'id'}, inplace=False) # only do this step if you've made sure to duplicate a test table in postgresql, then ensure that the dataframe is in the same shape as the postgresql table df3.to_sql('stg_subgraph_bank_1', con=db, if_exists='append', index=False)  For more content on data science, R, and Python find me on Twitter.\n","date":1636239600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636239600,"objectID":"815e2bb91eb83f9e8ab1bbc5438ddd8e","permalink":"/technical_notes/example_tech/pipeline_prep_index_before_insert_to_db/","publishdate":"2021-11-07T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/pipeline_prep_index_before_insert_to_db/","section":"technical_notes","summary":"Prep dataframe to insert to database Situation: When inserting to postgresql with sqlalchemy, the to_sql() function works, but we need to make sure we\u0026rsquo;re appending the id (primary key) column the right way \u0026ndash; incrementally.","tags":null,"title":"Prep dataframe to insert to database","type":"docs"},{"authors":null,"categories":null,"content":"Select a range of rows Situation: This is most useful if your data table does not have a primary key. You can use the table\u0026rsquo;s natural index.\nHere is a way to select a range of rows. OFFSET value is the number of rows to skip (here skipping 20500 rows before returning any rows). ALL indicates the max number of rows to return.\nSELECT * FROM table LIMIT ALL OFFSET 20500  If you wanted to start on row 15 and only return 10 rows, the query would be:\nSELECT * FROM table LIMIT 10 OFFSET 15  For more content on data science, R, and Python find me on Twitter.\n","date":1636239600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636239600,"objectID":"a19bd024a3dee4cf7c368b71e94bde4e","permalink":"/technical_notes/example_tech/postgresql_select_range_of_rows/","publishdate":"2021-11-07T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/postgresql_select_range_of_rows/","section":"technical_notes","summary":"Select a range of rows Situation: This is most useful if your data table does not have a primary key. You can use the table\u0026rsquo;s natural index.\nHere is a way to select a range of rows.","tags":null,"title":"Select a range of rows","type":"docs"},{"authors":null,"categories":null,"content":"Use pgAdmin to upload CSV to Postgres (Quick \u0026amp; Dirty) Situation: There are many ways to upload CSV into Postgres. This is the relatively quick and dirty way. This represents an infrequent step where we happen to be loading a 20,000+ rows as a one-time event with subsequent smaller, more regular, events.\nContext: The example below is part of a larger process of querying GraphQL in JSON and converting it to Pandas dataframe before getting it into Postgres. Here we are using a mixture of Excel and pgAdmin (Postgres client) to get the job done.\nNote: pgAdmin happens to be the Postgresql-client I\u0026rsquo;m using, but any client could work.\nPre-requisite Steps:\n  Create a database table in pgAdmin. Ideally, the columns are defined and consistent with the CSV data that\u0026rsquo;s about to be uploaded. 1a. Assuming a table has already been created, we will be using the INSERT statement, otherwise, it would be a CREATE TABLE.\n  To INSERT table, you\u0026rsquo;ll right click on a table (here we\u0026rsquo;re using, \u0026lsquo;stg_subgraph_bank\u0026rsquo;), select Scripts\u0026hellip;,then INSERT Scripts.\n  The following or close variation should appear. Here we are inserting into the stg_subgraph_bank table. In this example, there are 6 columns: to_address, amount_display, from_address, graph_id, tx_timestamp and timestamp_display.\nThe TRUNCATE TABLE is to remove existing data before inserting new data (if needed).  TRUNCATE TABLE public.stg_subgraph_bank; INSERT INTO public.stg_subgraph_bank( to_address, amount_display, from_address, graph_id, tx_timestamp, timestamp_display) VALUES (?, ?, ?, ?, ?, ?);  Then, we\u0026rsquo;re turning to Excel to prepare the data that will ultimately replace \u0026ldquo;VALUE (?, ?, ?, ?, ?, ?)\u0026rdquo;.\nThere is a CONCATENATE function in Excel that converts data from rows/columns (CSV format) into parentheses of string values.\n=CONCATENATE(\u0026quot;('\u0026quot;,C2,\u0026quot;','\u0026quot;,A2,\u0026quot;',\u0026quot;,B2,\u0026quot;','\u0026quot;,E2,\u0026quot;','\u0026quot;,D2,\u0026quot;'),\u0026quot;)  After that\u0026rsquo;s been created in Excel, we\u0026rsquo;re copying and pasting all 20,000+ rows (or however many) back into pgAdmin.\nHere\u0026rsquo;s a truncated version of the 20,000+ rows of data with the TRUNCATE command to remove existing data before inserting new data:\nTRUNCATE TABLE public.stg_subgraph_bank; INSERT INTO public.stg_subgraph_bank( to_address, amount_display, from_address, graph_id, tx_timestamp, timestamp_display) VALUES ('0x7a250d5630b4cf539739df2c5dacb4c659f2488d','14897.1883870177','0x59c1349bc6f28a427e78ddb6130ec669c2f39b48','0x0f433138b2a8f2997ef387ffcebec7cd204ab2053c43f8d4a6efaa74eddc0e0c-23','1620159318','Tue, 04 May 2021 20:15:18 GMT'), ('0x156d3129b2fd634d5b0817132401aa68b0126098','14897.1883870177','0x7a250d5630b4cf539739df2c5dacb4c659f2488d','0x0f433138b2a8f2997ef387ffcebec7cd204ab2053c43f8d4a6efaa74eddc0e0c-27','1620159318','Tue, 04 May 2021 20:15:18 GMT'), ('0x11ebc944350df20940fb10dd8782d654d6aad8c6','37422.0374220399','0x9d1f1847582261be41f5a54e8b60cad21400c74f','0x355666cd33644fd05b36a54e4ddcd14190a71eea08a291731b6cd9ec8950a199-387','1620159318','Tue, 04 May 2021 20:15:18 GMT'), ('0x5e7a1573620e0df38e41dd302f68d7d8e5b99bba','3231.14250158999','0x9d1f1847582261be41f5a54e8b60cad21400c74f','0x98f688d6adcdbb1a395b21c8f30b81ef0da8454d863e6d6f9a03305c082bae82-263','1620159320','Tue, 04 May 2021 20:15:20 GMT'), ('0x2db3c0f42022fdc8dfe70036fee85e48a24b88af','3949.39809822999','0xfe8cac7dc7ac38da9ba540eb4d1797d0417dcc41','0xc9e209771502f73334340eeea2b943f98d9663a9b1eb4370d23f34a3c860c007-106','1620159320','Tue, 04 May 2021 20:15:20 GMT');  Then run the query and the new data should populate the table.\nFor more content on data science, R, and Python find me on Twitter.\n","date":1635894000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635894000,"objectID":"f3a4f6f6b29f572be82161e45151c9c5","permalink":"/technical_notes/example_tech/sql_upload_csv_postgres_pgadmin/","publishdate":"2021-11-03T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/sql_upload_csv_postgres_pgadmin/","section":"technical_notes","summary":"Use pgAdmin to upload CSV to Postgres (Quick \u0026amp; Dirty) Situation: There are many ways to upload CSV into Postgres. This is the relatively quick and dirty way. This represents an infrequent step where we happen to be loading a 20,000+ rows as a one-time event with subsequent smaller, more regular, events.","tags":null,"title":"Use pgAdmin to upload CSV to Postgres","type":"docs"},{"authors":null,"categories":null,"content":"Check Equality of Two Columns Situation: There are two tables with two columns with different names. You want a simple script to check if the rows of those columns are equal, so the two tables can be joined.\nHere\u0026rsquo;s the SQL script, taken from this stackoverflow answer.\nSELECT CASE WHEN COLUMN1 = COLUMN2 THEN '1' ELSE '0' END AS MyDesiredResult FROM Table1 INNER JOIN Table2 ON Table1.PrimaryKey = Table2.ForeignKey  Here\u0026rsquo;s an application of this script used in DAO Dash. We are comparing two tables - discord_user and discord_messages by these two columns respectively:\n discord_user_id author_user_id  SELECT CASE WHEN discord_user_id = author_user_id THEN '1' ELSE '0' END AS AreColumnsEqual FROM discord_user d INNER JOIN discord_messages m ON d.discord_user_id = m.author_user_id  For more content on data science, R, and Python find me on Twitter.\n","date":1635721200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635721200,"objectID":"8d712fe957e42c4bce23e404aadf4f6e","permalink":"/technical_notes/example_tech/sql_check_equality_two_columns/","publishdate":"2021-11-01T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/sql_check_equality_two_columns/","section":"technical_notes","summary":"Check Equality of Two Columns Situation: There are two tables with two columns with different names. You want a simple script to check if the rows of those columns are equal, so the two tables can be joined.","tags":null,"title":"Check Equality of Two Columns","type":"docs"},{"authors":null,"categories":null,"content":"Introduction to Database Design Situation: There are notes from an 8 hour course on YouTube.\nTLDR  What is the relationship between entities (tables)? Physically draw out the lines and relationships (cardinality)  one-to-one one-to-many (\u0026lsquo;many\u0026rsquo; side is the Foreign Key) many-to-many (break into two one-to-many relationships w/ intermediary table)   Do we need Lookup Tables? Design Data Tables for Integrity  Entity Integrity - ID for uniqueness  Ensure Atomic Values (Natural Keys, if cannot, then add surrogate keys)   Referential Integrity - connect tables between Foreign Keys to Primary Keys Domain Integrity - identify data types of each variable (i.e., numeric, string, date) No repeating data   Identify which foreign key is NOT NULL (surrogate id will Auto-Increment) (modality) Normalize the data  1 NF (first normal form) - atomicity 2 NF (second normal form) - partial depenency 3 NF (third normal form) - transitive dependency (solution: take problematic columns and split into their own tables with foreign key)   Foreign Key Constraints; SQL statements:  ON DELETE  RESTRICT CASCADE SET NULL   ON UPDATE  RESTRICT CASCADE SET NULL     Deteremine which JOIN is needed to get the best \u0026ldquo;view\u0026rdquo;; which table goes after the FROM statement (which table is on the left?)  INNER JOIN LEFT JOIN SELF JOIN    What is a Relational Database?\n Entity (Rows)  Entity = User (a person, an object) Row = all attribute values for an entity   Attribute (Columns)  Attributes are about an entity (user name, name, password, address etc)   Mathematical name for a Row is a Tuple  RDBMS\n View Mechanism changes the how data is presented (i.e., we don\u0026rsquo;t want all rows x columns, just a subset)  Select only certain columns   View = Read Only (not everyone has access to Update for security purpose) RDBMS allows \u0026ldquo;transactions\u0026rdquo; Example: MySQL, SQL Server, PostgreSQL (open source) Database \u0026amp; Relational Database are not separate things  SQL\n Define structure (DDL, data definition language)  CREATE, structure \u0026amp; connection between tables - JOIN   Manipulates data (DML, data manipulation language) -  UPDATE data within tables    What is Database Design?\n Separate information into multiple tables, while preventing data integrity issues How do you measure whether a database is \u0026ldquo;good\u0026rdquo; or \u0026ldquo;bad\u0026rdquo;? \u0026ndash;\u0026gt; Data Integrity Good design prevents \u0026ldquo;data integrity issues\u0026rdquo;  All data up to date No repeating data No incorrect data No broken relationships   Conceptual Schema: How data is related. Logical Schema: Table structures (i.e., X number of columns, data types), number of tables Physical Schema: Implementing into database, table types, what server? how will people access?  Data Integrity\n Entity integrity  ID is used to enforce uniqueness of an entity (user)   Referential integrity  Foreign key constrains   Domain integrity  Range of what we\u0026rsquo;re storing (correct Data Types; integers, text or dates)   Note: Relational Database does not come from the word \u0026ldquo;relationship\u0026rdquo;, it comes from Relations which is a mathematical connection between Sets When we don\u0026rsquo;t have data integrity, we have errors When we have errors, data integrity allows us to correct those errors  Database Terms (Review)\n Data Database Relational Database (stores things in tables) DBMS (how to control database) RDBMS Null (when someone doesn\u0026rsquo;t enter a value) Anomolies (errors) Integrity (protect against anomolies)  Entity Referential - keep connection through Foreign Keys and Primary Keys Domain - correct data types   Entity - what we store Attributes*** - things about an entity Relations* - connection between two sets or Tables Tuple** or Row (all attributes about an entity) Table* - physical representation of  Rows** Columns***   File* (aka Table) Record** (aka Row) Field*** (aka Column) Value (something in a column) Entry (aka a Row) DB Design - process of designing table to have integrity Schema - structure of tables Normalization - steps to get the best data base design Naming Convention - consistency in naming Keys - something to make things in unique in a table and connect between tables  More Database Terms\n SQL  DDL data define language DML data manipulation language   SQL Keywords - reserved words (e.g., SELECT) Frontend - we program frontends so people can securely access the database (doesn\u0026rsquo;t allow us to type in SQL) Backend - serverside code to communicate with database Client side Server side - serves instances of the database to the client Server side scripting language Views - taking data from database and illustrating it in a different from how it\u0026rsquo;s stored Joins - connect data from multiple tables  ID foreign key connection    Atomic Values\n Everything in a database should be about 1 thing Example: \u0026ldquo;Paul Apivat Hanvongse\u0026rdquo; \u0026ndash; to make it atomic, create 3 separate columns (i.e., nickname, first name, last name) Atom - smallest indivisible piece (1 thing) but still makes sense to treat as 1 thing  example: Address - street, city, state, zip code (3 columns)    Relationships\n Relationship - connects two or more entities  Example: (Entities) Database \u0026ndash;\u0026gt; Student - Attribute \u0026ndash;\u0026gt; Professor - Attribute \u0026ndash;\u0026gt; Class - Attribute \u0026ndash;\u0026gt; Major - Attribute\nThere\u0026rsquo;s multiple relationships here; Student have a Major, Students are in a Class, Professors are part of a Major, Professors teach a Class.\nOne-to-One\n One Entity has connection with another Entity (e.g., Husband - Wife) Social security number unique to one person  One-to-Many\n Comments under a Youtube video;  User \u0026ndash;\u0026gt; Comment 1 \u0026ndash;\u0026gt; Comment 2 \u0026ndash;\u0026gt; Comment 3\nMany-to-Many\n Polygamous marriage (i.e., Multiple husbands have multiple wives) College: Class \u0026amp; Students; class can have multiple students \u0026amp; students can take multiple classes  Designing One-to-One Relationships\n Example: Person and Username Generally One-to-one relations are stored in the same table     ID name user_name     1 Apivat Paul   2 Caleb Caleb_Curry    There are times when one-to-one relations are stored in different tables (when you want to store extra attributes about the attribute)  Cardholder Entity    ID     first_name   last_name   card_id    (card_id \u0026lt;\u0026ndash;\u0026gt; ID )\nCard\n   ID     card_number   issue_date   late_fees   max    Summary - One-to-One Relationship\n Store attribute of the entity in the table OR use another table and connect with a foreign_key  Designing One-to-Many Relationships\n The \u0026ldquo;many\u0026rdquo; side is a foreign key to the \u0026ldquo;one\u0026rdquo; side User id stays the same.     User     user_id       (multiple cards)\n   Card     card_id   user_id       Card2     card2_id   user_id       Card3     card3_id   user_id    Parent Tables and Child Tables\n Tables are either Parent or Child Keys keep tables Unique Primary key = Parent (User ID) Foreign key = Child (user_id as a reference to User ID) Child points back to Parents In one-to-one, we don\u0026rsquo;t have to worry about parent or child In one-to-many, many Children point to a Parent When we have a Child table, we always know the Parent (but not vice versa)  Notation One to One \u0026ndash;\u0026gt; 1 : 1 One to Many \u0026ndash;\u0026gt; 1 : N Many to Many \u0026ndash;\u0026gt; M : N\nDesigning Many-to-Many Relationships\n M : N Classes : Students Parent \u0026lt;\u0026ndash;\u0026gt; Parent Solution: Break it up to TWO One-to-Many Relationships with an INTERMEDIARY or JUNCTION table to connect     ID Class     63 math   75 science   89 english       ID Student     8 John   17 Jake   16 Sally   6 Claire    Intermediary Table (Child Table for both Parents)\n   class_id student_id     75 8   89 8   63 17   75 17   89 17   89 16   63 6   89 6    Summary of Relationships\n Now we can design every \u0026ldquo;binary relationship\u0026rdquo; - any relationship between two Entities  Introduction to Keys\n Keys should be Unique Never Changing Never NULL What should be unique? (e.g., a user e-mail)  Could be a Natural Key (already in the table, no need to define a new column) Could be user name.   Key should be Never Changing (otherwise database integrity is compromised)  Primary Key Index\n Index - think Index in a Book; Index points you to the data Keys are a type of Index Indexs are used for  SELECT * FROM WHERE first_name = \u0026lsquo;Caleb\u0026rsquo; (need index for this)     Look Up Table\nExample look-up table of member status:\n member_status id is the foreign key that can point to a members table all connections stay the same even if member_status changes can set Foreign Key constraints     id member_status     1 gold   2 silver   3 bronze   4 first_quest_complete   5 guess_pass   6 level_1   7 level_2   8 level_3    (where member_status \u0026amp; student_id are One-to-Many)\n   id student member_status_id     8 John 3   17 Jake 3   16 Sally 6   6 Claire 7    (where member_status \u0026amp; student_id are Many-to-Many)\n   student_id member_status_id     8 3   8 4   17 6   17 7   17 8   16 1   6 4   6 5    Lookup Tables (w Keys) allow for:\n Integrity Uniqueness Improves functionality (no repeating data) Less work Allows for added complexity  Superkey and Candidate Key\n Two main kinds of keys:  Primary Key Foreign Key    Superkey\n Any number of columns that forces each row to be unique How do we know each row is unique and talks about one entity (user)? Superkey = any number column values to force that each row is unique Candidate key = the least number of column to force every row to be unique (ie., username)  You\u0026rsquo;ll never program a superkey\nCandidate Key\n Superkey is asking: \u0026ldquo;Can every row be unique?\u0026rdquo; Then, once we answer yes, Candidate key asks: \u0026ldquo;How many columns are needed (to force every row to be unique) ?\u0026rdquo; \u0026ndash; what\u0026rsquo;s the least number of columns Then, \u0026ldquo;How many Candidate Keys do we have?\u0026rdquo; THEN, decide which Candidate Key will be the PRIMARY KEY.  Primary Key \u0026amp; Alternate Keys Primary Key Possibilities\n username* email full name, last name, middle name, address, birthday  *out of these three, username is the best primary key\nWhat are the criteria for choosing a Primary Key?\n UNIQUE NEVER CHANGING NEVER NULL  Primary keys can also be an Index (use Select statement and how you connect most of your data).\nKeys not chosen to be the Primary Key become the Alternate Keys. Alternate Keys can be useful - you could use \u0026ldquo;email\u0026rdquo; (an alternate key) as an Index.\nSELECT * FROM table WHERE email = \u0026ldquo;value@gmail.com\u0026rdquo;\nSurrogate Keys and Natural Keys\n These are \u0026ldquo;categories\u0026rdquo; of Primary Keys You won\u0026rsquo;t search for these, but you\u0026rsquo;ll Design the Database with these in mind Natural Keys are NATURALLY in the table; they fit requirements for Primary Keys and already in the table (email, username) Surrogate Keys are ADDED to your table (i.e., id); they AUTO-INCREMENT  user \u0026ndash;\u0026gt; user_id sale \u0026ndash;\u0026gt; sale_id comment \u0026ndash;\u0026gt; comment_id     Should I use Surrogate Keys or Natural Keys?\n Natural keys = already there, but not always obvious which should e natural key Surrogate keys = easy, but you have to add a new column Choose one or the other  minor performance differences   Caleb personally uses Surrogate Key  Example: user \u0026ndash;\u0026gt; user_id    Foreign Key\n Foreign Key References a Primary Key (either same table or separate table) Every table has ONE Primary Key (could be composed of many columns) Every table can have MULTIPLE Foreign Keys referencing many other tables  Not NULL Foreign Keys\n Foreign Key constraints Every row is required to have a value if the column has \u0026ldquo;not Null\u0026rdquo; related to Cardinality (NOTE: sometimes you don\u0026rsquo;t want to set \u0026ldquo;Not NULL\u0026rdquo; because an id doesn\u0026rsquo;t currently exist; but sometimes you want to force that relationship to be there between Foreign Key and Primary Key) you either want Not NULL or Not Required (depending on the situation) Primary Key values should never change, Foreign Key values can change We don\u0026rsquo;t want Primary Key values to change, but we could have Foreign Key references change (?) Primary Key and Foreign Key should be the same data type   Foreign Key Constraints\n FK = Referential Integrity Make sure if you update Parent, ZChildren will update Prevent creating Children if there\u0026rsquo;s no Parent  SQL Statements that talk about FK constraints and refer to the Parent (Primary Key)\n ON DELETE = when we delete the Parent, we want to delete the Child  RESTRICT = (no action) throw an error whenever the Parent is deleted CASCADE = whatever we do to Parent, we do to Child (delete parent, delete child) SET NULL = if delete Parent, sets Child to NULL   ON UPDATE = when we update the Parent, we want to update the Child  RESTRICT = (no action) throw an error when try to Update Parent CASCADE = If we update Parent, Child updates as well SET NULL = if update Parents, thro     Every Foreign Key column value needs to reference a Primary Key value   Simple Key, Composite Key, Compound Key\n categories of Keys Simple Primary Keys - single column (e.g., username) Composite Primary Key has multiple columns, as a group as Primary Key; at least one column doesn\u0026rsquo;t have to be a key Compound Primary Key - combination of columns in intermediary tables (in many-to-many relations); Primary Keys are compounded; all columns have to be a key  For Intermediary Tables some people will\n add a surrogate_id to the Compound  REVIEW\n Superkey Candidate Key - least number of columns used to enforce uniqueness Primary Key** - the candidate key you select as the Main key Alternate Keys - the candidate keys you didn\u0026rsquo;t select as Primary Key Foreign Keys** - make connection between tables; references Primary Keys  Primary + Foreign\n Surrogate \u0026amp; Natural Keys - surrogate (user_id) is random with no real value; Natural is already contained in your database (don\u0026rsquo;t switch between these two) Rule: You should be able to enforce uniqueness by the columns that are Naturally already there, add a surrogate key if you want Rule 2: If you cannot define uniqueness naturally, you\u0026rsquo;ll need to rely on a Surrogate key (try to avoid)  (can switch between Simple + Composites - you won\u0026rsquo;t define these explicitly) 6. Simple Key - one column key 7. Composite Key - multiple column keys 8. Compound Key - multiple column keys\nForeign Key Constraints\n ON DELETE  RESTRICT CASCADE SET NULL   ON UPDATE  RESTRICT CASCADE SET NULL    Introduction to Entity Relationship Modeling\n A standard for Drawing Databases EER Model (Enhanced Entity Relationship Model) ERD (Enhanced Relationship Diagram) ER Model (Enhanced Relationship) DDL: Define Database Structure  {do actual drawing}\nCardinality\n one to one: |\u0026mdash;\u0026mdash;-| one to many: |\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;E one row to many rows many to one: E\u0026mdash;\u0026mdash;\u0026mdash;|  Modality\n Can assign \u0026ldquo;NOT NULL\u0026rdquo; to Foreign Key, to say each card must have an owner     card_holder (Primary Key) card Foreign Key card_number     7 7 12   12 7 48   368 7 98    368 112    4 Modalities card_owner \u0026mdash;\u0026mdash;\u0026ndash; card\n -|\u0026mdash;\u0026mdash;\u0026mdash;0\u0026ndash;|- one-to-one; one card_owner can have zero or 1 card -|\u0026mdash;\u0026mdash;\u0026mdash;|\u0026ndash;|- one-to-one; one card_owner must have 1 and only 1 card -|\u0026mdash;\u0026mdash;\u0026mdash;0\u0026ndash;E- one-to-many; one card_owner can have 0 or many cards -|\u0026mdash;\u0026mdash;\u0026mdash;|\u0026ndash;E- one-to-many; one card_owner must hv 1 or many cards  Introduction to Database Normalization\n Normalization is a process where we go through our database and correct things that may cause database problems like  data integrity problems repeating data    Third Main Forms (three step-by-step data normalization process)\n 1 NF (first normal form) 2 NF (second normal form) 3 NF (third normal form)  Systematic way to normalize a good structured database\n everything must be atomic think about how data depends on other data (dependencies) must go in sequential order 1 NF \u0026ndash;\u0026gt; 2 NF \u0026ndash;\u0026gt; 3 NF  1 NF (First Normal Form of Database Normalization)\n Atomicity (data must be atomic or about one thing)  Problem: \u0026ldquo;Address column\u0026rdquo; is not atomic (street, apt, city, country, zip) Solution: Break this column into multiple columns\nProblem: User enters two email(s) as a single value (two values in one cell) Still Problem: Generate same two users for two emails (two of same primary keys)\nSolution: Break into two tables - User table \u0026amp; Email table - turn this into a one-to-many (one user to many emails)\nUser user_id (primary key) first_name last_name\nEmail email_id email user_id (foreign key)\n2 NF (Second Normal Form of Database Normalization)\n Partial Dependency (when a column depends on part of a Primary Key) You need ot have a compound or composite key (Primary Key has to be multiple columns) Found in Many-to-Many relationships w/ Intermediary Tables  {see notebook for illustration}\n3 NF (Third Normal Form of Database Normalization)\n must do 1st and 2nd Normal Form before getting to 3 NF Transitive Dependency (when a column depends on a column, which depends on a Primary Key) Solution: Take a transitive dependency (problematic column), move them to a new table and reference them with a foreign key  Summary of Normal Forms\n 1 NF = making everything atomic 2 NF = removing partial dependencies 3 NF = removing transitive dependencies  Indexes (Clustered, Nonclustered, Composite Index)\n See a book\u0026rsquo;s index (or Phone Book) A list of where certain data points are Data is sorted in a way that can easily be found Nonclustered Index = tells you how to get to the data (book\u0026rsquo;s index)  points to the data a list of references that point to the data (like back of the book)    Clustered Index = organizes the actual data in a way that\u0026rsquo;s easy to use  organizes the actual data faster and better than non-clustered    Databases\n Rather than having to go through \u0026ldquo;all\u0026rdquo; the data (i.e., Table Scan), you create an index (index seek, makes queries faster) Downside: When you update the data, you have to update the index as well otherwise index becomes outdated and useless   You only want to create an index for frequently used data  Primary Key that is indexed makes it faster   Apply WHERE (sql query) column to an Indexed column Index increase speed of JOINS Whenever you\u0026rsquo;re JOINing certain columns, the two columns you\u0026rsquo;re joining should be Indexed  Data Types\n Date  date time datetime timestamp (can be millisecond or time, when something was \u0026ldquo;done\u0026rdquo; or when something was \u0026ldquo;created\u0026rdquo; or \u0026ldquo;updated\u0026rdquo; or when a new row is entered)   Numeric  integer (only whole numbers) decimal (more accurate) float / double (unsigned) binary   String  Char(8), varchar(8) 0 up to 8 characters text - longer strings (comments, paragraphs)    Everything above here is DDL - data definition language Below here are DML - data manipulation language** Introduction to JOINS\n Joins bring multiple tables into a presentable format  Inner Join\n Table A (customer table) Table B (card table) When there are rows that connect them, a new Table is presented Taking only the rows that intersect between two tables  Eliminate any customers that do not have a card Eliminate any cards that do not have a customer   Exclude rows that are not in both tables  Example:\nCustomer Table\n   customer (Primary Key)     customer_id   first_name   last_name    Card Table\n   card (Foreign Key)     card_id   customer_id   max_amount   monthly_bill   amount_paid   amount_owed    Example Inner Join of Customer Table \u0026amp; Card Table\n   first_name last_name amount_paid amount_owed     Paul Apivat 2200 3000   Paul Apivat 720 1000   Jimmy John 3000 5000    SELECT first_name, last_name, amount_paid, amount_owed FROM customer cu INNER JOIN card ca ON cu.customer_id = ca.customer_id\nINNER JOIN on 3 Tables\nSee illustration\nINNER JOIN on 3 Tables (with Example)\nSee illustration\nIntroduction to OUTER JOINS\n INNER JOIN - least rows LEFT (Outer) JOIN RIGHT (Outer) JOIN FULL (Outer) JOIN - most rows  RIGHT OUTER JOIN\n  same as Left Outer Join, except the Right Table keeps all the rows\n  IN PRACTICE, most people don\u0026rsquo;t use Right Joins; instead they\u0026rsquo;ll use a LEFT JOIN and just flip the tables\n  How do you know which table is LEFT or RIGHT?\n  SELECT column1, column2, column3 FROM this_table_is_Left LEFT JOIN\u0026hellip;.\nJOIN with NOT NULL Columns\n Not Null columns can cause some confusion when it comes to Joins If you want to return all of a column, don\u0026rsquo;t worry about NOT NULL, just put that table in the SELECT statement (i.e., make the table on the left side) and use a LEFT JOIN  {see illustrations}\nOuter JOIN Across 3 Tables Can combine\n LEFT (outer) JOIN with RIGHT (outer) JOIN  Aliases\n use when writing SELECT statements use AS  SELF JOIN\n take a Table and JOINING with itself How to think about it:   Duplicate your exact table Joining with itself  Illustration\n   v1     user   referred_by          v2     user_id       SELECT v1.first_name, v1.last_name, v1.email, v2.email FROM user AS v1 JOIN user AS v2 ON v1.referred_by = v2.user_id\n(Self-Join is taking the same table, \u0026ldquo;user\u0026rdquo;, and making a duplicate)\nFor more content on data science, R, and Python find me on Twitter.\n","date":1635202800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635202800,"objectID":"ef14bd20e2f59f1f4569e8ccda4398a7","permalink":"/technical_notes/example_tech/database_design_tips/","publishdate":"2021-10-26T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/database_design_tips/","section":"technical_notes","summary":"Introduction to Database Design Situation: There are notes from an 8 hour course on YouTube.\nTLDR  What is the relationship between entities (tables)? Physically draw out the lines and relationships (cardinality)  one-to-one one-to-many (\u0026lsquo;many\u0026rsquo; side is the Foreign Key) many-to-many (break into two one-to-many relationships w/ intermediary table)   Do we need Lookup Tables?","tags":null,"title":"Database Design Course by Caleb Curry","type":"docs"},{"authors":null,"categories":null,"content":"Continuous to Discrete Plotting Situation: When plotting integers on the x-axis, it shows up in a ggplot like 2.5, 5, 7.5 and we want each bar to be plotted discretely (e.g. 1,2,3,4,5\u0026hellip;)\ngov_partcipation %\u0026gt;% ggplot(aes(x = n, y = nn)) + geom_col(aes(fill = as.factor(n))) + geom_text(aes(label = nn), vjust = -0.5, color = \u0026quot;white\u0026quot;) + # this sets the sequence from 1 to 10 with a break of 1 # turns a continuous sequence (2.5) into a discrete one (1,2,3...) scale_x_continuous(breaks=seq(1,10, 1))  For more content on data science, R, and Python find me on Twitter.\n","date":1633734000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633734000,"objectID":"1af3604da7a86ee02c1041cf65c19c76","permalink":"/technical_notes/example_tech/rstats_tip_continuous_to_discrete_plot/","publishdate":"2021-10-09T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip_continuous_to_discrete_plot/","section":"technical_notes","summary":"Continuous to Discrete Plotting Situation: When plotting integers on the x-axis, it shows up in a ggplot like 2.5, 5, 7.5 and we want each bar to be plotted discretely (e.","tags":null,"title":"Continuous to Discrete Plotting","type":"docs"},{"authors":null,"categories":null,"content":"Top to Bottom Situation: AFTER you display bar charts in a specific order because you successfully specified the factor levels order, you simply want to reverse the order (from top-to-bottom).\nContext: I\u0026rsquo;ve created a specific factor order and simply want to reverse the order so what was on top is now on the bottom, using scale_y_descrete(limits = rev()), the rev() and levels() function are used in tandem to specific the reverse.\nnew %\u0026gt;% count(name) %\u0026gt;% ggplot(aes(x = n, y = name, fill = name)) + geom_col() + geom_text(aes(label = n), hjust = -0.2, color = \u0026quot;white\u0026quot;) + # reverse from top to bottom on the y-axis # note levels for a specific column new$name already set previously scale_y_discrete(limits = rev(levels(new$name)))  Right to Left Works the same way, but using scale_x_discrete.\nFor more content on data science, R, and Python find me on Twitter.\n","date":1633734000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633734000,"objectID":"064da6c68391862a098bdb7eceb73ff8","permalink":"/technical_notes/example_tech/rstats_tip_reverse_plotting/","publishdate":"2021-10-09T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip_reverse_plotting/","section":"technical_notes","summary":"Top to Bottom Situation: AFTER you display bar charts in a specific order because you successfully specified the factor levels order, you simply want to reverse the order (from top-to-bottom).","tags":null,"title":"Reverse Plotting","type":"docs"},{"authors":null,"categories":null,"content":"Creating 2 Y-Axes in a Plot Situation: You want to create two y-axes. The left y-axis measures an amount, while the right y-axis converts the amount to a percentage.\nContext: This may be useful when you have a bar chart depicting relative amounts (e.g., membership numbers) and you require another y-axis (right side) to map another amount as a percentage of the former (e.g., % of members who successfully activated on their first day).\nThis is exampe is taken from the Bankless DAO Community Growth metrics where we looked at:\n How many new members successfully activate on their first day?\n First, unlike typical use cases, we are not using pivot_longer in this example, but instead keep the data in a wide format (pivot_wider). We are visualizing 3 separate variables (columns) here, including: new_members, pct_communicated and pct_opened_channels.\nIn addition, we\u0026rsquo;ll have a hard-coded fourth line that represents the industry \u0026ldquo;benchmark\u0026rdquo;.\nBecause we are not using a pivot_longer function, we will have to manually create our own legend. This will be explained in the commented code below:\nfirst_activation %\u0026gt;% rename( time = \u0026quot;interval_start_timestamp\u0026quot;, new = \u0026quot;new_members\u0026quot;, talked = \u0026quot;pct_communicated\u0026quot;, visited = \u0026quot;pct_opened_channels\u0026quot; ) %\u0026gt;% ggplot(aes(x = time)) + # note: 3 separate charts: one bar and two lines geom_bar(aes(y = new, color = \u0026quot;New members\u0026quot;), stat = \u0026quot;identity\u0026quot;, fill = \u0026quot;black\u0026quot;) + # The left y-axis goes from 0-1500, 'talked' and 'visited' is multiplied by 1500/100 = 15 # color is set to a string so it can be repurposed in the manually created legend geom_line(aes(y = talked*15, color = \u0026quot;% talked (voice or text)\u0026quot;), size = 1.5) + geom_line(aes(y = visited*15, color = \u0026quot;% visited more than 3 channels\u0026quot;), size = 1.5) + geom_line(y = 480, color = \u0026quot;orange\u0026quot;, size = 0.2) + # scale_y_continuous(sec.axis) is the key to having two y-axes # The left y-axis goes from 0-1500, so sec.axis has ~./1500*100 # name indicates both left and right y-axis label scale_y_continuous( name = \u0026quot;New Members\u0026quot;, sec.axis = sec_axis(trans = ~./1500*100, name = \u0026quot;% Activated\u0026quot;) ) + # We have to manually set color because we didn't actually set color in geom_line above scale_color_manual(values = c(\u0026quot;white\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;black\u0026quot;)) + # setting the color = \u0026quot;Legend\u0026quot; allows us to indicate on the chart where legend is labs( title = \u0026quot;How many new members successfully activate on their first day?\u0026quot;, x = \u0026quot;\u0026quot;, color = \u0026quot;Legend\u0026quot; )  Converting Second Y-axis to another number aside from Percentage (%) This is similar to the example above, except the math is adjusted so that if you wanted to convert two scales (and neither is a %). This is appropriate for when visualizing both a total amount (Messages sent) and average amount (Messages per communicator).\nmsg_avg %\u0026gt;% rename( time = \u0026quot;interval_start_timestamp\u0026quot;, per = \u0026quot;messages_per_communicator\u0026quot; ) %\u0026gt;% ggplot(aes(x = time)) + geom_bar(aes(y = messages, color = \u0026quot;Messages sent\u0026quot;), stat = \u0026quot;identity\u0026quot;, fill = \u0026quot;black\u0026quot;) + # conversion between two y-axis (similar to % conversion) # Here we convert 6000 to 12 geom_line(aes(y = per*6000/12, color = \u0026quot;Messages per communicator\u0026quot;), size = 1.5) + geom_line(y = 5000, color = \u0026quot;orange\u0026quot;, size = 0.2) + scale_y_continuous( name = \u0026quot;Messages sent\u0026quot;, sec.axis = sec_axis(trans = ~./6000*12, name = \u0026quot;Message per ommunicator\u0026quot;) ) + scale_color_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;black\u0026quot;))  For more content on data science, R, and Python find me on Twitter.\n","date":1633647600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633647600,"objectID":"e2828f6d4ea7858cdf0bdbc4bcda22db","permalink":"/technical_notes/example_tech/rstats_viz_2_y_axes/","publishdate":"2021-10-08T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_viz_2_y_axes/","section":"technical_notes","summary":"Creating 2 Y-Axes in a Plot Situation: You want to create two y-axes. The left y-axis measures an amount, while the right y-axis converts the amount to a percentage.\nContext: This may be useful when you have a bar chart depicting relative amounts (e.","tags":null,"title":"Two Y-Axes","type":"docs"},{"authors":null,"categories":null,"content":"Manually Ordering Factors Situation: Sometimes you have want to display bar charts in a specific order, but the numbers get re-arranged or group_by and sort orders by value, but you have a specific order in mind.\nContext: In this example, I have \u0026ldquo;Proposals\u0026rdquo; numbered \u0026ldquo;1-10\u0026rdquo;. I want to display them in order, but number 10 doesn\u0026rsquo;t go after 9, but instead goes 1, 10, 2, 3, and so on\u0026hellip;I need to manually set the factor order so the bar chart displays exactly how I\u0026rsquo;d like.\nI used fct_relevel (factor re-level), choose a specific column, and the second parameter is a vector of values with the factors manually arranged according to how I\u0026rsquo;d like. Then I set it to that specific column so R knows the desired factor level.\n# manually arranging factors - num # note: move \u0026quot;proposal 10\u0026quot; to the end # them apply newly arranged factor levels to column of interest new$num \u0026lt;- fct_relevel(new$num, c(\u0026quot;proposal 1\u0026quot;, \u0026quot;proposal 2\u0026quot;, \u0026quot;proposal 3\u0026quot;, \u0026quot;proposal 4\u0026quot;, \u0026quot;proposal 5\u0026quot;, \u0026quot;proposal 6\u0026quot;, \u0026quot;proposal 7\u0026quot;, \u0026quot;proposal 8\u0026quot;, \u0026quot;proposal 9\u0026quot;, \u0026quot;proposal 10\u0026quot;))  I can also achieve this for string values that don\u0026rsquo;t have an obvious order. Here i\u0026rsquo;m ordering the proposals by a known sequence (but not obvious to the audience).\n# manually arrange factors - name # then apply newly arranged factor levels to column of interest new$name \u0026lt;- fct_relevel(new$name, c(\u0026quot;Approve the Bankless DAO Genesis Proposal\u0026quot;, \u0026quot;What charity should CMS Holdings donate 100k towards\u0026quot;, \u0026quot;Badge Distribution for Second Airdrop\u0026quot;, \u0026quot;Reward Season 0 Active Members\u0026quot;, \u0026quot;Bankless DAO Season 1\u0026quot;, \u0026quot;BanklessDAO Season 1 Grants Committee Ratification\u0026quot;, \u0026quot;BED Index Logo Contest\u0026quot;, \u0026quot;Request for funds for Notion's ongoing subscription\u0026quot;, \u0026quot;Transfer ownership of the treasury multisig wallet from the genesis team to the DAO\u0026quot;, \u0026quot;Bankless DAO Season 2\u0026quot;))  For more content on data science, R, and Python find me on Twitter.\n","date":1633647600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633647600,"objectID":"5fa0299907c9b5f3a4c1192f062185e3","permalink":"/technical_notes/example_tech/rstats_tip_manually_order_factors/","publishdate":"2021-10-08T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip_manually_order_factors/","section":"technical_notes","summary":"Manually Ordering Factors Situation: Sometimes you have want to display bar charts in a specific order, but the numbers get re-arranged or group_by and sort orders by value, but you have a specific order in mind.","tags":null,"title":"Manually Ordering Factors","type":"docs"},{"authors":null,"categories":null,"content":"Survey Data Cleaning NOTE: These tips are a distillation of data cleaning techniques I picked up in the course of cleaning data for the first DAO Creators Survey (Gitcoin x BanklessDAO).\nThe DAO Creators Survey was a two part survey sampling 442 and 256 respondents to approximately 50 survey questions ranging from demographics to web3 tooling, DAO compensation/healthcare and income stability, to name a few.\nThe questions ranged from highly structured (i.e., multiple choice, multiple response options and dropdown boxes) to highly unstructured (i.e., qualitative responses).\nI created approximately 50 charts for this report and each chart presented unique data cleaning challenges. However, I will describe a base foundation and areas of overlap so the next project is easier.\nTwo articles were used for reference, but because this project optimized for speed, I did not do a full text analysis.\n  How to Generate Word Clouds in R  Text Mining with R: A Tidy Approach  Pre-Cleaning Steps The first move for any survey is to change column names into more manageable short codes and then delete identifying information to preserve privacy, for example:\ndf1 \u0026lt;- df %\u0026gt;% # Rename: shorten column names to be manageable rename( timestamp = \u0026quot;Timestamp\u0026quot;, daos_work_for = \u0026quot;what DAO(s) do you work for? for each DAO, how many hours/month do you work? (feel free to include multiple)\u0026quot;, city = \u0026quot;what city are you based in?\u0026quot;, twitter = \u0026quot;whats your twitter username?\u0026quot;, eth_addr = \u0026quot;whats your ETH address?\u0026quot;, ) %\u0026gt;% # delete identifying information select(-twitter, -eth_addr, -email)  Baseline Step: Convert Text to Tidy Format This requires the tidytext package and a couple functions. The flow is to use unnest_tokens() to separate a string of words into a vector of individual words. Then follow-up with anti_join() to get rid of stop words (a corpus of words is provided with tidytext).\nThen, group and tally, which can be achieved with group_by() and tally(sort = TRUE) or one function count(, sort = TRUE).\ndaos_work_tbl %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% anti_join(stop_words) %\u0026gt;% view()  If there are too many words, we can filter() and drop NA responses. With dplyr these operations can be chained to ggplot2 to visualize the output.\ndaos_work_tbl %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% anti_join(stop_words) %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 3) %\u0026gt;% drop_na() %\u0026gt;%  String Detect Sometimes you need to use str_detect() to see how many instances of a string are present in a column. If there is a match of string detected, you want to categorize survey responses. This is structured combining if_else() conditionals with str_detect().\nThis first requires creating an empty column:\n# create empty column daos_work_long$bin \u0026lt;- NA # use if_else and str_detect daos_work_long$bin \u0026lt;- if_else((str_detect(daos_work_long$word, \u0026quot;cre8\u0026quot;)==TRUE), \u0026quot;cre8rdao\u0026quot;, \u0026quot;NA\u0026quot;) daos_work_long$bin \u0026lt;- if_else((str_detect(daos_work_long$word, \u0026quot;mstable\u0026quot;)==TRUE), \u0026quot;mstable\u0026quot;, daos_work_long$bin) daos_work_long$bin \u0026lt;- if_else((str_detect(daos_work_long$word, \u0026quot;marrow\u0026quot;)==TRUE), \u0026quot;marrow dao\u0026quot;, daos_work_long$bin) daos_work_long$bin \u0026lt;- if_else((str_detect(daos_work_long$word, \u0026quot;badger\u0026quot;)==TRUE), \u0026quot;badger dao\u0026quot;, daos_work_long$bin) daos_work_long$bin \u0026lt;- if_else((str_detect(daos_work_long$word, \u0026quot;raid\u0026quot;)==TRUE), \u0026quot;raid guild\u0026quot;, daos_work_long$bin) daos_work_long$bin \u0026lt;- if_else((str_detect(daos_work_long$word, \u0026quot;metagame\u0026quot;)==TRUE), \u0026quot;metagame\u0026quot;, daos_work_long$bin)  String Match In some situations, you may want to see if a string contains a specific word. The function to use here is str_match(). Here, we\u0026rsquo;re seeing if a string contains either yes or yeah or no or not:\nincome_stability_tbl2 \u0026lt;- income_stability_tbl %\u0026gt;% mutate(phrase = strsplit(as.character(text), \u0026quot;,\u0026quot;)) %\u0026gt;% unnest(phrase) %\u0026gt;% count(phrase, sort = TRUE) %\u0026gt;% mutate( phrase_no = str_match(phrase, \u0026quot;[Nn]o|[Nn]ot\u0026quot;)[,1], phrase_no = str_to_lower(phrase_no) ) %\u0026gt;% mutate( phrase_yes = str_match(phrase, \u0026quot;[Yy]es|[Yy]eah\u0026quot;)[,1], phrase_yes = str_to_lower(phrase_yes) )  Handling each survey question (column) separately This requires splitting each column off. You could turn it into a vector first, then tibble or just subset a dataframe:\ncomp_denom_v \u0026lt;- as.vector(df1$comp_denom) comp_denom_tbl \u0026lt;- tibble(line = 1:445, text = comp_denom_v)  Manually add numbers Surprisingly, it was not easy to add items from the same category:\n   Item Number     Zebra 8   Zebra 17    It should be more straight forward to add Zebra. But instead we have to really manually add. For example, here i\u0026rsquo;m manually changing the n for bankless dao to 35:\n# bankless dao = 35 daos_work_long2$n[12] \u0026lt;- 35  Delete specific rows There are two ways to delete rows. First is to subset (a base R operation):\ndaos_work_long3 \u0026lt;- daos_work_long2[-c(4, 6, 13, 14, 15, 19, 24, 28, 31, 42, 46, 51, 52, 53, 61, 63, 67, 68, 69, 70, 78, 81, 87, 95, 100, 103),] %\u0026gt;% arrange(desc(n))  The second way is to use slice in dplyr. Slice can be used to select or re-order rows as well:\nusd_earning_tbl3 \u0026lt;- usd_earning_tbl2 %\u0026gt;% slice(4, 6, 7, 1:3, 5, 8:9)  Assigning Factors to Preserve Order for Visualization After using slice to re-order rows, we can use mutate() and as_factor() to create factors for visualization. This preserves the order we want (e.g., age range on the x-axis):\n# reorder rows, save as new df usd_earning_tbl3 \u0026lt;- usd_earning_tbl2 %\u0026gt;% slice(4, 6, 7, 1:3, 5, 8:9) # need to sort by factors before visualize usd_earning_tbl3 %\u0026gt;% mutate(text_factor = as_factor(text))  Separate String at Comma Sometimes, simply turning a string into tidytext doesn\u0026rsquo;t work because meaning phrases of two or three words inadvertently get split, so we may need to split by comma with mutate() and strsplit(), in lieu of using unnest_tokens(), then group and tally:\ntask_tabl2 \u0026lt;- task_tbl %\u0026gt;% mutate(phrase = strsplit(as.character(text), \u0026quot;,\u0026quot;)) %\u0026gt;% unnest(phrase) %\u0026gt;% count(phrase, sort = TRUE) %\u0026gt;% view()  Github Repo See data cleaning scripts here\n","date":1631055600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631055600,"objectID":"8241520fe505dc9305f7662943beffdc","permalink":"/technical_notes/example_tech/data_cleaning_tip1/","publishdate":"2021-09-08T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/data_cleaning_tip1/","section":"technical_notes","summary":"Survey Data Cleaning NOTE: These tips are a distillation of data cleaning techniques I picked up in the course of cleaning data for the first DAO Creators Survey (Gitcoin x BanklessDAO).","tags":null,"title":"Survey Data Cleaning","type":"docs"},{"authors":null,"categories":null,"content":"Reading in JSON data from an Open API The following example is an Open API from the Ministry of Public Health in Thailand.\nThe following script consumes the API using the httr package, then transforms JSON to dataframe via the jsonlite package.\ninstall.packages(\u0026quot;httr\u0026quot;) install.packages(\u0026quot;jsonlite\u0026quot;) library(httr) library(jsonlite) # send a GET request to the Ministry of Public Health Open API # consume API to receive JSON file url \u0026lt;- \u0026quot;https://covid19.th-stat.com/api/open/timeline\u0026quot; resp \u0026lt;- GET(url = url) # convert JSON file into text text_json \u0026lt;- content(resp, as = \u0026quot;text\u0026quot;, encoding = \u0026quot;UTF-8\u0026quot;) # read text from JSON jfile \u0026lt;- fromJSON(text_json) # save as data frame df \u0026lt;- as.data.frame(jfile) # view data frame View(df)  ","date":1618527600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618527600,"objectID":"1356217abea5f9c805d7b65acfcd5741","permalink":"/technical_notes/example_tech/rstats_open_api/","publishdate":"2021-04-16T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_open_api/","section":"technical_notes","summary":"Reading in JSON data from an Open API The following example is an Open API from the Ministry of Public Health in Thailand.\nThe following script consumes the API using the httr package, then transforms JSON to dataframe via the jsonlite package.","tags":null,"title":"Consume Open API with R","type":"docs"},{"authors":null,"categories":null,"content":"Creating a Circular Dendrogram with ggraph I tried creating a Circular Dendrogram using reproducible code from R Graph Gallery.\nHowever, the code on the website has some issues so I submitted a pull request to fix it.\nThe code below, for the most part, match the original on R Graph Gallery, except 4 lines of code that were updated to fix the geom_node_text issue:\n# libraries library(ggraph) library(igraph) library(tidyverse) library(RColorBrewer) # create data.frame d1=data.frame(from=\u0026quot;origin\u0026quot;, to=paste(\u0026quot;group\u0026quot;, seq(1,10), sep=\u0026quot;\u0026quot;)) d2=data.frame(from=rep(d1$to, each=10), to=paste(\u0026quot;subgroup\u0026quot;, seq(1,100), sep=\u0026quot;_\u0026quot;)) edges=rbind(d1, d2) # create vertices vertices = data.frame( name = unique(c(as.character(edges$from), as.character(edges$to))) , value = runif(111) ) vertices$group = edges$from[ match( vertices$name, edges$to ) ] vertices$id=NA myleaves=which(is.na( match(vertices$name, edges$from) )) nleaves=length(myleaves) vertices$id[ myleaves ] = seq(1:nleaves) # change the angle of the geom_node_text # angle and hjust must be consistent vertices$angle = 360 / nleaves * vertices$id + 110 # adjust angle calculation vertices$hjust\u0026lt;-ifelse( vertices$angle \u0026lt; 291, 1, 0) # adjust hjust vertices$angle\u0026lt;-ifelse(vertices$angle \u0026lt; 291, vertices$angle+180, vertices$angle) # adjust where 180 is added # crate mygraph object (dendrogram) mygraph \u0026lt;- graph_from_data_frame( edges, vertices=vertices ) # plot the dendrogram ggraph(mygraph, layout = 'dendrogram', circular = TRUE) + geom_edge_diagonal(colour=\u0026quot;grey\u0026quot;) + scale_edge_colour_distiller(palette = \u0026quot;RdPu\u0026quot;) + geom_node_text(aes(x = x*1.15, y=y*1.15, filter = leaf, label=name, angle = angle, hjust=hjust, colour=group), size=2.7, alpha=1) + geom_node_point(aes(filter = leaf, x = x*1.07, y=y*1.07, colour=group, size=value, alpha=0.2)) + scale_colour_manual(values= rep( brewer.pal(9,\u0026quot;Paired\u0026quot;) , 30)) + scale_size_continuous( range = c(0.1,10) ) + theme_void() + theme( legend.position=\u0026quot;none\u0026quot;, plot.margin=unit(c(0,0,0,0),\u0026quot;cm\u0026quot;), ) + expand_limits(x = c(-1.3, 1.3), y = c(-1.3, 1.3)) + coord_flip() # add coord_flip  For more content on data science, R, and Python find me on Twitter.\n","date":1614726000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614726000,"objectID":"ec40d6dbd4308b0955ce6c23580c7380","permalink":"/technical_notes/example_tech/rstats_viz_circular_dendrogram/","publishdate":"2021-03-03T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_viz_circular_dendrogram/","section":"technical_notes","summary":"Creating a Circular Dendrogram with ggraph I tried creating a Circular Dendrogram using reproducible code from R Graph Gallery.\nHowever, the code on the website has some issues so I submitted a pull request to fix it.","tags":null,"title":"Circular Dendrogram","type":"docs"},{"authors":null,"categories":null,"content":"Finding Non-Matching Rows Before Joining Columns We often need to join two columns from different data frames. Rows to be joined are assumed to have the same value.\nEven different casing means those values will not be joined. For example: \u0026ldquo;Nigeria\u0026rdquo; and \u0026ldquo;NIGERIA\u0026rdquo; will not be joined.\nIt\u0026rsquo;s particularly useful to know which values are not in sync when you have a list of countries and you want to join with one of the map libraries (e.g., ggmap). If the country is spelt differently, the join doesn\u0026rsquo;t happen.\nEnter anti_join:\nworld_map1 \u0026lt;- world_map %\u0026gt;% mutate(id = region) df1 \u0026lt;- df %\u0026gt;% mutate(id = country) # use anti_join to figure out which rows are not aligned anti_join(world_map1, df1, by = \u0026quot;id\u0026quot;)  ","date":1612479600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612479600,"objectID":"149c46044eee7745edc61d5dd7bffc7a","permalink":"/technical_notes/example_tech/rstats_tip6/","publishdate":"2021-02-05T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip6/","section":"technical_notes","summary":"Finding Non-Matching Rows Before Joining Columns We often need to join two columns from different data frames. Rows to be joined are assumed to have the same value.\nEven different casing means those values will not be joined.","tags":null,"title":"Use Anti_Joins to Find Non-Matching Rows","type":"docs"},{"authors":null,"categories":null,"content":"Extending the number of colors available in a palette You might be using the RColorBrewer library and one of the palettes: Sequential, Qualitative or Diverging:\nSequential includes: Blues, BuGn, BuPu, GnBu, Greens, Greys, Oranges, OrRd, PuBu, PuBuGn, PuRd, Purples, RdPu, Reds, YlGn, YlGnBu YlOrBr, YlOrRd.\nQualitative includes: Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3\nDiverging includes: BrBG, PiYG, PRGn, PuOr, RdBu, RdGy, RdYlBu, RdYlGn, Spectral\nThese palettes 8-9 colors, at most 12. But what if you need more? Here\u0026rsquo;show to extend the palettes:\nlibrary(RColorBrewer) number.colors \u0026lt;- 46 mycolors \u0026lt;- colorRampPalette(brewer.pal(8, \u0026quot;Set1\u0026quot;))(number.colors) df %\u0026gt;% ggplot(aes(x=x, y=y)) + geom_boxplot() + geom_point() + scale_color_manual(values = mycolors)  ","date":1612479600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612479600,"objectID":"1f3b9c1069a9ccd30778aa93238294cd","permalink":"/technical_notes/example_tech/rstats_viz_extend_color/","publishdate":"2021-02-05T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_viz_extend_color/","section":"technical_notes","summary":"Extending the number of colors available in a palette You might be using the RColorBrewer library and one of the palettes: Sequential, Qualitative or Diverging:\nSequential includes: Blues, BuGn, BuPu, GnBu, Greens, Greys, Oranges, OrRd, PuBu, PuBuGn, PuRd, Purples, RdPu, Reds, YlGn, YlGnBu YlOrBr, YlOrRd.","tags":null,"title":"Extend Color Palettes","type":"docs"},{"authors":null,"categories":null,"content":"Changing Date formats with Lubridate For every data visualization project that involves using dates on one of the axes, I always find myself having to re-format the date so that visualization \u0026ldquo;works\u0026rdquo;.\nHere\u0026rsquo;s a workflow that is recommended at the start of any data visualization project.\nlibrary(tidyverse) library(lubridate) # handling dates df %\u0026gt;% # handling date first mutate( date = original_date_variable %\u0026gt;% ymd(), year = date %\u0026gt;% year(), month = date %\u0026gt;% month(), day = date %\u0026gt;% day(), year_month = make_datetime(year, month) # combine year \u0026amp; month )  ","date":1607382000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607382000,"objectID":"86bf631fc9aea458a7cb6815db3df196","permalink":"/technical_notes/example_tech/rstats_dates/","publishdate":"2020-12-08T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_dates/","section":"technical_notes","summary":"Changing Date formats with Lubridate For every data visualization project that involves using dates on one of the axes, I always find myself having to re-format the date so that visualization \u0026ldquo;works\u0026rdquo;.","tags":null,"title":"Formating Dates with Lubridate","type":"docs"},{"authors":null,"categories":null,"content":"Ways of handling nested data Recently, I downloaded JSON data from BigQuery and had to make sense of the data. This starts with getting the data into tabular form.\nHere are the libraries I used:\nlibrary(jsonlite) library(tidyverse)  First, read in JSON data. Once read in, we check its class type to see that its a list. We\u0026rsquo;ll want to get it into a data frame.\n# read data out into Large list (321 elements, 2.4 Mb) # each row is *another* list funnel \u0026lt;- lapply(readLines(\u0026quot;bq-mixpanel-funnel.json\u0026quot;), fromJSON) # \u0026quot;list\u0026quot; class class(funnel)  After searching online, three approaches continually resurfaced.\nFirst, using unlist() and converting into matrix() before wrapping that in a data.frame():\n# Approach 1: convert to matrix, array unlist_funnel \u0026lt;- matrix(unlist(funnel), byrow = TRUE, ncol = length(funnel[[1]])) rownames(unlist_funnel) \u0026lt;- names(funnel) as.data.frame(unlist_funnel) %\u0026gt;% view()  These next approaches get us closer (note: I know from interacting with the data in BigQuery that there should be 321 rows):\n# Approach 2: Convert list to data frame df \u0026lt;- data.frame(matrix(unlist(funnel), nrow = length(funnel), byrow = TRUE)) df2 \u0026lt;- data.frame(matrix(unlist(funnel), nrow = length(funnel), byrow = FALSE)) df3 \u0026lt;- data.frame(matrix(unlist(funnel), nrow = 321, byrow = TRUE), stringsAsFactors = FALSE)  The next approach is to use lapply()\n#works but everything is on one column unlist(lapply(funnel, c)) %\u0026gt;% view() # this makes everything a list, but we want everything into a vector t(lapply(funnel, c)) %\u0026gt;% view()  Finally, the approach that worked best, in this particular case was sapply(). This functions turns things into vector, which can then be converted into a dataframe:\n#still the ideal, this works because 'c' is used ot combine lists t(sapply(funnel, c)) %\u0026gt;% view()  ","date":1602370800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602370800,"objectID":"3dfa4cd1a73e03115bfb119a69ccbcbf","permalink":"/technical_notes/example_tech/rstats_unnest/","publishdate":"2020-10-11T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_unnest/","section":"technical_notes","summary":"Ways of handling nested data Recently, I downloaded JSON data from BigQuery and had to make sense of the data. This starts with getting the data into tabular form.\nHere are the libraries I used:","tags":null,"title":"Reading and manipulating nested data","type":"docs"},{"authors":null,"categories":null,"content":"Setting up a barebones table with {reactable} There are several packages to style your tables. This note will help you get setup with a basic table using the reactable package. With just a few lines of code, you can have a table with pagination and column sorting.\nThe data for this note comes from TidyTuesday 2020-09-22, \u0026ldquo;Himalayan Climbers\u0026rdquo;.\nThis note assumes that data has been wrangled and in tibble form, ready to be styled into a table.\nSample Tibble Here, I\u0026rsquo;ve saved my tibble of 20 rows and 3 columns in df.\n\u0026gt; df # A tibble: 20 x 3 peak attempts fail_rate \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 Everest 21813 0.54 2 Cho Oyu 8890 0.570 3 Ama Dablam 8406 0.479 4 Manaslu 4593 0.621 5 Dhaulagiri I 2592 0.789 6 Makalu 2405 0.764 7 Lhotse 2379 0.638 8 Baruntse 2190 0.708 9 Pumori 1780 0.706 10 Annapurna I 1669 0.821 11 Kangchenjunga 1385 0.682 12 Himlung Himal 1308 0.573 13 Annapurna IV 812 0.845 14 Putha Hiunchuli 738 0.599 15 Tilicho 670 0.781 16 Tukuche 462 0.753 17 Jannu 339 0.782 18 Langtang Lirung 338 0.84 19 Makalu II 322 0.758 20 Nuptse 303 0.934  Load Libraries library(tidyverse) library(reactable) library(htmltools)  Basic Table The amazing thing is, with just this one line, you have a barebones table with pagination (with 20 rows, it shows 10 at a time; this can be adjusted) and sorting for both columns.\nYou can check out the rest of the repo here\nreactable(df)  Adding Bar Charts for Each Row Of course, bare bones is not much to look at, so adding bar charts is essential for visually communicating quantities and percentages. However, you\u0026rsquo;ll need to use the htmltools package to begin adding div to your chart.\n# Bar Charts can be added with a function bar_chart \u0026lt;- function(label, width = \u0026quot;100%\u0026quot;, height = \u0026quot;14px\u0026quot;, fill = \u0026quot;#00bfc4\u0026quot;, background = NULL){ bar \u0026lt;- div(style = list(background = fill, width = width, height = height)) chart \u0026lt;- div(style = list(flexGrow = 1, marginLeft = \u0026quot;6px\u0026quot;, background = background), bar) div(style = list(display = \u0026quot;flex\u0026quot;, alignItems = \u0026quot;center\u0026quot;), label, chart) } # The bar_chart function is then inserted into the numeric columns reactable( df, defaultSorted = \u0026quot;attempts\u0026quot;, columns = list( peak = colDef( name = \u0026quot;Peaks\u0026quot; ), attempts = colDef( name = \u0026quot;Attempts (#)\u0026quot;, defaultSortOrder = \u0026quot;desc\u0026quot;, #format = colFormat(separators = TRUE), # Render Bar charts using a custom cell render function cell = function(value){ width \u0026lt;- paste0(value * 100 / max(df$attempts), \u0026quot;%\u0026quot;) # Add thousands separators value \u0026lt;- format(value, big.mark = \u0026quot;,\u0026quot;) # Fix each label using the width of the widest number (incl. thousands separators) value \u0026lt;- format(value, width = 9, justify = 'right') bar_chart(value, width = width, fill = \u0026quot;#3fc1c9\u0026quot;) }, # And left-align the columns align = \u0026quot;left\u0026quot;, # Use the operating system's default monospace font, and # preserve the white space to prevent it from being collapsed by default style = list(fontFamily = \u0026quot;monospace\u0026quot;, whiteSpace = \u0026quot;pre\u0026quot;) ), fail_rate = colDef( name = \u0026quot;Fail (%)\u0026quot;, defaultSortOrder = \u0026quot;desc\u0026quot;, #format = colFormat(percent = TRUE, digits = 1) # Render Bar charts using a custom cell render function cell = function(value){ # Format as percentage with 1 decimal place value \u0026lt;- paste0(format(value * 100, nsmall = 1), \u0026quot;%\u0026quot;) # Fix width here to align single and double-digit percentages value \u0026lt;- format(value, width = 5, justify = \u0026quot;right\u0026quot;) bar_chart(value, width = value, fill = \u0026quot;#fc5185\u0026quot;, background = \u0026quot;#e1e1e1\u0026quot;) }, # And left-align the columns align = \u0026quot;left\u0026quot;, style = list(fontFamily = \u0026quot;monospace\u0026quot;, whiteSpace = \u0026quot;pre\u0026quot;) ) ) )  ","date":1600988400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600988400,"objectID":"755be6aa7da017af419014c999ef276a","permalink":"/technical_notes/example_tech/rstats_viz_reactable/","publishdate":"2020-09-25T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_viz_reactable/","section":"technical_notes","summary":"Setting up a barebones table with {reactable} There are several packages to style your tables. This note will help you get setup with a basic table using the reactable package. With just a few lines of code, you can have a table with pagination and column sorting.","tags":null,"title":"Styling tables with reactable","type":"docs"},{"authors":null,"categories":null,"content":"Changing the x-axis from decimals to integers When creating plots in ggplot2 you\u0026rsquo;ll often want to customize the x-axis so that values appear on a certain interval. In the example below, I wanted to change the intervals from 0.25, 0.50, 0.75 to 1,2,3,4 and so on. In this specific instance, I wanted each season of the show Friends to have its down tick on the x-axis (note: the show had ten seasons).\nThis operation changes the x-axis ticks from having decimals to being integers.\nlibrary(ggplot) ggplot(total_data, aes(x = season, y = episode, fill=imdb_rating)) + geom_tile() + scale_fill_gradient(low = '#FFF580', high = '#FF4238') + ## the seq() function defines the start and end numbers ## 'by =' indicates the desired interval scale_x_continuous(breaks = seq(1,10, by = 1)) + theme_classic()  ","date":1600124400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600124400,"objectID":"e92bd85ddda5cee358e10fe7c90aff40","permalink":"/technical_notes/example_tech/rstats_viz_scale_x/","publishdate":"2020-09-15T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_viz_scale_x/","section":"technical_notes","summary":"Changing the x-axis from decimals to integers When creating plots in ggplot2 you\u0026rsquo;ll often want to customize the x-axis so that values appear on a certain interval. In the example below, I wanted to change the intervals from 0.","tags":null,"title":"Decimals to Integers","type":"docs"},{"authors":null,"categories":null,"content":"Treemapify There are several options for visualizing treemaps in R. This note focuses on Treemapify, a package maintained by David Wilkins.\nI favor this approach over the treemap package because it is compatible with ggplot2 and allows users to access its' functionality.\nHere\u0026rsquo;s an example Treemap I created to visualize the dominant emotions displayed for the iconic 90\u0026rsquo;s sitcom, Friends. You can find out more about the Friends dataset here.\nOther visualizations I created for the Friends project can also be found here.\nBelow, we can see that geom_treemap, geom_treemap_subgroup_border and geom_treemap_subgroup_text are layers that works seamlessly with other layers like scale_fill_manual, theme, and labs that are staples of the ggplot2 package.\nBottom line, it\u0026rsquo;s easier to customize treemaps from the treemapify package than the treemap package.\nlibrary(treemapify) ggplot(friends_emo_tree, aes(area = n, label = speaker, subgroup = emotion)) + geom_treemap(aes(fill = emotion, alpha = n)) + geom_treemap_subgroup_border(color = 'white') + geom_treemap_subgroup_text(place = 'bottom', grow = T, alpha = 0.3, color = 'black', min.size = 0) + geom_treemap_text(color = 'white', fontface = 'italic', place = 'centre', reflow = T) + scale_fill_manual(values = c('#FF4238', '#FFDC00', '#42A2D6', '#9A0006', '#FFF580', '#00009E')) + theme( plot.background = element_rect(fill = '#36454F'), legend.position = 'none', title = element_text(colour = 'white', family = 'Friends') ) + labs( title = 'The One with the Dominant Emotions', caption = 'Viz: @paulapivat | Data: #TidyTuesday' )  ","date":1600124400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600124400,"objectID":"b9d7ec81504df4de64455ab116182243","permalink":"/technical_notes/example_tech/rstats_viz_treemapify/","publishdate":"2020-09-15T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_viz_treemapify/","section":"technical_notes","summary":"Treemapify There are several options for visualizing treemaps in R. This note focuses on Treemapify, a package maintained by David Wilkins.\nI favor this approach over the treemap package because it is compatible with ggplot2 and allows users to access its' functionality.","tags":null,"title":"GGPlot Flavored Treemaps","type":"docs"},{"authors":null,"categories":null,"content":"Marginal Distribution with ggplot2 and ggExtra The data in this example is from the UN Statistics Division Sustainable Development Goal, Indicator 4.4.1.\nAlso check out the r-graph-gallery.com for inspiration.\nHere\u0026rsquo;s the breakdown:\n Load Packages and Libraries  The key here is the ggExtra package.\ninstall.packages('ggExtra') library(ggExtra) library(tidyverse)  Create a basic scatter plot  The key here is using pivot_wider to give all Type of skill their own columns. We\u0026rsquo;ll then pick out specific columns (i.e., COPA, EMAIL, PCPR) to summarize, then plot on the x- and y- axes.\n# Basic Scatter Plot (color cluster by Gender) p \u0026lt;- data %\u0026gt;% select(GeoAreaName, TimePeriod, Sex, `Type of skill`, Value) %\u0026gt;% group_by(GeoAreaName, TimePeriod, Sex, `Type of skill`, Value) %\u0026gt;% pivot_wider(names_from = `Type of skill`, values_from = Value) %\u0026gt;% mutate( COPA = as.numeric(COPA), EMAIL = as.numeric(EMAIL), PCPR = as.numeric(PCPR) ) %\u0026gt;% # Group by GeoAreaName, across TimePeriod, Sex group_by(GeoAreaName, Sex) %\u0026gt;% summarize( avg_COPA = mean(COPA, na.rm = TRUE), avg_EMAIL = mean(EMAIL, na.rm = TRUE), avg_PCPR = mean(PCPR, na.rm = TRUE) ) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(x = avg_PCPR, y = avg_EMAIL, color = Sex)) + geom_point()  Use ggMarginal() to create the marginal distribution along the side of the scatter plots. This is a function from the ggExtra package.  # Scatter Plot with Marginal Distribution ggMarginal(p, type = 'histogram')  This particular chart is especially useful to highlight different distributions.\n","date":1599001200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599001200,"objectID":"9b951ca5d2ded69beeec9c9bddcbaf55","permalink":"/technical_notes/example_tech/data_viz_tip2/","publishdate":"2020-09-02T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/data_viz_tip2/","section":"technical_notes","summary":"Marginal Distribution with ggplot2 and ggExtra The data in this example is from the UN Statistics Division Sustainable Development Goal, Indicator 4.4.1.\nAlso check out the r-graph-gallery.com for inspiration.\nHere\u0026rsquo;s the breakdown:","tags":null,"title":"Visualize Scatterplots with Marginal Distribution using ggExtra","type":"docs"},{"authors":null,"categories":null,"content":"Calculating Percentiles When we have a list of values in a column, how can we determine which values are under/over the 25th percentile, 50th percentile or 75th percentile?\nHere the example are countries' average percentages of the population with, broadly speaking, ICT Skills as determine by the Sustainable Development Goals, Indicator 4.4.1.\nThere are three methods. First, manually calculating values for the 25th, 50th and 75th percentile with the quantile() function.\n# Country mean_values at 25th, 50th and 75th percentile data %\u0026gt;% select(GeoAreaName, Value, Sex, `Type of skill`, TimePeriod, Units) %\u0026gt;% rename(type_of_skill = `Type of skill`) %\u0026gt;% mutate(Value = as.numeric(Value)) %\u0026gt;% group_by(GeoAreaName) %\u0026gt;% summarize( mean_value = mean(Value) ) %\u0026gt;% mutate( min_mean = min(mean_value), iqr_25_percentile = quantile(mean_value, probs = c(0.25)), iqr_50_percentile = quantile(mean_value, probs = c(0.50)), iqr_75_percentile = quantile(mean_value, probs = c(0.75)), max_mean = max(mean_value) ) %\u0026gt;% arrange(desc(mean_value))  The second approach is to use the ntile() function:\n# Creating bins using ntile() data %\u0026gt;% select(GeoAreaName, Value, Sex, `Type of skill`, TimePeriod, Units) %\u0026gt;% rename(type_of_skill = `Type of skill`) %\u0026gt;% mutate(Value = as.numeric(Value)) %\u0026gt;% group_by(GeoAreaName) %\u0026gt;% summarize( mean_value = mean(Value) ) %\u0026gt;% mutate( mean_value_binned = ntile(mean_value, 4) ) %\u0026gt;% arrange(desc(mean_value))  The third approach uses the purrr package and the partial function that can be used with dplyr's summarize_at() function. Check out the source\n ## Using purrr library(purrr) p \u0026lt;- c(0.25, 0.50, 0.75) p_names \u0026lt;- map_chr(p, ~paste0(.x*100, \u0026quot;%\u0026quot;)) p_funs \u0026lt;- map(p, ~partial(quantile, probs = .x, na.rm = TRUE)) %\u0026gt;% set_names(nm = p_names) p_funs data %\u0026gt;% select(GeoAreaName, Value, Sex, `Type of skill`, TimePeriod, Units) %\u0026gt;% rename(type_of_skill = `Type of skill`) %\u0026gt;% mutate(Value = as.numeric(Value)) %\u0026gt;% group_by(GeoAreaName) %\u0026gt;% summarize( mean_value = mean(Value) ) %\u0026gt;% summarize_at(vars(mean_value), funs(!!!p_funs))  ","date":1598482800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598482800,"objectID":"c9640b108f821e003f9ce51a09f19de2","permalink":"/technical_notes/example_tech/rstats_tip4/","publishdate":"2020-08-27T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip4/","section":"technical_notes","summary":"Calculating Percentiles When we have a list of values in a column, how can we determine which values are under/over the 25th percentile, 50th percentile or 75th percentile?\nHere the example are countries' average percentages of the population with, broadly speaking, ICT Skills as determine by the Sustainable Development Goals, Indicator 4.","tags":null,"title":"Calculating 25th, 50th and 75th Percentile of Column Values","type":"docs"},{"authors":null,"categories":null,"content":"Data Wrangling: Does Order matter? In short, yes, it matters. But when and where?\nBelow are examples to highlight when function order matters and when it doesn\u0026rsquo;t. The source for the raw data used in this illustration are from the United Nations' Statistics Division for Sustainable Development Goals (SDG) Indicators (Goal 4, Target 4.4).\nSee also UN Statistics Wiki on Indicator 4.4.1.\n# Example chain of functions to determine proportion of Thailand's population to have certain ICT skills in 2018 data %\u0026gt;% select(GeoAreaName, Value, Sex, `Type of skill`, TimePeriod) %\u0026gt;% rename(type_of_skill = `Type of skill`) %\u0026gt;% mutate( Value = as.double(Value) ) %\u0026gt;% filter(GeoAreaName == 'Thailand') %\u0026gt;% filter(TimePeriod == 2018) %\u0026gt;% group_by(type_of_skill) %\u0026gt;% summarize( mean_value = mean(Value), median_value = median(Value) ) %\u0026gt;% ungroup() %\u0026gt;% arrange(desc(mean_value))  Here, the filter functions are moved up to be before rename and mutate. The ordering here does not matter.\ndata %\u0026gt;% select(GeoAreaName, Value, Sex, `Type of skill`, TimePeriod) %\u0026gt;% # putting filter before rename, mutate is fine filter(GeoAreaName == 'Thailand') %\u0026gt;% filter(TimePeriod == 2018) %\u0026gt;% rename(type_of_skill = `Type of skill`) %\u0026gt;% mutate( Value = as.double(Value) ) %\u0026gt;% group_by(type_of_skill) %\u0026gt;% summarize( mean_value = mean(Value), median_value = median(Value) ) %\u0026gt;% ungroup() %\u0026gt;% arrange(desc(mean_value))  Furthermore, we could even experiment with the filter function being before or after select. Here, ordering also does not matter.\ndata %\u0026gt;% filter(GeoAreaName == 'Thailand') %\u0026gt;% filter(TimePeriod == 2018) %\u0026gt;% select(GeoAreaName, Value, Sex, `Type of skill`, TimePeriod) %\u0026gt;% rename(type_of_skill = `Type of skill`) %\u0026gt;% mutate( Value = as.double(Value) ) %\u0026gt;% group_by(type_of_skill) %\u0026gt;% summarize( mean_value = mean(Value), median_value = median(Value) ) %\u0026gt;% ungroup() %\u0026gt;% arrange(desc(mean_value))  Here is there order does matter. When we try to move the two filter functions below group_by, summarize and ungroup, the filtering does not work. By the time we get to filter(GeoAreaName == 'Thailand') in this example, GeoAreaName has been removed because we did not group_by GeoAreaName, so we get an error.\n# Running this code, we'll get the ERROR: Problem with `filter()` input `..1`. x object 'GeoAreaName' not found ℹ Input `..1` is # `GeoAreaName == \u0026quot;Thailand\u0026quot;`. data %\u0026gt;% select(GeoAreaName, Value, Sex, `Type of skill`, TimePeriod) %\u0026gt;% relocate(Sex, Value, GeoAreaName) %\u0026gt;% rename(type_of_skill = `Type of skill`) %\u0026gt;% mutate( Value = as.double(Value) ) %\u0026gt;% # filter was previously here group_by(type_of_skill) %\u0026gt;% summarize( mean_value = mean(Value), median_value = median(Value) ) %\u0026gt;% ungroup() %\u0026gt;% # moving filter down below group_by \u0026amp; summarize() does not work filter(GeoAreaName == 'Thailand') %\u0026gt;% filter(TimePeriod == 2018) %\u0026gt;% arrange(desc(mean_value))  Finally, if we want to use filter on the results of the mutate function, we see that order does matter. By the time we get to the final filter(Value \u0026lt; 10), the Value variable is no longer available to us because we did not group_by and summarize by Value (instead we created mean_value and median_value).\n data %\u0026gt;% filter(GeoAreaName == 'Thailand') %\u0026gt;% filter(TimePeriod == 2018) %\u0026gt;% select(GeoAreaName, Value, Sex, `Type of skill`, TimePeriod) %\u0026gt;% rename(type_of_skill = `Type of skill`) %\u0026gt;% mutate( Value = as.double(Value) ) %\u0026gt;% # filtering for Values less than 10 does work here filter(Value \u0026lt; 10) %\u0026gt;% group_by(type_of_skill) %\u0026gt;% summarize( mean_value = mean(Value), median_value = median(Value) ) %\u0026gt;% ungroup() %\u0026gt;% arrange(desc(mean_value)) %\u0026gt;% # filter for Values less than 10 does not work down here filter(Value \u0026lt; 10)  ","date":1598482800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598482800,"objectID":"1c27e97755595f70e8977c11b286d8c0","permalink":"/technical_notes/example_tech/rstats_tip5/","publishdate":"2020-08-27T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip5/","section":"technical_notes","summary":"Data Wrangling: Does Order matter? In short, yes, it matters. But when and where?\nBelow are examples to highlight when function order matters and when it doesn\u0026rsquo;t. The source for the raw data used in this illustration are from the United Nations' Statistics Division for Sustainable Development Goals (SDG) Indicators (Goal 4, Target 4.","tags":null,"title":"Does order of operation matter among dplyr functions?","type":"docs"},{"authors":null,"categories":null,"content":"Nested and Hierarchical Data When you have data with multiple subgroups, one option is to treat them as nested and/or hierarchical data.\nIn this technical note, I\u0026rsquo;ll outline how to create a dendrogram.\nThe data used is from the Extinct Plants data set from TidyTuesday.\nHere\u0026rsquo;s the breakdown:\n Load Packages and Libraries  install.packages(\u0026quot;ggraph\u0026quot;) install.packages(\u0026quot;igraph\u0026quot;) library(ggraph) library(igraph)  Create a data frame with three levels  Taking the plants data frame, I do some wrangling to get the desired columns.\nplants_data \u0026lt;- plants %\u0026gt;% select(group, binomial_name) %\u0026gt;% group_by(group) %\u0026gt;% arrange(group) %\u0026gt;% mutate( level1 = 'center', level2 = group, level3 = binomial_name ) %\u0026gt;% # important to ungroup here ungroup() %\u0026gt;% select(level1:level3)  Create an edge list  # transform it to an edge list plants_edges_level1_2 \u0026lt;- plants_data %\u0026gt;% select(level1, level2) %\u0026gt;% unique %\u0026gt;% rename(from=level1, to=level2) plants_edges_level2_3 \u0026lt;- plants_data %\u0026gt;% select(level2, level3) %\u0026gt;% unique %\u0026gt;% rename(from=level2, to=level3) plants_edge_list=rbind(plants_edges_level1_2, plants_edges_level2_3)  Plot a basic chart  Because I have many observations, I\u0026rsquo;m optiing to use a \u0026ldquo;circular\u0026rdquo; dendrogram.\n# plot plant dendogram plantgraph \u0026lt;- graph_from_data_frame(plants_edge_list) ggraph(plantgraph, layout = \u0026quot;dendrogram\u0026quot;, circular = TRUE) + geom_edge_diagonal() + geom_node_point() + theme_void()  Add text to the end of the edges  # add text \u0026amp; color(leaf) ggraph(plantgraph, layout = \u0026quot;dendrogram\u0026quot;, circular = TRUE) + geom_edge_diagonal() + geom_node_text(aes(label = name, filter=leaf), hjust = 1, size = 1) + geom_node_point()  NOTE: The breakdown of plant groupings are listed below. We can see the Flowering Plants disproportionately out number other groups like Ferns \u0026amp; Allies, Cycad, Mosses, Algae and Conifer.\nWhen visualizing, we\u0026rsquo;re better off separating Flowering Plants from the other groups.\nplants %\u0026gt;% group_by(group) %\u0026gt;% tally(sort = TRUE) # A tibble: 6 x 2 group n \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; 1 Flowering Plant 471 2 Ferns and Allies 13 3 Cycad 8 4 Mosses 4 5 Algae 3 6 Conifer 1  Here\u0026rsquo;s a sample picture of the plants\n  Dendrogram.   ","date":1598482800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598482800,"objectID":"6223f32efc8983e145b906f2ba2cd15b","permalink":"/technical_notes/example_tech/data_viz_tip1/","publishdate":"2020-08-27T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/data_viz_tip1/","section":"technical_notes","summary":"Nested and Hierarchical Data When you have data with multiple subgroups, one option is to treat them as nested and/or hierarchical data.\nIn this technical note, I\u0026rsquo;ll outline how to create a dendrogram.","tags":null,"title":"Creating a dendrogram with R and ggraph","type":"docs"},{"authors":null,"categories":null,"content":"With tidyr 1.0.0, there are several enhancements, one of which are pivot_wider() and pivot_longer().\nUsing pivot_wider() In another post, the spread() function was introduced as a way to observe the \u0026ldquo;tidy\u0026rdquo; principle of data formatting for analysis.\nThe pivot_wider() function is an updated of spread() and is much more intuitive. Here\u0026rsquo;s how it works:\n# PIVOT_WIDER - even better than Spread data %\u0026gt;% filter(GeoAreaName==\u0026quot;Morocco\u0026quot; | GeoAreaName==\u0026quot;Qatar\u0026quot;) %\u0026gt;% select(GeoAreaName, TimePeriod, Sex, `Type of skill`, Value) %\u0026gt;% group_by(GeoAreaName, TimePeriod, Sex, `Type of skill`, Value) %\u0026gt;% pivot_wider(names_from = `Type of skill`, values_from = Value)  In this data set, Type of skill represents, broadly speaking, ICT Skills broken down into eight categories in this column. By using pivot_wider() each sub-category of ICT Skills gets it\u0026rsquo;s own column, thus making the overall data frame wider.\nUsing pivot_longer() Conversely, there\u0026rsquo;s also pivot_longer for the opposite effect. This next code chunk is part of my attempt for TidyTuesday (\u0026lsquo;Extinct Plants\u0026rsquo; for the week of 2020-08-18)\nThe cols parameter determines the range of columns to be changed from wide to long. The names_to parameter sets the new column name and values_to indicates the value of the new columns.\n# PIVOT_LONGER - better than Gather plants %\u0026gt;% select(binomial_name, threat_AA:action_NA) %\u0026gt;% pivot_longer(cols = threat_AA:action_NA, names_to = \u0026quot;action\u0026quot;, values_to = \u0026quot;count\u0026quot;) %\u0026gt;% ggplot(aes(x = binomial_name, y = action, fill = count)) + geom_tile() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  ","date":1598396400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598396400,"objectID":"182f35a6a1bdc5bf30fd2f7628c58e38","permalink":"/technical_notes/example_tech/rstats_tip3/","publishdate":"2020-08-26T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip3/","section":"technical_notes","summary":"With tidyr 1.0.0, there are several enhancements, one of which are pivot_wider() and pivot_longer().\nUsing pivot_wider() In another post, the spread() function was introduced as a way to observe the \u0026ldquo;tidy\u0026rdquo; principle of data formatting for analysis.","tags":null,"title":"Using the pivot_wider() function","type":"docs"},{"authors":null,"categories":null,"content":"One principle of tidy data is to change from wide to long; and conversely, long to wide.\nHere\u0026rsquo;s a concrete example:\nUsing spread() The first part of the below pre-processing steps include subsetting the original data frame (data) by selecting two countries for comparison (Morocco and Qatar) on specific variables such as: GeoAreaName, TimePeriod, Sex, Type of skill and Value.\nThen employing group_by to ensure all rows are unique. The next line is key as it addresses an error that each row must be marked by a unique id key.\nFinally, the spread() function allows us to see each countries' relative performance on various ICT skills. Please consult meta-data for more details.\ndata %\u0026gt;% filter(GeoAreaName==\u0026quot;Morocco\u0026quot; | GeoAreaName==\u0026quot;Qatar\u0026quot;) %\u0026gt;% select(GeoAreaName, TimePeriod, Sex, `Type of skill`, Value) %\u0026gt;% group_by(GeoAreaName, TimePeriod, Sex, `Type of skill`, Value) %\u0026gt;% # Error: Each row of output must be identified by a unique combination of keys. # rowid_to_column() address this error tibble::rowid_to_column() %\u0026gt;% spread(key = `Type of skill`, value = Value)  ","date":1598310000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598310000,"objectID":"3a8d20bf9e6f96b168329d46fd40ce86","permalink":"/technical_notes/example_tech/rstats_tip2/","publishdate":"2020-08-25T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip2/","section":"technical_notes","summary":"One principle of tidy data is to change from wide to long; and conversely, long to wide.\nHere\u0026rsquo;s a concrete example:\nUsing spread() The first part of the below pre-processing steps include subsetting the original data frame (data) by selecting two countries for comparison (Morocco and Qatar) on specific variables such as: GeoAreaName, TimePeriod, Sex, Type of skill and Value.","tags":null,"title":"Use the spread() function","type":"docs"},{"authors":null,"categories":null,"content":"Understanding reproducibility and the set.seed() function in R is best achieved through generating various random numbers. Here are some more tips for making your work reproducible:\nUsing set.seed() Example of reproducibility in fitting ML models using set.seed()\n#First Line set.seed(1234) #Second Line model_05_rand_forest_ranger \u0026lt;- rand_forest( mode = \u0026quot;regression\u0026quot;, mtry = 4, trees = 1000, min_n = 10 ) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;, splitrule = \u0026quot;extratrees\u0026quot;, importance = \u0026quot;impurity\u0026quot;) %\u0026gt;% fit(price ~ ., data = train_tbl %\u0026gt;% select(-id, -model, -model_tier)) #Third Line model_05_rand_forest_ranger %\u0026gt;% calc_metrics(test_tbl)  Random Numbers Here are several ways to get random numbers. These examples are informed by the R Cookbook, see here\n# get one random number using runif() from base-R, stats package # default 0 to 1 runif(1) # get two random numbers runif(2) # get a vector of three random numbers # increase range beyond the default, -10 to 110 runif(3, min = -10, max = 110) # ensure three random numbers do *not* have decimals # use floor() function to round down floor(runif(3, min = -10, max = 110)) # sample() function does the same thing - using just one function # replace parameter: should sampling be with or without replacement? sample(-10:110, 3, replace = TRUE) # Reproducibility # use set.seed() before any of the aforementioned random number generators set.seed(123) sample(-10:110, 3, replace = FALSE)  Random Numbers from a Normal Distribution # Get five random numbers from a normal distribution # Here the default is mean = 0, standard deviation = 1. rnorm(5) # Change mean and standard deviation away from default rnorm(5, mean = 66, sd = 12) # Ensure reproducibility with set.seed() set.seed(123) rnorm(5, mean = 66, sd = 12) # Ensure normal distribution by setting sufficiently large number with rnorm() # Ensure reproducibility # Plot a histogram set.seed(123) x \u0026lt;- rnorm(500, mean = 66, sd = 12) hist(x)  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2f98573b9b610f1d3926df768ccda2dd","permalink":"/technical_notes/example_tech/rstats_tip1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/rstats_tip1/","section":"technical_notes","summary":"Understanding reproducibility and the set.seed() function in R is best achieved through generating various random numbers. Here are some more tips for making your work reproducible:\nUsing set.seed() Example of reproducibility in fitting ML models using set.","tags":null,"title":"Make your work reproducible","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTechnical Tip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTechnical Tip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"88f0a6e6ab74e40f15b0be6609fd5342","permalink":"/technical_notes/example_tech/technical_notes2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/technical_notes/example_tech/technical_notes2/","section":"technical_notes","summary":"Here are some more tips for getting started with Academic:\nTechnical Tip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Technical Notes Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Introduction  Transactions  Breaking Down Transactions  Blocks  Gas  Introduction Many Ethereum tutorials target developers, but there’s a lack of educational resources for data analyst or for people who wish to see on-chain data without running a client or node.\nThis tutorial helps readers understand fundamental Ethereum concepts including transactions, blocks and gas by querying on-chain data with structured query language (SQL) through an interface provided by Dune Analytics.\nOn-chain data can help us understand Ethereum, the network, and as an economy for computing power and should serve as a base for understanding challenges facing Ethereum today (i.e., rising gas prices) and, more importantly, discussions around scaling solutions.\nTransactions A user’s journey on Ethereum starts with initializing a user-controlled account or an entity with an ETH balance. There are two account types - user-controlled or a smart contract (see ethereum.org).\nAny account can be viewed on a block explorer like Etherscan. Block explorers are a portal to Ethereum’s data. They display, in real-time, data on blocks, transactions, miners, accounts and other on-chain activity (see here).\nHowever, a user may wish to query the data directly to reconcile the information provided by external block explorers. Dune Analytics provides this capability to anyone with some knowledge of SQL.\nFor reference, the smart contract account for the Ethereum Foundation (EF) can be viewed on Etherscan.\nOne thing to note is that all accounts, including the EF’s, has a public address that can be used to send and receive transactions.\nThe account balance on Etherscan comprises regular transactions and internal transactions. Internal transactions, despite the name, are not actual transactions that change the state of the chain. They are value transfers initiated by executing a contract ( source). Since internal transactions have no signature, they are not included on the blockchain and cannot be queried with Dune Analytics.\nTherefore, this tutorial will focus on regular transactions. This can be queried as such:\nWITH temp_table AS ( SELECT hash, block_number, block_time, \u0026quot;from\u0026quot;, \u0026quot;to\u0026quot;, value / 1e18 AS ether, gas_used, gas_price / 1e9 AS gas_price_gwei FROM ethereum.\u0026quot;transactions\u0026quot; WHERE \u0026quot;to\u0026quot; = '\\xde0B295669a9FD93d5F28D9Ec85E40f4cb697BAe' ORDER BY block_time DESC ) SELECT hash, block_number, block_time, \u0026quot;from\u0026quot;, \u0026quot;to\u0026quot;, ether, (gas_used * gas_price_gwei) / 1e9 AS txn_fee FROM temp_table  This will yield the same information as provided on Etherscan\u0026rsquo;s transaction page. For comparison, here are the two sources:\nEtherscan  EF\u0026rsquo;s contract page on Etherscan.\nDune Analytics You can find dashboard here. Click on the table to see the query (also see above).\nBreaking_Down_Transactions A submitted transaction includes several pieces of information including ( source):\n Recipient: The receiving address (queried as \u0026ldquo;to\u0026rdquo;) Signature: While a sender\u0026rsquo;s private keys signs a transaction, what we can query with SQL is a sender\u0026rsquo;s public address (\u0026ldquo;from\u0026rdquo;). Value: This is the amount of ETH transferred (see ether column). Data: This is arbitrary data that\u0026rsquo;s been hashed (see data column) gasLimit: The maximum amount of gas, or the cost of computation, that can be consumed by a transaction (see gas_limit). gasPrice: The fee the sender pays to sign a transaction to the blockchain. Gas is denominated in Gwei which is 0.000000001 ETH (nine decimal places).  We can query these specific pieces of information for transactions to the Ethereum Foundation public address:\nSELECT \u0026quot;to\u0026quot;, \u0026quot;from\u0026quot;, value / 1e18 AS ether, data, gas_limit, gas_price / 1e9 AS gas_price_gwei, gas_used, ROUND(((gas_used / gas_limit) * 100),2) AS gas_used_pct FROM ethereum.\u0026quot;transactions\u0026quot; WHERE \u0026quot;to\u0026quot; = '\\xde0B295669a9FD93d5F28D9Ec85E40f4cb697BAe' ORDER BY block_time DESC  Blocks Each transaction will change the state of the Ethereum virtual machine ( EVM) ( source). Transactions are broadcasted to the network to be verified and included in a block. Each transaction is associated with a block number. To see the data, we could query a specific block number: 12396854 (the most recent block among Ethereum Foundation transactions as of this writing, 11/5/21).\nMoreover, when we query the next two blocks, we can see that each block contains the hash of the previous block (i.e., parent hash), illustrating how the blockchain is formed.\nEach block contains a reference to it parent block. This is shown below between the hash and parent_hash columns ( source):\nHere is the query on Dune Analytics:\nSELECT time, number, difficulty, hash, parent_hash, nonce FROM ethereum.\u0026quot;blocks\u0026quot; WHERE \u0026quot;number\u0026quot; = 12396854 OR \u0026quot;number\u0026quot; = 12396855 OR \u0026quot;number\u0026quot; = 12396856 LIMIT 10  We can examine a block by querying time, block number, difficulty, hash, parent hash, and nonce.\nThe only thing this query does not cover is list of transaction which requires a separate query below and state root. A full or archival node will store all transactions and state transitions, allowing for clients to query the state of the chain at any time. Because this requires large storage space, we can separate chain data from state data:\n Chain data (list of blocks, transactions) State data (result of each transaction’s state transition)  State root falls in the latter and is implicit data (not stored on-chain), while chain data is explicit and stored on the chain itself ( source).\nFor this tutorial, we\u0026rsquo;ll be focusing on on-chain data that can be queried with SQL via Dune Analytics.\nAs stated above, each block contains a list of transactions, we can query this by filtering for a specific block. We\u0026rsquo;ll try the most recent block, 12396854:\nSELECT * FROM ethereum.\u0026quot;transactions\u0026quot; WHERE block_number = 12396854 ORDER BY block_time DESC`  Here\u0026rsquo;s the SQL output on Dune:\nThis single block being added to the chain changes the state of the Ethereum virtual machine ( EVM). Dozens sometimes, hundreds of transactions are verified at once. In this specific case, 222 transactions were included.\nTo see how many were actually successful, we would add another filter to count successful transactions:\nWITH temp_table AS ( SELECT * FROM ethereum.\u0026quot;transactions\u0026quot; WHERE block_number = 12396854 AND success = true ORDER BY block_time DESC ) SELECT COUNT(success) AS num_successful_txn FROM temp_table  For block 12396854, out of 222 total transactions, 204 were successfully verified:\nTransactions requests occur dozens of times per second, but blocks are committed approximately once every 15 seconds ( source).\nTo see that there is one block produced approximately every 15 seconds, we could take the number of seconds in a day (86400) divided by 15 to get an estimate average number of blocks per day (~ 5760).\nThe chart for Ethereum blocks produced per day (2016 - present) is:\nThe average number of blocks produced daily over this time period is ~5,874:\nThe queries are:\n# query to visualize number of blocks produced daily since 2016 SELECT DATE_TRUNC('day', time) AS dt, COUNT(*) AS block_count FROM ethereum.\u0026quot;blocks\u0026quot; GROUP BY dt OFFSET 1 # average number of blocks produced per day WITH temp_table AS ( SELECT DATE_TRUNC('day', time) AS dt, COUNT(*) AS block_count FROM ethereum.\u0026quot;blocks\u0026quot; GROUP BY dt OFFSET 1 ) SELECT AVG(block_count) AS avg_block_count FROM temp_table  The average number of blocks produced per day since 2016 is slightly above that number at 5,874. Alternatively, dividing 86400 seconds by 5874 average blocks comes out to 14.7 seconds or approximately one block every 15 seconds.\nGas Blocks are bounded in size. Each block has a gas limit which is collectively set by miners and the network to prevent arbitrarily large block size to be less of a strain on full node in terms of disk space and speed requirements ( source).\nOne way to conceptualize block gas limit is to think of it as the supply of available block space in which to batch transactions. The block gas limit can be queried and visualized from 2016 to present day:\nSELECT DATE_TRUNC('day', time) AS dt, AVG(gas_limit) AS avg_block_gas_limit FROM ethereum.\u0026quot;blocks\u0026quot; GROUP BY dt OFFSET 1  Then there is the actual gas used daily to pay for computing done on the Ethereum chain (i.e., sending transaction, calling a smart contract, minting an NFT). This is the demand for available Ethereum block space:\nSELECT DATE_TRUNC('day', time) AS dt, AVG(gas_used) AS avg_block_gas_used FROM ethereum.\u0026quot;blocks\u0026quot; GROUP BY dt OFFSET 1  We can also juxtapose these two charts together to see how demand and supply line up:\nTherefore we can understand gas prices as a function of demand for Ethereum block space, given available supply.\nFinally, we may want to query average daily gas prices for the Ethereum chain, however, doing so result in an especially long query time, so we’ll filter our query to the average amount of gas paid per transaction by the Ethereum Foundation.\nWe can see gas prices paid in transaction to the Ethereum Foundation address over the years. Here is the query:\nSELECT block_time, gas_price / 1e9 AS gas_price_gwei, value / 1e18 AS eth_sent FROM ethereum.\u0026quot;transactions\u0026quot; WHERE \u0026quot;to\u0026quot; = '\\xde0B295669a9FD93d5F28D9Ec85E40f4cb697BAe' ORDER BY block_time DESC  Summary With this tutorial, we understand foundational Ethereum concepts and how the Ethereum blockchain works by querying and getting a feel for on-chain data.\nThe dashboard that holds all code used in this tutorial can be found here.\nFor more use of data to explore web3 find me on Twitter.\n","date":1620691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620691200,"objectID":"426009517b1ebb46567f126884df0e2e","permalink":"/post/query_ethereum/","publishdate":"2021-05-11T00:00:00Z","relpermalink":"/post/query_ethereum/","section":"post","summary":"A tutorial to help readers learn how Ethereum works by running SQL queries and getting a feel for on-chain data","tags":["Ethereum","SQL","Dune Analytics","Querying","Analytics"],"title":"Learn Foundational Ethereum Topics with SQL","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Overview  Exploratory Questions  Web Scraping  Data Cleaning  Data Visualization  Text Mining  Overview \u0026ldquo;Let\u0026rsquo;s order Thai.\u0026rdquo;\n\u0026ldquo;Great, what\u0026rsquo;s your go-to dish?\u0026rdquo;\n\u0026ldquo;Pad Thai.”\nThis has bugged me for years and is the genesis for this project.\nPeople need to know they have other choices aside from Pad Thai. Pad Thai is one of 53 individual dishes and stopping there risks missing out on at least 201 shared Thai dishes (source: wikipedia).\nThis project is an opportunity to build a data set of Thai dishes by scraping tables off Wikipedia. We will use Python for web scraping and R for visualization. Web scraping is done in Beautiful Soup (Python) and pre-processed further with dplyr and visualized with ggplot2.\nFurthermore, we\u0026rsquo;ll use the tidytext package in R to explore the names of Thai dishes (in English) to see if we can learn some interest things from text data.\nFinally, there is an opportunity to make an open source contribution.\nThe project repo is here.\nExploratory_Questions The purpose of this analysis is to generate questions.\nBecause exploratory analysis is iterative, these questions were generated in the process of manipulating and visualizing data. We can use these questions to structure the rest of the post:\n How might we organized Thai dishes? What is the best way to organized the different dishes? Which raw material(s) are most popular? Which raw materials are most important? Could you learn about Thai food just from the names of the dishes?  Web_Scraping We scraped over 300 Thai dishes. For each dish, we got:\n Thai name Thai script English name Region Description  First, we\u0026rsquo;ll use the following Python libraries/modules:\nimport requests from bs4 import BeautifulSoup import urllib.request import urllib.parse import urllib.error import ssl import pandas as pd  We\u0026rsquo;ll use requests to send an HTTP requests to the wikipedia url we need. We\u0026rsquo;ll access network sockets using \u0026lsquo;secure sockets layer\u0026rsquo; (SSL). Then we\u0026rsquo;ll read in the html data to parse it with Beautiful Soup.\nBefore using Beautiful Soup, we want to understand the structure of the page (and tables) we want to scrape under inspect element on the browser (note: I used Chrome). We can see that we want the table tag, along with class of wikitable sortable.\nThe main function we\u0026rsquo;ll use from Beautiful Soup is findAll() and the three parameters are th (Header Cell in HTML table), tr (Row in HTML table) and td (Standard Data Cell).\nFirst, we\u0026rsquo;ll save the table headers in a list, which we\u0026rsquo;ll use when creating an empty dictionary to store the data we need.\nheader = [item.text.rstrip() for item in all_tables[0].findAll('th')] table = dict([(x, 0) for x in header])  Initially, we want to scrape one table, knowing that we\u0026rsquo;ll need to repeat the process for all 16 tables. Therefore we\u0026rsquo;ll use a nested loop. Because all tables have 6 columns, we\u0026rsquo;ll want to create 6 empty lists.\nWe\u0026rsquo;ll scrape through all table rows tr and check for 6 cells (which we should have for 6 columns), then we\u0026rsquo;ll append the data to each empty list we created.\n# loop through all 16 tables a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # 6 empty list (for 6 columns) to store data a1 = [] a2 = [] a3 = [] a4 = [] a5 = [] a6 = [] # nested loop for looping through all 16 tables, then all tables individually for i in a: for row in all_tables[i].findAll('tr'): cells = row.findAll('td') if len(cells) == 6: a1.append([string for string in cells[0].strings]) a2.append(cells[1].find(text=True)) a3.append(cells[2].find(text=True)) a4.append(cells[3].find(text=True)) a5.append(cells[4].find(text=True)) a6.append([string for string in cells[5].strings])  You\u0026rsquo;ll note the code for a1 and a6 are slightly different. In retrospect, I found that cells[0].find(text=True) did not yield certain texts, particularly if they were links, therefore a slight adjustment is made.\nThe strings tag returns a NavigableString type object while text returns a unicode object (see stack overflow explanation).\nAfter we\u0026rsquo;ve scrapped the data, we\u0026rsquo;ll need to store the data in a dictionary before converting to data frame:\n# create dictionary table = dict([(x, 0) for x in header]) # append dictionary with corresponding data list table['Thai name'] = a1 table['Thai script'] = a2 table['English name'] = a3 table['Image'] = a4 table['Region'] = a5 table['Description'] = a6 # turn dict into dataframe df_table = pd.DataFrame(table)  For a1 and a6, we need to do an extra step of joining the strings together, so I\u0026rsquo;ve created two additional corresponding columns, Thai name 2 and Description2:\n# Need to Flatten Two Columns: 'Thai name' and 'Description' # Create two new columns df_table['Thai name 2'] = \u0026quot;\u0026quot; df_table['Description2'] = \u0026quot;\u0026quot; # join all words in the list for each of 328 rows and set to thai_dishes['Description2'] column # automatically flatten the list df_table['Description2'] = [ ' '.join(cell) for cell in df_table['Description']] df_table['Thai name 2'] = [ ' '.join(cell) for cell in df_table['Thai name']]  After we\u0026rsquo;ve scrapped all the data and converted from dictionary to data frame, we\u0026rsquo;ll write to CSV to prepare for data cleaning in R (note: I saved the csv as thai_dishes.csv, but you can choose a different name).\nData_Cleaning Data cleaning is typically non-linear.\nWe\u0026rsquo;ll manipulate the data to explore, learn about the data and see that certain things need cleaning or, in some cases, going back to Python to re-scrape. The columns a1 and a6 were scraped differently from other columns due to missing data found during exploration and cleaning.\nFor certain links, using .find(text=True) did not work as intended, so a slight adjustment was made.\nFor this post, R is the tool of choice for cleaning the data.\nHere are other data cleaning tasks:\n Changing column names (snake case)  # read data df \u0026lt;- read_csv(\u0026quot;thai_dishes.csv\u0026quot;) # change column name df \u0026lt;- df %\u0026gt;% rename( Thai_name = `Thai name`, Thai_name_2 = `Thai name 2`, Thai_script = `Thai script`, English_name = `English name` )   Remove newline escape sequence (\\n)  # remove \\n from all columns ---- df$Thai_name \u0026lt;- gsub(\u0026quot;[\\n]\u0026quot;, \u0026quot;\u0026quot;, df$Thai_name) df$Thai_name_2 \u0026lt;- gsub(\u0026quot;[\\n]\u0026quot;, \u0026quot;\u0026quot;, df$Thai_name_2) df$Thai_script \u0026lt;- gsub(\u0026quot;[\\n]\u0026quot;, \u0026quot;\u0026quot;, df$Thai_script) df$English_name \u0026lt;- gsub(\u0026quot;[\\n]\u0026quot;, \u0026quot;\u0026quot;, df$English_name) df$Image \u0026lt;- gsub(\u0026quot;[\\n]\u0026quot;, \u0026quot;\u0026quot;, df$Image) df$Region \u0026lt;- gsub(\u0026quot;[\\n]\u0026quot;, \u0026quot;\u0026quot;, df$Region) df$Description \u0026lt;- gsub(\u0026quot;[\\n]\u0026quot;, \u0026quot;\u0026quot;, df$Description) df$Description2 \u0026lt;- gsub(\u0026quot;[\\n]\u0026quot;, \u0026quot;\u0026quot;, df$Description2)   Add/Mutate new columns (major_groupings, minor_groupings):  # Add Major AND Minor Groupings ---- df \u0026lt;- df %\u0026gt;% mutate( major_grouping = as.character(NA), minor_grouping = as.character(NA) )   Edit rows for missing data in Thai_name column: 26, 110, 157, 234-238, 240, 241, 246  Note: This was only necessary the first time round, after the changes are made to how I scraped a1 and a6, this step is no longer necessary:\n# If necessary; may not need to do this after scraping a1 and a6 - see above # Edit Rows for missing Thai_name df[26,]$Thai_name \u0026lt;- \u0026quot;Khanom chin nam ngiao\u0026quot; df[110,]$Thai_name \u0026lt;- \u0026quot;Lap Lanna\u0026quot; df[157,]$Thai_name \u0026lt;- \u0026quot;Kai phat khing\u0026quot; df[234,]$Thai_name \u0026lt;- \u0026quot;Nam chim chaeo\u0026quot; df[235,]$Thai_name \u0026lt;- \u0026quot;Nam chim kai\u0026quot; df[236,]$Thai_name \u0026lt;- \u0026quot;Nam chim paesa\u0026quot; df[237,]$Thai_name \u0026lt;- \u0026quot;Nam chim sate\u0026quot; df[238,]$Thai_name \u0026lt;- \u0026quot;Nam phrik i-ke\u0026quot; df[240,]$Thai_name \u0026lt;- \u0026quot;Nam phrik kha\u0026quot; df[241,]$Thai_name \u0026lt;- \u0026quot;Nam phrik khaep mu\u0026quot; df[246,]$Thai_name \u0026lt;- \u0026quot;Nam phrik pla chi\u0026quot;   save to \u0026ldquo;edit_thai_dishes.csv\u0026rdquo;  # Write new csv to save edits made to data frame write_csv(df, \u0026quot;edit_thai_dishes.csv\u0026quot;)  Data_Visualization There are several ways to visualize the data. Because we want to communicate the diversity of Thai dishes, aside from Pad Thai, we want a visualization that captures the many, many options.\nI opted for a dendrogram. This graph assumes hierarchy within the data, which fits our project because we can organize the dishes in grouping and sub-grouping.\nHow might we organized Thai dishes? We first make a distinction between individual and shared dishes to show that Pad Thai is not even close to being the best individual dish. And, in fact, more dishes fall under the shared grouping.\nTo avoid cramming too much data into one visual, we\u0026rsquo;ll create two separate visualizations for individual vs. shared dishes.\nHere is the first dendrogram representing 52 individual dish alternatives to Pad Thai.\nCreating a dendrogram requires using the ggraph and igraph libraries. First, we\u0026rsquo;ll load the libraries and sub-set our data frame by filtering for Individual Dishes:\ndf \u0026lt;- read_csv(\u0026quot;edit_thai_dishes.csv\u0026quot;) library(ggraph) library(igraph) df %\u0026gt;% select(major_grouping, minor_grouping, Thai_name, Thai_script) %\u0026gt;% filter(major_grouping == 'Individual dishes') %\u0026gt;% group_by(minor_grouping) %\u0026gt;% count()  We create edges and nodes (i.e., from and to) to create the sub-groupings within Individual Dishes (i.e., Rice, Noodles and Misc):\n# Individual Dishes ---- # data: edge list d1 \u0026lt;- data.frame(from=\u0026quot;Individual dishes\u0026quot;, to=c(\u0026quot;Misc Indiv\u0026quot;, \u0026quot;Noodle dishes\u0026quot;, \u0026quot;Rice dishes\u0026quot;)) d2 \u0026lt;- df %\u0026gt;% select(minor_grouping, Thai_name) %\u0026gt;% slice(1:53) %\u0026gt;% rename( from = minor_grouping, to = Thai_name ) edges \u0026lt;- rbind(d1, d2) # plot dendrogram (idividual dishes) indiv_dishes_graph \u0026lt;- graph_from_data_frame(edges) ggraph(indiv_dishes_graph, layout = \u0026quot;dendrogram\u0026quot;, circular = FALSE) + geom_edge_diagonal(aes(edge_colour = edges$from), label_dodge = NULL) + geom_node_text(aes(label = name, filter = leaf, color = 'red'), hjust = 1.1, size = 3) + geom_node_point(color = \u0026quot;whitesmoke\u0026quot;) + theme( plot.background = element_rect(fill = '#343d46'), panel.background = element_rect(fill = '#343d46'), legend.position = 'none', plot.title = element_text(colour = 'whitesmoke', face = 'bold', size = 25), plot.subtitle = element_text(colour = 'whitesmoke', face = 'bold'), plot.caption = element_text(color = 'whitesmoke', face = 'italic') ) + labs( title = '52 Alternatives to Pad Thai', subtitle = 'Individual Thai Dishes', caption = 'Data: Wikipedia | Graphic: @paulapivat' ) + expand_limits(x = c(-1.5, 1.5), y = c(-0.8, 0.8)) + coord_flip() + annotate(\u0026quot;text\u0026quot;, x = 47, y = 1, label = \u0026quot;Miscellaneous (7)\u0026quot;, color = \u0026quot;#7CAE00\u0026quot;)+ annotate(\u0026quot;text\u0026quot;, x = 31, y = 1, label = \u0026quot;Noodle Dishes (24)\u0026quot;, color = \u0026quot;#00C08B\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 8, y = 1, label = \u0026quot;Rice Dishes (22)\u0026quot;, color = \u0026quot;#C77CFF\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 26, y = 2, label = \u0026quot;Individual\\nDishes\u0026quot;, color = \u0026quot;#F8766D\u0026quot;)  What is the best way to organized the different dishes? There are approximately 4X as many shared dishes as individual dishes, so the dendrogram should be circular to fit the names of all dishes in one graphic.\nA wonderful resource I use regularly for these types of visuals is the R Graph Gallery. There was a slight issue in how the text angles were calculated so I submitted a PR to fix.\nPerhaps distinguishing between individual and shared dishes is too crude, within the dendrogram for 201 shared Thai dishes, we can see further sub-groupings including Curries, Sauces/Pastes, Steamed, Grilled, Deep-Fried, Fried \u0026amp; Stir-Fried, Salads, Soups and other Misc:\n# Shared Dishes ---- df %\u0026gt;% select(major_grouping, minor_grouping, Thai_name, Thai_script) %\u0026gt;% filter(major_grouping == 'Shared dishes') %\u0026gt;% group_by(minor_grouping) %\u0026gt;% count() %\u0026gt;% arrange(desc(n)) d3 \u0026lt;- data.frame(from=\u0026quot;Shared dishes\u0026quot;, to=c(\u0026quot;Curries\u0026quot;, \u0026quot;Soups\u0026quot;, \u0026quot;Salads\u0026quot;, \u0026quot;Fried and stir-fried dishes\u0026quot;, \u0026quot;Deep-fried dishes\u0026quot;, \u0026quot;Grilled dishes\u0026quot;, \u0026quot;Steamed or blanched dishes\u0026quot;, \u0026quot;Stewed dishes\u0026quot;, \u0026quot;Dipping sauces and pastes\u0026quot;, \u0026quot;Misc Shared\u0026quot;)) d4 \u0026lt;- df %\u0026gt;% select(minor_grouping, Thai_name) %\u0026gt;% slice(54:254) %\u0026gt;% rename( from = minor_grouping, to = Thai_name ) edges2 \u0026lt;- rbind(d3, d4) # create a vertices data.frame. One line per object of hierarchy vertices = data.frame( name = unique(c(as.character(edges2$from), as.character(edges2$to))) ) # add column with group of each name. Useful to later color points vertices$group = edges2$from[ match(vertices$name, edges2$to)] # Add information concerning the label we are going to add: angle, horizontal adjustment and potential flip # calculate the ANGLE of the labels vertices$id=NA myleaves=which(is.na(match(vertices$name, edges2$from))) nleaves=length(myleaves) vertices$id[myleaves] = seq(1:nleaves) vertices$angle = 360 / nleaves * vertices$id + 90 # calculate the alignment of labels: right or left vertices$hjust\u0026lt;-ifelse( vertices$angle \u0026lt; 275, 1, 0) # flip angle BY to make them readable vertices$angle\u0026lt;-ifelse(vertices$angle \u0026lt; 275, vertices$angle+180, vertices$angle) # plot dendrogram (shared dishes) shared_dishes_graph \u0026lt;- graph_from_data_frame(edges2) ggraph(shared_dishes_graph, layout = \u0026quot;dendrogram\u0026quot;, circular = TRUE) + geom_edge_diagonal(aes(edge_colour = edges2$from), label_dodge = NULL) + geom_node_text(aes(x = x*1.15, y=y*1.15, filter = leaf, label=name, angle = vertices$angle, hjust= vertices$hjust, colour= vertices$group), size=2.7, alpha=1) + geom_node_point(color = \u0026quot;whitesmoke\u0026quot;) + theme( plot.background = element_rect(fill = '#343d46'), panel.background = element_rect(fill = '#343d46'), legend.position = 'none', plot.title = element_text(colour = 'whitesmoke', face = 'bold', size = 25), plot.subtitle = element_text(colour = 'whitesmoke', margin = margin(0,0,30,0), size = 20), plot.caption = element_text(color = 'whitesmoke', face = 'italic') ) + labs( title = 'Thai Food is Best Shared', subtitle = '201 Ways to Make Friends', caption = 'Data: Wikipedia | Graphic: @paulapivat' ) + #expand_limits(x = c(-1.5, 1.5), y = c(-0.8, 0.8)) + expand_limits(x = c(-1.5, 1.5), y = c(-1.5, 1.5)) + coord_flip() + annotate(\u0026quot;text\u0026quot;, x = 0.4, y = 0.45, label = \u0026quot;Steamed\u0026quot;, color = \u0026quot;#F564E3\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 0.2, y = 0.5, label = \u0026quot;Grilled\u0026quot;, color = \u0026quot;#00BA38\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = -0.2, y = 0.5, label = \u0026quot;Deep-Fried\u0026quot;, color = \u0026quot;#DE8C00\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = -0.4, y = 0.1, label = \u0026quot;Fried \u0026amp;\\n Stir-Fried\u0026quot;, color = \u0026quot;#7CAE00\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = -0.3, y = -0.4, label = \u0026quot;Salads\u0026quot;, color = \u0026quot;#00B4F0\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = -0.05, y = -0.5, label = \u0026quot;Soups\u0026quot;, color = \u0026quot;#C77CFF\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 0.3, y = -0.5, label = \u0026quot;Curries\u0026quot;, color = \u0026quot;#F8766D\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 0.5, y = -0.1, label = \u0026quot;Misc\u0026quot;, color = \u0026quot;#00BFC4\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 0.5, y = 0.1, label = \u0026quot;Sauces\\nPastes\u0026quot;, color = \u0026quot;#B79F00\u0026quot;)  Text_Mining Which raw material(s) are most popular? One way to answer this question is to use text mining to tokenize by either word and count the words by frequency as one measure of popularity.\nIn the below bar chart, we see frequency of words across all Thai Dishes. Mu (หมู) which means pork in Thai appears most frequently across all dish types and sub-grouping. Next we have kaeng (แกง) which means curry. Phat (ผัด) comings in third suggesting \u0026ldquo;stir-fry\u0026rdquo; is a popular cooking mode.\nAs we can see not all words refer to raw materials, so we may not be able to answer this question directly.\nlibrary(tidytext) library(scales) # new csv file after data cleaning (see above) df \u0026lt;- read_csv(\u0026quot;../web_scraping/edit_thai_dishes.csv\u0026quot;) df %\u0026gt;% select(Thai_name, Thai_script) %\u0026gt;% # can substitute 'word' for ngrams, sentences, lines unnest_tokens(ngrams, Thai_name) %\u0026gt;% # to reference thai spelling: group_by(Thai_script) group_by(ngrams) %\u0026gt;% tally(sort = TRUE) %\u0026gt;% # alt: count(sort = TRUE) filter(n \u0026gt; 9) %\u0026gt;% # visualize # pipe directly into ggplot2, because using tidytools ggplot(aes(x = n, y = reorder(ngrams, n))) + geom_col(aes(fill = ngrams)) + scale_fill_manual(values = c( \u0026quot;#c3d66b\u0026quot;, \u0026quot;#70290a\u0026quot;, \u0026quot;#2f1c0b\u0026quot;, \u0026quot;#ba9d8f\u0026quot;, \u0026quot;#dda37b\u0026quot;, \u0026quot;#8f5e23\u0026quot;, \u0026quot;#96b224\u0026quot;, \u0026quot;#dbcac9\u0026quot;, \u0026quot;#626817\u0026quot;, \u0026quot;#a67e5f\u0026quot;, \u0026quot;#be7825\u0026quot;, \u0026quot;#446206\u0026quot;, \u0026quot;#c8910b\u0026quot;, \u0026quot;#88821b\u0026quot;, \u0026quot;#313d5f\u0026quot;, \u0026quot;#73869a\u0026quot;, \u0026quot;#6f370f\u0026quot;, \u0026quot;#c0580d\u0026quot;, \u0026quot;#e0d639\u0026quot;, \u0026quot;#c9d0ce\u0026quot;, \u0026quot;#ebf1f0\u0026quot;, \u0026quot;#50607b\u0026quot; )) + theme_minimal() + theme(legend.position = \u0026quot;none\u0026quot;) + labs( x = \u0026quot;Frequency\u0026quot;, y = \u0026quot;Words\u0026quot;, title = \u0026quot;Frequency of Words in Thai Cuisine\u0026quot;, subtitle = \u0026quot;Words appearing at least 10 times in Individual or Shared Dishes\u0026quot;, caption = \u0026quot;Data: Wikipedia | Graphic: @paulapivat\u0026quot; )  We can also see words common to both Individual and Shared Dishes. We see other words like nuea (beef), phrik (chili) and kaphrao (basil leaves).\n# frequency for Thai_dishes (Major Grouping) ---- # comparing Individual and Shared Dishes (Major Grouping) thai_name_freq \u0026lt;- df %\u0026gt;% select(Thai_name, Thai_script, major_grouping) %\u0026gt;% unnest_tokens(ngrams, Thai_name) %\u0026gt;% count(ngrams, major_grouping) %\u0026gt;% group_by(major_grouping) %\u0026gt;% mutate(proportion = n / sum(n)) %\u0026gt;% select(major_grouping, ngrams, proportion) %\u0026gt;% spread(major_grouping, proportion) %\u0026gt;% gather(major_grouping, proportion, c(`Shared dishes`)) %\u0026gt;% select(ngrams, `Individual dishes`, major_grouping, proportion) # Expect warming message about missing values ggplot(thai_name_freq, aes(x = proportion, y = `Individual dishes`, color = abs(`Individual dishes` - proportion))) + geom_abline(color = 'gray40', lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + geom_text(aes(label = ngrams), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.01), low = \u0026quot;red\u0026quot;, high = \u0026quot;blue\u0026quot;) + # low = \u0026quot;darkslategray4\u0026quot;, high = \u0026quot;gray75\u0026quot; theme_minimal() + theme(legend.position = \u0026quot;none\u0026quot;, legend.text = element_text(angle = 45, hjust = 1)) + labs(y = \u0026quot;Individual Dishes\u0026quot;, x = \u0026quot;Shared Dishes\u0026quot;, color = NULL, title = \u0026quot;Comparing Word Frequencies in the names Thai Dishes\u0026quot;, subtitle = \u0026quot;Individual and Shared Dishes\u0026quot;, caption = \u0026quot;Data: Wikipedia | Graphics: @paulapivat\u0026quot;)  Which raw materials are most important? We can only learn so much from frequency, so text mining practitioners have created term frequency - inverse document frequency to better reflect how important a word is in a document or corpus (further details here).\nAgain, the words don\u0026rsquo;t necessarily refer to raw materials, so this question can\u0026rsquo;t be fully answered directly here.\nCould you learn about Thai food just from the names of the dishes? The short answer is \u0026ldquo;yes\u0026rdquo;.\nWe learned just from frequency and \u0026ldquo;term frequency - inverse document frequency\u0026rdquo; not only the most frequent words, but the relative importance within the current set of words that we have tokenized with tidytext. This informs us of not only popular raw materials (Pork), but also dish types (Curries) and other popular mode of preparation (Stir-Fry).\nWe can even examine the network of relationships between words. Darker arrows suggest a stronger relationship between pairs of words, for example \u0026ldquo;nam phrik\u0026rdquo; is a strong pairing. This means \u0026ldquo;chili sauce\u0026rdquo; in Thai and suggests the important role that it plays across many types of dishes.\nWe learned above that \u0026ldquo;mu\u0026rdquo; (pork) appears frequently. Now we see that \u0026ldquo;mu\u0026rdquo; and \u0026ldquo;krop\u0026rdquo; are more related than other pairings (note: \u0026ldquo;mu krop\u0026rdquo; means \u0026ldquo;crispy pork\u0026rdquo;). We also saw above that \u0026ldquo;khao\u0026rdquo; appears frequently in Rice dishes. This alone is not surprising as \u0026ldquo;khao\u0026rdquo; means rice in Thai, but we see here \u0026ldquo;khao phat\u0026rdquo; is strongly related suggesting that fried rice (\u0026ldquo;khao phat\u0026rdquo;) is quite popular.\n# Visualizing a network of Bi-grams with {ggraph} ---- library(igraph) library(ggraph) set.seed(2021) thai_dish_bigram_counts \u0026lt;- df %\u0026gt;% select(Thai_name, minor_grouping) %\u0026gt;% unnest_tokens(bigram, Thai_name, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% separate(bigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% count(word1, word2, sort = TRUE) # filter for relatively common combinations (n \u0026gt; 2) thai_dish_bigram_graph \u0026lt;- thai_dish_bigram_counts %\u0026gt;% filter(n \u0026gt; 2) %\u0026gt;% graph_from_data_frame() # polishing operations to make a better looking graph a \u0026lt;- grid::arrow(type = \u0026quot;closed\u0026quot;, length = unit(.15, \u0026quot;inches\u0026quot;)) set.seed(2021) ggraph(thai_dish_bigram_graph, layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, 'inches')) + geom_node_point(color = \u0026quot;dodgerblue\u0026quot;, size = 5, alpha = 0.7) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + labs( title = \u0026quot;Network of Relations between Word Pairs\u0026quot;, subtitle = \u0026quot;{ggraph}: common nodes in Thai food\u0026quot;, caption = \u0026quot;Data: Wikipedia | Graphics: @paulapivat\u0026quot; ) + theme_void()  Finally, we may be interested in word relationships within individual dishes.\nThe below graph shows a network of word pairs with moderate-to-high correlations. We can see certain words clustered near each other with relatively dark lines: kaeng (curry), pet (spicy), wan (sweet), khiao (green curry), phrik (chili) and mu (pork). These words represent a collection of ingredient, mode of cooking and description that are generally combined.\nset.seed(2021) # Individual Dishes individual_dish_words \u0026lt;- df %\u0026gt;% select(major_grouping, Thai_name) %\u0026gt;% filter(major_grouping == 'Individual dishes') %\u0026gt;% mutate(section = row_number() %/% 10) %\u0026gt;% filter(section \u0026gt; 0) %\u0026gt;% unnest_tokens(word, Thai_name) # assume no stop words individual_dish_cors \u0026lt;- individual_dish_words %\u0026gt;% group_by(word) %\u0026gt;% filter(n() \u0026gt;= 2) %\u0026gt;% # looking for co-occuring words, so must be 2 or greater pairwise_cor(word, section, sort = TRUE) individual_dish_cors %\u0026gt;% filter(correlation \u0026lt; -0.40) %\u0026gt;% graph_from_data_frame() %\u0026gt;% ggraph(layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = correlation, size = correlation), show.legend = TRUE) + geom_node_point(color = \u0026quot;green\u0026quot;, size = 5, alpha = 0.5) + geom_node_text(aes(label = name), repel = TRUE) + labs( title = \u0026quot;Word Pairs in Individual Dishes\u0026quot;, subtitle = \u0026quot;{ggraph}: Negatively correlated (r = -0.4)\u0026quot;, caption = \u0026quot;Data: Wikipedia | Graphics: @paulapivat\u0026quot; ) + theme_void()  Summary We have completed an exploratory data project where we scraped, clean, manipulated and visualized data using a combination of Python and R. We also used the tidytext package for basic text mining task to see if we could gain some insights into Thai cuisine using words from dish names scraped off Wikipedia.\nFor more content on data science, R, Python, SQL and more, find me on Twitter.\n","date":1616025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616025600,"objectID":"99a9ff86a2ff9b6e25094a1c95796380","permalink":"/post/thai_dishes_project/","publishdate":"2021-03-18T00:00:00Z","relpermalink":"/post/thai_dishes_project/","section":"post","summary":"Using R and Python to scrape, pre-process, wrangle and visualize data.","tags":["Web Scraping","Data Viz\"","Text Mining","RStats","ggplot2","Python"],"title":"Pad Thai is a Terrible Choice","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Data  Implication  Overview In this post, I use Python and R to access, parse, manipulate, then visualize data from Crypto51.app to show the strong relationship between Market Capitalization and Cost to Attack among public crypto networks.\nThe more a network is thought to be worth, the more expensive it is to attack. An important, but often overlooked reason to celebrate price gains.\nData In this post, I query an API endpoint setup at Crypto51.app to get JSON data. Then, I use Python to parse and convert to dataframe. Finally, I use R to wrangle and visualize.\nLet\u0026rsquo;s get it!\nHere is the Python code to read in JSON and convert to a dataframe:\nimport pandas as pd import json import requests r = requests.get('https://api.crypto51.app/coins.json') dct = dict() dct = r.json() # loop through: # last_updated # coins for x, y in dct.items(): print(x) type(dct['coins']) # list len(dct['coins']) # 57 dictionaries in side this list # convert list of 57 dictionaries into a pandas dataframe df = pd.DataFrame.from_dict(dct['coins']) df.head() df.to_csv('crypto51.csv', index=False)  After creating a CSV, I\u0026rsquo;m transition to R, out of preference for dataframe manipulation and visualization with this tool (you could do the following in pandas and seaborn).\nWe\u0026rsquo;ll load the tidyverse and read in the CSV file we created. Then we\u0026rsquo;ll use a series of magrittr pipes to sequence our data manipulation in one flow. We\u0026rsquo;ll remove projects with no market_cap data. We\u0026rsquo;ll remove the Handshake project because of missing data for attack_hourly_cost.\nWe\u0026rsquo;ll change attack_hourly_cost data type into numeric. Then we\u0026rsquo;ll use ggplot2 to visualize a scatter plot with both X and Y axes transformed with scale_*_log10() to make the scatter plot more interpretable.\nBitcoin and Ethereum are annotated as the two leading projects (see chart below).\nlibrary(tidyverse) df \u0026lt;- read_csv(\u0026quot;crypto51.csv\u0026quot;) df %\u0026gt;% # remove projects with no market_cap slice(1:38) %\u0026gt;% filter(attack_hourly_cost != \u0026quot;?\u0026quot;) %\u0026gt;% # change character to numeric mutate( attack_hourly_cost = as.numeric(attack_hourly_cost) ) %\u0026gt;% ggplot(aes(x=market_cap, y=attack_hourly_cost)) + geom_point(aes(size = log10(market_cap)), color = \u0026quot;white\u0026quot;, alpha = 0.8) + # use log10 transformation to make chart more interpretable scale_y_log10(label= scales::dollar) + scale_x_log10(label= scales::dollar) + theme_minimal() + theme( legend.position = 'none', panel.background = element_rect(fill = \u0026quot;dodger blue\u0026quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.background = element_rect(fill = \u0026quot;dodger blue\u0026quot;), plot.title = element_text(colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 30, margin = margin(10,0,30,0)), plot.caption = element_text(color = \u0026quot;white\u0026quot;), axis.title = element_text(colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;), axis.title.x = element_text(margin = margin(30,0,10,0)), axis.text = element_text(colour = \u0026quot;white\u0026quot;, face = \u0026quot;bold\u0026quot;), axis.title.y = element_text(margin = margin(0,20,0,30), angle = 0) ) + labs( x = \u0026quot;Market Capitalization\u0026quot;, y = \u0026quot;Attack\\nHourly\\nCost\u0026quot;, title = \u0026quot;The More a Crypto Network is Worth,\\n the Harder it is to Attack.\u0026quot;, caption = \u0026quot;Data: www.crypto51.app | Graphics: @paulapivat\u0026quot; ) + # annotate instead of geom_text annotate(\u0026quot;text\u0026quot;, x = 205174310335, y = 800000, label = \u0026quot;Bitcoin\u0026quot;, color = \u0026quot;white\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 30762751140, y = 418437, label = \u0026quot;Ethereum\u0026quot;, color = \u0026quot;white\u0026quot;)  Implication The More a Crypto Network is Worth, the Harder it is to Attack All time high.\nYou\u0026rsquo;d be hard pressed to find three more delicious words than these.\nWhen it comes to crypto, everyone keeps an eye on their portfolio value.\nYour bags aside, there is another reason to celebrate price gains.\nBitcoin\u0026rsquo;s big innovation was making digital transaction difficult to replicate (unlike most digital files that are easily duplicated).\nNodes follow the longest chain as the \u0026ldquo;correct\u0026rdquo; chain. However, this opens things up for any node(s) with more than 51% of the network hashing power to pull shenanigans, such as double-spending. Sending funds to one address on the main chain and the same funds to another address on a different chain.\nMore hardware and hash power allow a node to secretly mine a side chain, which they can later “fool” the rest of the network into accepting.\nSince their inception, Bitcoin and Ethereum have gotten more difficult to mine over time. And when price increases, the capital costs of buying new equipment goes up.\nThis makes it more difficult for any one entity to accumulate too much hash power and pull shenanigans. As a result, the entire network is more secure.\nIn fact, the data provided by crypto51.app suggests a near perfect correlation between Market Capitalization and Cost to Attack, at r = 0.94.\nThe more a crypto network is worth, the more expensive it is to attack.\nAnother reason to celebrate price gains.\nReferences  www.crypto51.app  For more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1612656000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612656000,"objectID":"27b82660a0d6ff7190a4fd4e136d5ea5","permalink":"/post/crypto_price_attack/","publishdate":"2021-02-07T00:00:00Z","relpermalink":"/post/crypto_price_attack/","section":"post","summary":"Using R and Python to read, pre-process, wrangle and visualize data.","tags":["Cryptocurrency","Bitcoin","Ethereum","Data Viz","RStats","ggplot2","Python"],"title":"The More a Network is Worth, the Harder it is to Attack","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Overview  Getting Data  Tokenization  Normalizing Sentences  Frequency  Sentiment Analysis  Data Transformation  Visualization  References  Overview Why Sentiment Analysis? NLP is subfield of linguistic, computer science and artificial intelligence ( wiki), and you could spend years studying it.\nHowever, I wanted a quick dive to a get an intuition for how NLP works, and we\u0026rsquo;ll do that via sentiment analysis, categorizing text by their polarity.\nWe can\u0026rsquo;t help but feel motivated to see insights about our own social media post, so we\u0026rsquo;ll turn to a well known platform.\nHow well does Facebook know us? To find out, I downloaded 14 years of posts to apply text and sentiment analysis. We\u0026rsquo;l use Python to read and parse json data from Facebook.\nWe\u0026rsquo;ll perform tasks such as tokenization and normalization aided by Python\u0026rsquo;s Natural Language Toolkit, NLTK. Then, we\u0026rsquo;ll use the Vader module (Hutto \u0026amp; Gilbert, 2014) for rule-based (lexicon) sentiment analysis.\nFinally, we\u0026rsquo;ll transition our work flow to R and the tidyverse for data manipulation and visualization.\nGetting_Data First, you\u0026rsquo;ll need to download your own Facebook data by following: Setting \u0026amp; Privacy \u0026gt; Setting \u0026gt; Your Facebook Information \u0026gt; Download Your Information \u0026gt; (select) Posts.\nBelow, I named my file your_posts_1.json, but you can change this. We\u0026rsquo;ll use Python\u0026rsquo;s json module read in data. We can get a feel for the data with type and len.\nimport json # load json into python, assign to 'data' with open('your_posts_1.json') as file: data = json.load(file) type(data) # a list type(data[0]) # first object in the list: a dictionary len(data) # my list contains 2166 dictionaries  Here are the Python libraries we use in this post:\nimport pandas as pd from nltk.sentiment.vader import SentimentIntensityAnalyzer from nltk.stem import LancasterStemmer, WordNetLemmatizer # OPTIONAL (more relevant for individual words) from nltk.corpus import stopwords from nltk.probability import FreqDist import re import unicodedata import nltk import json import inflect import matplotlib.pyplot as plt   Natural Language Tookkit is a popular Python platform for working with human language data. While it has over 50 lexical resources, we\u0026rsquo;ll use the Vader Sentiment Lexicon, that is specifically attuned to sentiments expressed in social media.\n Regex (regular expressions) will be used to remove punctuation.\n Unicode Database will be used to remove non-ASCII characters.\n JSON module helps us to read in json from Facebook.\n Inflect helps us to convert numbers to words.\n Pandas is a powerful data manipulation and data analysis tool for when we save our text data into a data frame and write to csv.\nAfter we have our data, we\u0026rsquo;ll dig through to get actual text data (our posts).\nWe\u0026rsquo;ll store this in a list.\nNote: the data key occasionally returns an empty array and we want to skip over those by checking if len(v) \u0026gt; 0.\n# create empty list empty_lst = [] # multiple nested loops to store all post in empty list for dct in data: for k, v in dct.items(): if k == 'data': if len(v) \u0026gt; 0: for k_i, v_i in vee[0].items(): if k_i == 'post': empty_lst.append(v_i) print(\u0026quot;This is the empty list: \u0026quot;, empty_lst) print(\u0026quot;\\nLength of list: \u0026quot;, len(empty_lst))  We now have a list of strings.\nTokenization We\u0026rsquo;ll loop through our list of strings (empty_lst) to tokenize each sentence with nltk.sent_tokenize(). We want to split the text into individual sentences.\nThis yields a list of list, which we\u0026rsquo;ll flatten:\n# - list of list, len: 1762 (each list contain sentences) nested_sent_token = [nltk.sent_tokenize(lst) for lst in empty_lst] # flatten list, len: 3241 flat_sent_token = [item for sublist in nested_sent_token for item in sublist] print(\u0026quot;Flatten sentence token: \u0026quot;, len(flat_sent_token))  Normalizing_Sentences For context on the functions used in this section, check out this article by Matthew Mayo on Text Data Preprocessing.\nFirst, we\u0026rsquo;ll remove non-ASCII characters (remove_non_ascii(words)) including: #, -, ' and ?, among many others. Then we\u0026rsquo;ll lowercase (to_lowercase(words)), remove punctuation (remove_punctuation(words)), replace numbers (replace_numbers(words)), and remove stopwords (remove_stopwords(words)).\nExample stopwords are: your, yours, yourself, yourselves, he, him, his, himself etc.\nThis allows us to have each sentence be on equal playing field.\n# Remove Non-ASCII def remove_non_ascii(words): \u0026quot;\u0026quot;\u0026quot;Remove non-ASCII character from List of tokenized words\u0026quot;\u0026quot;\u0026quot; new_words = [] for word in words: new_word = unicodedata.normalize('NFKD', word).encode( 'ascii', 'ignore').decode('utf-8', 'ignore') new_words.append(new_word) return new_words # To LowerCase def to_lowercase(words): \u0026quot;\u0026quot;\u0026quot;Convert all characters to lowercase from List of tokenized words\u0026quot;\u0026quot;\u0026quot; new_words = [] for word in words: new_word = word.lower() new_words.append(new_word) return new_words # Remove Punctuation , then Re-Plot Frequency Graph def remove_punctuation(words): \u0026quot;\u0026quot;\u0026quot;Remove punctuation from list of tokenized words\u0026quot;\u0026quot;\u0026quot; new_words = [] for word in words: new_word = re.sub(r'[^\\w\\s]', '', word) if new_word != '': new_words.append(new_word) return new_words # Replace Numbers with Textual Representations def replace_numbers(words): \u0026quot;\u0026quot;\u0026quot;Replace all interger occurrences in list of tokenized words with textual representation\u0026quot;\u0026quot;\u0026quot; p = inflect.engine() new_words = [] for word in words: if word.isdigit(): new_word = p.number_to_words(word) new_words.append(new_word) else: new_words.append(word) return new_words # Remove Stopwords def remove_stopwords(words): \u0026quot;\u0026quot;\u0026quot;Remove stop words from list of tokenized words\u0026quot;\u0026quot;\u0026quot; new_words = [] for word in words: if word not in stopwords.words('english'): new_words.append(word) return new_words # Combine all functions into Normalize() function def normalize(words): words = remove_non_ascii(words) words = to_lowercase(words) words = remove_punctuation(words) words = replace_numbers(words) words = remove_stopwords(words) return words  The below screen cap gives us an idea of the difference between sentence normalization vs non-normalization.\nsents = normalize(flat_sent_token) print(\u0026quot;Length of sentences list: \u0026quot;, len(sents)) # 3194  NOTE: The process of stemming and lemmatization makes more sense for individuals words (over sentences), so we won\u0026rsquo;t use them here.\nFrequency You can use the FreqDist() function to get the most common sentences. Then, you could plot a line chart for a visual comparison of the most frequent sentences.\nAlthough simple, counting frequencies can yield some insights.\nfrom nltk.probability import FreqDist # Find frequency of sentence fdist_sent = FreqDist(sents) fdist_sent.most_common(10) # Plot fdist_sent.plot(10)  Sentiment_Analysis We\u0026rsquo;ll use the Vader module from NLTK. Vader stands for:\n Valence, Aware, Dictionary and sEntiment Reasoner.\n We are taking a Rule-based/Lexicon approach to sentiment analysis because we have a fairly large dataset, but lack labeled data to build a robust training set. Thus, Machine Learning would not be ideal for this task.\nTo get an intuition for how the Vader module works, we can visit the github repo to view vader_lexicon.txt ( source). This is a dictionary that has been empirically validated. Sentiment ratings are provided by 10 independent human raters (pre-screened, trained and checked for inter-rater reliability).\nScores range from (-4) Extremely Negative to (4) Extremely Positive, with (0) as Neutral. For example, \u0026ldquo;die\u0026rdquo; is rated -2.9, while \u0026ldquo;dignified\u0026rdquo; has a 2.2 rating. For more details visit their ( repo).\nWe\u0026rsquo;ll create two empty lists to store the sentences and the polarity scores, separately.\nsentiment captures each sentence and sent_scores, which initializes the nltk.sentiment.vader.SentimentIntensityAnalyzer to calculate polarity_score of each sentence (i.e., negative, neutral, positive).\nsentiment2 captures each polarity and value in a list of tuples.\nThe below screen cap should give you a sense of what we have:\nAfter we have appended each sentence (sentiment) and their polarity scores (sentiment2, negative, neutral, positive), we\u0026rsquo;ll create data frames to store these values.\nThen, we\u0026rsquo;ll write the data frames to CSV to transition to R. Note that we set index to false when saving for CSV. Python starts counting at 0, while R starts at 1, so we\u0026rsquo;re better off re-creating the index as a separate column in R.\nNOTE: There are more efficient ways for what I\u0026rsquo;m doing here. My solution is to save two CSV files and move the work flow over to R for further data manipulation and visualization. This is primarily a personal preference for handling data frames and visualizations in R, but I should point out this can be done with pandas and matplotlib.\n# nltk.download('vader_lexicon') sid = SentimentIntensityAnalyzer() sentiment = [] sentiment2 = [] for sent in sents: sent1 = sent sent_scores = sid.polarity_scores(sent1) for x, y in sent_scores.items(): sentiment2.append((x, y)) sentiment.append((sent1, sent_scores)) # print(sentiment) # sentiment cols = ['sentence', 'numbers'] result = pd.DataFrame(sentiment, columns=cols) print(\u0026quot;First five rows of results: \u0026quot;, result.head()) # sentiment2 cols2 = ['label', 'values'] result2 = pd.DataFrame(sentiment2, columns=cols2) print(\u0026quot;First five rows of results2: \u0026quot;, result2.head()) # save to CSV result.to_csv('sent_sentiment.csv', index=False) result2.to_csv('sent_sentiment_2.csv', index=False)  Data_Transformation From this point forward, we\u0026rsquo;ll be using R and the tidyverse for data manipulation and visualization. RStudio is the IDE of choice here. We\u0026rsquo;ll create an R Script to store all our data transformation and visualization process. We should be in the same directory in which the above CSV files were created with pandas.\nWe\u0026rsquo;ll load the two CSV files we saved and the tidyverse library:\nlibrary(tidyverse) # load data df \u0026lt;- read_csv(\u0026quot;sent_sentiment.csv\u0026quot;) df2 \u0026lt;- read_csv('sent_sentiment_2.csv')  We\u0026rsquo;ll create another column that matches the index for the first data frame (sent_sentiment.csv). I save it as df1, but you could overwrite the original df if you wanted.\n# create a unique identifier for each sentence df1 \u0026lt;- df %\u0026gt;% mutate(row = row_number())  Then, for the second data frame (sent_sentiment_2.csv), we\u0026rsquo;ll create another column matching the index, but also use pivot_wider from the tidyr package. NOTE: You\u0026rsquo;ll want to group_by label first, then use mutate to create a unique identifier.\nWe\u0026rsquo;ll then use pivot_wider to ensure that all polarity values (negative, neutral, positive) have their own columns.\nBy creating a unique identifier using mutate and row_number(), we\u0026rsquo;ll be able to join (left_join) by row.\nFinally, I save the operation to df3 which allows me to work off a fresh new data frame for visualization.\n# long-to-wide for df2 # note: first, group by label; then, create a unique identifier for each label then use pivot_wider df3 \u0026lt;- df2 %\u0026gt;% group_by(label) %\u0026gt;% mutate(row = row_number()) %\u0026gt;% pivot_wider(names_from = label, values_from = values) %\u0026gt;% left_join(df1, by = 'row') %\u0026gt;% select(row, sentence, neg:compound, numbers)  Visualization First, we\u0026rsquo;ll visualize the positive and negative polarity scores separately, across all 3194 sentences (your numbers will vary).\nHere are positivity scores:\nHere are negativity scores:\nWhen I sum positivity and negativity scores to get a ratio, it\u0026rsquo;s approximately 568:97 or 5.8x more positive than negative according to the Vader (Valance Aware Dictionary and Sentiment Reasoner).\nThe Vader module will take in every sentence and assign a valence score from -1 (most negative) to 1 (most positive). We can classify sentences as pos (positive), neu (neutral) and neg(negative) or as a composite (compound) score (i.e., normalized, weighted composite score). For more details, see vader-sentiment documentation.\nHere is a chart to see both positive and negative scores together (positive = blue, negative = red, neutral = black).\nFinally, we can also use histograms to see the distribution of negative and positive sentiment among the sentences:\nNon-Normalized Data It turns out the Vader module is fully capable of analyzing sentences with punctuation, word-shape (capitalization for emphasis), slang and even utf-8 encoded emojis.\nSo to see if there would be any difference if we implemented sentiment analysis without normalization, I re-ran all the analyses above.\nHere are the two version of data for comparison. Top for normalization and bottom for non-normalized.\nWhile there are expected slight differences, they are only slight.\nSummary I downloaded 14 years worth of Facebook posts to run a rule-based sentiment analysis and visualize the results, using a combination of Python and R.\nI enjoyed using both for this project and sought to play to their strengths. I found parsing JSON straight-forward with Python, but once we transition to data frames, I was itching to get back to R.\nBecause we lacked labeled data, using a rule-based/lexicon-approach to sentiment analysis made sense. Now that we have a label for valence scores, it may be possible to take a machine learning approach to predict the valence of future posts.\nReferences  Hutto, C.J. \u0026amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.  For more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1611619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611878400,"objectID":"9b6b9e2bf1204ad9daac270b98fe4462","permalink":"/post/sentiment_analysis/","publishdate":"2021-01-26T00:00:00Z","relpermalink":"/post/sentiment_analysis/","section":"post","summary":"Using Python and R to read, pre-process, wrangle and visualize data.","tags":["R","Data Science","Facebook Data","Python","Sentiment Analysis","Text Analysis"],"title":"How Positive are Your Facebook Posts?","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Overview  Exploring Relationships  Overview \u0026amp; Setup This post uses various R libraries and functions to help you explore your Twitter Analytics Data. The first thing to do is download data from analytics.twitter.com. The assumption here is that you\u0026rsquo;re already a Twitter user and have been using for at least 6 months.\nOnce there, you\u0026rsquo;ll click on the Tweets tab, which should bring you to your Tweet activity with the option to Export data:\nOnce you click on Export data, you\u0026rsquo;ll choose \u0026ldquo;By day\u0026rdquo;, which provides your impressions and engagements metrics for everyday (you\u0026rsquo;ll also select the time period, in the drop down menu right next to Export data - the default is \u0026ldquo;Last 28 Days\u0026rdquo;).\nNote: The other option is to choose \u0026ldquo;By Tweet\u0026rdquo; and that will download the text of each Tweet along with associated metrics. We could potentially do fun text analysis with this, but we\u0026rsquo;ll save that for another post.\nFor this post, I downloaded all available data, which goes five months back.\nAfter downloading, you\u0026rsquo;ll want to read in the data and, in our case, combine all five months into one data frame, we\u0026rsquo;ll use the readr package and read_csv() function contained in tidyverse. Then we\u0026rsquo;ll use rbind() to combine five data frames by rows:\nlibrary(tidyverse) # load data from September to mid-January df1 \u0026lt;- read_csv(\u0026quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20200901_20201001_en.csv\u0026quot;) df2 \u0026lt;- read_csv(\u0026quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201001_20201101_en.csv\u0026quot;) df3 \u0026lt;- read_csv(\u0026quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201101_20201201_en.csv\u0026quot;) df4 \u0026lt;- read_csv(\u0026quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201201_20210101_en.csv\u0026quot;) df5 \u0026lt;- read_csv(\u0026quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20210101_20210112_en.csv\u0026quot;) # combining ALL five dataframes into ONE, by rows df \u0026lt;- rbind(df1, df2, df3, df4, df5)  Exploring_Relationships Twitter analytics tracks several metric that are broadly grouped under Engagements, including: retweets, replies, likes, user profile clicks, url clicks, hashtag clicks, detail expands, media views and media engagements.\nThere are other metrics like \u0026ldquo;app opens\u0026rdquo; and \u0026ldquo;promoted engagements\u0026rdquo;, which are services I have not used and so do not have any data available.\nA Guiding Question It\u0026rsquo;s useful to have a guiding question as it helps focus your exploration. Let\u0026rsquo;s say, I was interested in whether one of my tweets prompted a reader to click on my profile. The metric for this is user profile clicks.\nMy initial guiding question for this post is:\n Which metrics are most strongly correlated with User Profile Clicks?\n You could simply use the cor.test() function, which comes with base R, to go one by one between each metric and User Profile Click. For example, below we calculate the correlation between three pairs of variables, User Profile Clicks and retweets, replies and likes, separately. After awhile, this can get tedious.\ncor.test(x = df$`user profile clicks`, y = df$retweets) cor.test(x = df$`user profile clicks`, y = df$replies) cor.test(x = df$`user profile clicks`, y = df$likes)  A quicker way to explore the relationship between pairs of metrics throughout a dataset is to use a correlelogram.\nWe\u0026rsquo;ll start with base R. You\u0026rsquo;ll want to limit the number of variables you visualize so the correlelogram doesn\u0026rsquo;t become too cluttered. Here are four variables that correlate the highest with User Profile Clicks:\n# four columns are selected along with user profile clicks to plot df %\u0026gt;% select(8, 12, 19:20, `user profile clicks`) %\u0026gt;% plot(pch = 20, cex = 1.5, col=\u0026quot;#69b3a2\u0026quot;)  Here\u0026rsquo;s a visual:\nHere are another four metrics with moderate relationships:\ndf %\u0026gt;% select(6:7, 10:11, `user profile clicks`) %\u0026gt;% plot(pch = 20, cex = 1.5, col=\u0026quot;#69b3a2\u0026quot;)  Visually, you can see the moderate relationship scatter plots are more dispersed, with a less identifiable direction.\nWhile base R is dependable, we can get more informative plots with the GGally package. Here are the four highly correlated variables with User Profile Clicks:\nlibrary(GGally) # GGally, Strongest Related df %\u0026gt;% select(8, 12, 19:20, `user profile clicks`) %\u0026gt;% ggpairs( diag = NULL, title = \u0026quot;Strongest Relationships with User Profile Clicks: Sep 2020 - Jan 2021\u0026quot;, axisLabels = c(\u0026quot;internal\u0026quot;), xlab = \u0026quot;Value\u0026quot; )  Here\u0026rsquo;s the correlelogram between the four most highly correlated variables with user profile clicks:\nHere are the moderately correlated variables with User Profile Clicks:\nAs you can see, not only do these provide scatter plots, but they also show the numerical values of the correlation between each pair of variables, which is much more informative than base R.\nNow, its entirely possible that the pattern of correlation in your data is different as the initial patterns we\u0026rsquo;re seeing here are not meant to generalize to a different dataset.\nFor more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1610668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610668800,"objectID":"edbfb0c178681228bd80d10289a4c6f4","permalink":"/post/twitter_analytics/","publishdate":"2021-01-15T00:00:00Z","relpermalink":"/post/twitter_analytics/","section":"post","summary":"Using R to Explore Twitter Data","tags":["R","Data Science","Twitter Analytics","Analytics"],"title":"Analyzing Your Twitter Data","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Overview  Setup  Gradient Descent  From Scratch  Take Away  Overview In this post, we\u0026rsquo;ll explore Gradient Descent from the ground up starting conceptually, then using code to build up our intuition brick by brick.\nWhile this post is part of an ongoing series where I document my progress through Data Science from Scratch by Joel Grus, for this post I am drawing on external sources including Aurélien Geron\u0026rsquo;s Hands-On Machine Learning to provide a context for why and when gradient descent is used.\nWe\u0026rsquo;ll also be using external libraries such as numpy, that are generally avoided in Data Science from Scratch, to help highlight concepts.\nWhile the book introduces gradient descent as a standalone topic, I find it more intuitive to reason about it within the context of a regression problem.\nSetup In any modeling task, there is error, and our objective is minimize the errors so that when we develop models from our training data, we\u0026rsquo;ll have some confidence that the predictions will work in testing and completely new data.\nWe\u0026rsquo;ll train a linear regression model. Our dataset will only have three data points. To create the model, we\u0026rsquo;ll setting up parameters (slope and intercept) that best \u0026ldquo;fits\u0026rdquo; the data (i.e., best-fitting line), for example:\nWe know the values for both x and y, so we can calculate the slope and intercept directly through the normal equation, which is the analytical approach to finding regression coefficients (slope and intercept):\n# Normal Equation import numpy as np import matplotlib.pyplot as plt x = np.array([2, 4, 5]) y = np.array([45, 85, 105]) # computing Normal Equation x_b = np.c_[np.ones((3, 1)), x] # add x0 = 1 to each of three instances theta = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y) # array([ 5., 20.]) theta  The key line is np.linalg.inv() which computes the multiplicative inverse of a matrix.\nOur slope is 20 and intercept is 5 (i.e., theta).\nWe could also have used the more familiar \u0026ldquo;rise over run\u0026rdquo; ((85 - 45) / (4 - 2)) or (40/2) or 20, but we want to illustrate the normal equation which should come in handy when we go beyond the simplistic three data point example.\nWe could also use the LinearRegression class from sklearn to call the least squares (np.linalg.lstsq()) function directly:\n# Linear Regression from sklearn.linear_model import LinearRegression import numpy as np x = np.array([2, 4, 5]) y = np.array([45, 85, 105]) x = x.reshape(-1, 1) # reshape because sklearn expect 2D array x_b = np.c_[np.ones((3, 1)), x] # add x0 = 1 to each of three instances theta, residuals, rank, s = np.linalg.lstsq(x_b, y, rcond=1e-6) # array([ 5., 20.]) print(\u0026quot;theta:\u0026quot;, theta)  This appraoch also yields the slope (20) and intercept (5) directly.\nWe know the parameters of x and y in our example, but we want to see how learning from data would work. Here\u0026rsquo;s the equation we\u0026rsquo;re working with:\ny = 20 * x + 5  And here\u0026rsquo;s what it looks like (intercept = 5, slope = 20)\nGradient_Descent Why? The normal equation and the least squares approach can handle large training sets efficiently, but when your model has a large number of features or too many training instances to fit into memory, gradient descent is an often used alternative.\nMoreover, linear least squares assume the errors have a normal distribution and the relationship in the data is linear (this is where closed-form solutions like the normal equation excel). When the data is non-linear, an iterative solution (gradient descent) can be used.\nWith linear regression we seek to minimize the sum-of-squares differences between the observed data and the predicted values (aka the error), in a non-iterative fashion.\nAlternatively, we use gradient descent to find the slope and intercept that minimizes the average squared error, however, in an iterative fashion.\nUsing Gradient Descent to Fit a Model The process for gradient descent is to start with a random slope and intercept, then compute the gradient of the mean squared error, while adjusting the slope/intercept (theta) in the direction that continues to minimize the error. This is repeated iteratively until we find a point where errors are most minimized.\nNOTE: This section builds heavily on a previous post on linear algebra. You\u0026rsquo;ll want to read this post to get a feel for the functions used to construct the functions we see in this post.\nfrom typing import TypeVar, List, Iterator import math import random import matplotlib.pyplot as plt from typing import Callable from typing import List import numpy as np x = np.array([2, 4, 5]) # instead of putting y directly, we'll use the equation: 20 * x + 5, which is a direct representation of its relationship to x # y = np.array([45, 85, 105]) # both x and y are represented in inputs inputs = [(x, 20 * x + 5) for x in range(2, 6)]  First, we\u0026rsquo;ll start with random values for the slope and intercept; we\u0026rsquo;ll also establish a learning rate, which controls how much a change in the model is warranted in response to the estimated error each time the model parameters (slope and intercept) are updated.\n# 1. start with a random value for slope and intercept theta = [random.uniform(-1, 1), random.uniform(-1, 1)] learning_rate = 0.001  Next, we\u0026rsquo;ll compute the mean of the gradients, then adjust the slope/intercept in the direction of minimizing the gradient, which is based on the error.\nYou\u0026rsquo;ll note that this for-loop has 100 iterations. The more iterations we go through, the more that errors are minimized and the more we approach a slope/intercept where the model \u0026ldquo;fits\u0026rdquo; the data better.\nYou can see in this list, [linear_gradient(x, y, theta) for x, y in inputs], that our linear_gradient function is applied to the known x and y values in the list of tuples, inputs, along with random values for slope/intercept (theta).\nWe multiply each x value with a random value for slope, then add a random value for intercept. This yields the initial prediction. Error is the gap between the initial prediction and actual y values. We minimize the squared error by using its gradient.\n# start with a function that determines the gradient based on the error from a single data point def linear_gradient(x: float, y: float, theta: Vector) -\u0026gt; Vector: slope, intercept = theta predicted = slope * x + intercept # model prediction error = (predicted - y) # error is (predicted - actual) squared_error = error ** 2 # minimize squared error grad = [2 * error * x, 2 * error] # using its gradient return grad  The linear_gradient function along with initial parameters are then passed to vector_mean, which utilize scalar_multiply and vector_sum:\ndef vector_mean(vectors: List[Vector]) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Computes the element-wise average\u0026quot;\u0026quot;\u0026quot; n = len(vectors) return scalar_multiply(1/n, vector_sum(vectors)) def scalar_multiply(c: float, v: Vector) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Multiplies every element by c\u0026quot;\u0026quot;\u0026quot; return [c * v_i for v_i in v] def vector_sum(vectors: List[Vector]) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Sum all corresponding elements (componentwise sum)\u0026quot;\u0026quot;\u0026quot; # Check that vectors is not empty assert vectors, \u0026quot;no vectors provided!\u0026quot; # Check the vectorss are all the same size num_elements = len(vectors[0]) assert all(len(v) == num_elements for v in vectors), \u0026quot;different sizes!\u0026quot; # the i-th element of the result is the sum of every vector[i] return [sum(vector[i] for vector in vectors) for i in range(num_elements)]  This yields the gradient. Then, each gradient_step is deteremined as our function adjusts the initial random theta values (slope/intercept) in the direction that minimizes the error.\ndef gradient_step(v: Vector, gradient: Vector, step_size: float) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Moves `step_size` in the `gradient` direction from `v`\u0026quot;\u0026quot;\u0026quot; assert len(v) == len(gradient) step = scalar_multiply(step_size, gradient) return add(v, step) def add(v: Vector, w: Vector) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Adds corresponding elements\u0026quot;\u0026quot;\u0026quot; assert len(v) == len(w), \u0026quot;vectors must be the same length\u0026quot; return [v_i + w_i for v_i, w_i in zip(v, w)]  All this comes together in this for-loop to print out how the slope and intercept change with each iteration (we start with 100):\nfor epoch in range(100): # start with 100 \u0026lt;--- change this figure to try different iterations # compute the mean of the gradients grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs]) # take a step in that direction theta = gradient_step(theta, grad, -learning_rate) print(epoch, grad, theta) slope, intercept = theta #assert 19.9 \u0026lt; slope \u0026lt; 20.1, \u0026quot;slope should be about 20\u0026quot; #assert 4.9 \u0026lt; intercept \u0026lt; 5.1, \u0026quot;intercept should be about 5\u0026quot; print(\u0026quot;slope\u0026quot;, slope) print(\u0026quot;intercept\u0026quot;, intercept)  Iterative Descent At 100 iterations, the slope is 18.87 and intercept is 4.87 and the gradient is -32.48 (error for the slope) and -8.45 (error for the intercept). These numbers suggest that we need to decrease the slope and intercept from our random starting point, but our emphasis needs to be on decreasing the slope.\nAt 200 iterations, the slope is 19.97 and intercept is 4.86 and the gradient is -1.76 (error for the slope) and -0.48 (error for the intercept). Our errors have been reduced significantly.\nAt 1000 iterations, the slope is 19.97 (not much difference from 200 iterations) and intercept is 5.09 and the gradients are markedly lower at -0.004 (error for the slope) and 0.02 (error for the intercept). Here the errors may not be much different from zero and we are near our optimal point.\nIn summary, the normal equation and regression approaches gave us a slope of 20 and intercept of 5. With gradient descent, we approached these values with each successive iterations, 1000 iterations yielding less error than 100 or 200 iterations.\nFrom_Scratch As mentioned above, the functions used to compute the gradients and adjust the slope/intercept build on functions we explored in this post. Here\u0026rsquo;s a visual showing how the functions we used to iteratively arrive at the slope and intercept through gradient descent was built:\nTake_Away Gradient descent is an optimization technique often used in machine learning and in this post, we built some intuition around how it works by applying it to a simple linear regression problem, favoring code over math (which we\u0026rsquo;ll return to in a later post). Gradient Descent is useful if you are expecting computational complexity due to the number of features or training instances.\nWe placed gradient descent in context, in comparison to a more analytical approach, normal equation and the least squares method, both of which are non-iterative.\nFurthermore, we saw how the functions used in this post can be traced back to a previous post on linear algebra, thus giving us a big picture view of how the building blocks of data science and an intuition for areas we\u0026rsquo;ll need to explore at a deeper, perhaps at a more mathematical, level.\nThis post is part of an ongoing series where I document my progress through Data Science from Scratch by Joel Grus.\nFor more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1608595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608595200,"objectID":"626ffe1ccb20d8a58bdc3ed539075d81","permalink":"/post/dsfs_8/","publishdate":"2020-12-22T00:00:00Z","relpermalink":"/post/dsfs_8/","section":"post","summary":"An overview of Gradient Descent","tags":["Python","Data Science","Machine Learning","Gradient Descent"],"title":"Gradient Descent -- Data Science from Scratch (ch8)","type":"post"},{"authors":[],"categories":[],"content":"   Load Libraries The two main libraries are tidyverse (mostly dplyr so you can just load that if you want) and sunburstR. There are other packages for sunburst plots including: plotly and ggsunburst (of ggplot), but we’ll explore sunburstR in this post.\nlibrary(tidyverse) library(sunburstR)  Load Data \u0026amp; Explore The data is from week 50 of TidyTuesday, exploring the BBC’s top 100 influential women of 2020.\nwomen \u0026lt;- read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-12-08/women.csv\u0026#39;) head(women)  Add Continents The original dataset organized 100 women by category, country, role and description. I found that for employing the sunburst plot, I would want to group countries together by continents.\nI manually added country names to continent vectors, then added a new column to the women dataframe to conditionally add continent name.\nWe could then focus on six continents rather than 65 separate countries.\n# add continent as character vector asia \u0026lt;- c(\u0026#39;Afghanistan\u0026#39;, \u0026#39;Bangladesh\u0026#39;, \u0026#39;China\u0026#39;, \u0026#39;Exiled Uighur from Ghulja (in Chinese, Yining)\u0026#39;, \u0026#39;Hong Kong\u0026#39;, \u0026#39;India\u0026#39;, \u0026#39;Indonesia\u0026#39;, \u0026#39;Iran\u0026#39;, \u0026#39;Iraq/UK\u0026#39;, \u0026#39;Japan\u0026#39;, \u0026#39;Kyrgyzstan\u0026#39;, \u0026#39;Lebanon\u0026#39;, \u0026#39;Malaysia\u0026#39;, \u0026#39;Myanmar\u0026#39;, \u0026#39;Nepal\u0026#39;, \u0026#39;Pakistan\u0026#39;, \u0026#39;Singapore\u0026#39;, \u0026#39;South Korea\u0026#39;, \u0026#39;Syria\u0026#39;, \u0026#39;Thailand\u0026#39;, \u0026#39;UAE\u0026#39;, \u0026#39;Vietnam\u0026#39;, \u0026#39;Yemen\u0026#39;) south_america \u0026lt;- c(\u0026#39;Argentina\u0026#39;, \u0026#39;Brazil\u0026#39;, \u0026#39;Colombia\u0026#39;, \u0026#39;Ecuador\u0026#39;, \u0026#39;Peru\u0026#39;, \u0026#39;Venezuela\u0026#39;) oceania \u0026lt;- c(\u0026#39;Australia\u0026#39;) europe \u0026lt;- c(\u0026#39;Belarus\u0026#39;, \u0026#39;Finland\u0026#39;, \u0026#39;France\u0026#39;, \u0026#39;Germany\u0026#39;, \u0026#39;Italy\u0026#39;, \u0026#39;Netherlands\u0026#39;, \u0026#39;Northern Ireland\u0026#39;, \u0026#39;Norway\u0026#39;, \u0026#39;Republic of Ireland\u0026#39;, \u0026#39;Russia\u0026#39;, \u0026#39;Turkey\u0026#39;, \u0026#39;UK\u0026#39;, \u0026#39;Ukraine\u0026#39;, \u0026#39;Wales, UK\u0026#39;) africa \u0026lt;- c(\u0026#39;Benin\u0026#39;, \u0026#39;DR Congo\u0026#39;, \u0026#39;Egypt\u0026#39;, \u0026#39;Ethiopia\u0026#39;, \u0026#39;Kenya\u0026#39;, \u0026#39;Morocco\u0026#39;, \u0026#39;Mozambique\u0026#39;, \u0026#39;Nigeria\u0026#39;, \u0026#39;Sierra Leone\u0026#39;, \u0026#39;Somalia\u0026#39;, \u0026#39;Somaliland\u0026#39;, \u0026#39;South Africa\u0026#39;, \u0026#39;Tanzania\u0026#39;, \u0026#39;Uganda\u0026#39;, \u0026#39;Zambia\u0026#39;, \u0026#39;Zimbabwe\u0026#39;) north_america \u0026lt;- c(\u0026#39;El Salvador\u0026#39;, \u0026#39;Jamaica\u0026#39;, \u0026#39;Mexico\u0026#39;, \u0026#39;US\u0026#39;) # add new column for continent women \u0026lt;- women %\u0026gt;% mutate(continent = NA) # add continents to women dataframe women$continent \u0026lt;- ifelse(women$country %in% asia, \u0026#39;Asia\u0026#39;, women$continent) women$continent \u0026lt;- ifelse(women$country %in% south_america, \u0026#39;South America\u0026#39;, women$continent) women$continent \u0026lt;- ifelse(women$country %in% oceania, \u0026#39;Oceania\u0026#39;, women$continent) women$continent \u0026lt;- ifelse(women$country %in% europe, \u0026#39;Europe\u0026#39;, women$continent) women$continent \u0026lt;- ifelse(women$country %in% africa, \u0026#39;Africa\u0026#39;, women$continent) women$continent \u0026lt;- ifelse(women$country %in% north_america, \u0026#39;North America\u0026#39;, women$continent) women  Data Wrangling The key to using the sunburstR package with this specific dataset is the wrangling that happens to filter by continents we created above. We’ll also want to get rid of dashes with mutate_at as dashes are structurally needed to render the sunburst plots.\nBelow, I’ve filtered the women data frame into Africa and Asia (the same could be done for North and South America and Europe as well).\nThe two most important operations here are the creation of the path and V2 columns that will later be parameters for rendering the sunburst plots.\n# Filter for Africa africa_name \u0026lt;- women %\u0026gt;% select(continent, category, role, name) %\u0026gt;% # remove dash within dplyr pipe mutate_at(vars(3, 4), funs(gsub(\u0026quot;-\u0026quot;, \u0026quot;\u0026quot;, .))) %\u0026gt;% filter(continent==\u0026#39;Africa\u0026#39;) %\u0026gt;% mutate( path = paste(continent, category, role, name, sep = \u0026quot;-\u0026quot;) ) %\u0026gt;% slice(2:100) %\u0026gt;% mutate( V2 = 1 ) ## Warning: `funs()` is deprecated as of dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. # Filter for Asia asia_name \u0026lt;- women %\u0026gt;% select(continent, category, role, name) %\u0026gt;% # remove dash within dplyr pipe mutate_at(vars(3, 4), funs(gsub(\u0026quot;-\u0026quot;, \u0026quot;\u0026quot;, .))) %\u0026gt;% filter(continent==\u0026#39;Asia\u0026#39;) %\u0026gt;% mutate( path = paste(continent, category, role, name, sep = \u0026quot;-\u0026quot;) ) %\u0026gt;% slice(2:100) %\u0026gt;% mutate( V2 = 1 )  Sunburst: Africa Ultimately, I found the information best presented by continent as the base of the sunburst plot, followed by category, specific roles and the names of each of the 100 women honored by the BBC.\nMoreover, by presenting the data by continent, you can focus on just five specific color as you decide on a palette.\nI wouldn’t recommend trying to pick a color for each role or name; it becomes too unweildy. Just pick five colors for the two inner most rings of the sunburst plot and it’ll shuffle the rest of the colors.\n# Africa sunburst(data = data.frame(xtabs(V2~path, africa_name)), legend = FALSE, colors = c(\u0026quot;D99527\u0026quot;, \u0026quot;6F7239\u0026quot;, \u0026quot;CE4B3C\u0026quot;, \u0026quot;C8AC70\u0026quot;, \u0026quot;018A9D\u0026quot;))      Legend     {\"x\":{\"data\":{\"children\":[{\"name\":\"Africa\",\"children\":[{\"name\":\"Creativity\",\"children\":[{\"name\":\"Artist and curator\",\"children\":[{\"name\":\"Mulenga Kapwepwe\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Musician\",\"children\":[{\"name\":\"Angelique Kidjo\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Rapper\",\"children\":[{\"name\":\"Houda Abouz\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Singer/songwriter\",\"children\":[{\"name\":\"Bulelwa Mkutukana\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Writer and filmmaker\",\"children\":[{\"name\":\"Tsitsi Dangarembga\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"}],\"colname\":\"X2\"},{\"name\":\"Identity\",\"children\":[{\"name\":\"Filmmaker\",\"children\":[{\"name\":\"Uyaiedu IkpeEtim\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Socialjustice activist\",\"children\":[{\"name\":\"Josina Machel\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"}],\"colname\":\"X2\"},{\"name\":\"Knowledge\",\"children\":[{\"name\":\"Activist\",\"children\":[{\"name\":\"Ishtar Lakhani\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Doctor\",\"children\":[{\"name\":\"Jemimah Kariuki\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Journalist\",\"children\":[{\"name\":\"Douce Namwezi N'Ibamba\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Lawyer\",\"children\":[{\"name\":\"Rebeca Gyumi\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Mental health expert\",\"children\":[{\"name\":\"Ethelreda NakimuliMpungu\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"}],\"colname\":\"X2\"},{\"name\":\"Leadership\",\"children\":[{\"name\":\"Activist\",\"children\":[{\"name\":\"Aisha Yesufu\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Campaigner\",\"children\":[{\"name\":\"Nadeen Ashraf\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Climate activist\",\"children\":[{\"name\":\"Vanessa Nakate\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Coptic nun\",\"children\":[{\"name\":\"Maggie Gobran\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Environmental activist\",\"children\":[{\"name\":\"Phyllis Omido\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"FGM educator\",\"children\":[{\"name\":\"Ubah Ali\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Mayor\",\"children\":[{\"name\":\"Yvonne AkiSawyerr\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Peace activist\",\"children\":[{\"name\":\"Ilwad Elman\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"}],\"colname\":\"X2\"}],\"colname\":\"X1\"}],\"name\":\"root\"},\"options\":{\"legendOrder\":null,\"colors\":[\"D99527\",\"6F7239\",\"CE4B3C\",\"C8AC70\",\"018A9D\"],\"valueField\":\"size\",\"percent\":true,\"count\":false,\"explanation\":null,\"breadcrumb\":[],\"legend\":false,\"sortFunction\":null,\"sumNodes\":true}},\"evals\":[],\"jsHooks\":[]}  Sunburst: Asia # Asia sunburst(data = data.frame(xtabs(V2~path, asia_name)), legend = FALSE, colors = c(\u0026quot;#e6e0ae\u0026quot;, \u0026quot;#dfbc5e\u0026quot;, \u0026quot;#ee6146\u0026quot;, \u0026quot;#d73c37\u0026quot;, \u0026quot;#b51f09\u0026quot;))      Legend     {\"x\":{\"data\":{\"children\":[{\"name\":\"Asia\",\"children\":[{\"name\":\"Creativity\",\"children\":[{\"name\":\"Actor\",\"children\":[{\"name\":\"Mahira Khan\",\"size\":1,\"colname\":\"X4\"},{\"name\":\"Michelle Yeoh\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Architect\",\"children\":[{\"name\":\"Chu Kim Duc\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Feminist activist\",\"children\":[{\"name\":\"Nandar\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Filmmaker\",\"children\":[{\"name\":\"Waad alKateab\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Landscape architect\",\"children\":[{\"name\":\"Kotchakorn Voraakhom\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Musician\",\"children\":[{\"name\":\"Isaivani\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Sake master brewer\",\"children\":[{\"name\":\"Miho Imada\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"}],\"colname\":\"X2\"},{\"name\":\"Identity\",\"children\":[{\"name\":\"Activist\",\"children\":[{\"name\":\"Hayat Mirshad\",\"size\":1,\"colname\":\"X4\"},{\"name\":\"Laleh Osmany\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"UN Women ambassador/model\",\"children\":[{\"name\":\"Cindy Bishop\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Writer\",\"children\":[{\"name\":\"Muyesser Abdul’ehed Hendan\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"}],\"colname\":\"X2\"},{\"name\":\"Knowledge\",\"children\":[{\"name\":\"Activist\",\"children\":[{\"name\":\"Febfi Setyawati\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Computational geneticist\",\"children\":[{\"name\":\"Pardis Sabeti\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Crematorium technician\",\"children\":[{\"name\":\"Sapana Roka Magar\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Doctor\",\"children\":[{\"name\":\"Leo YeeSin\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Global health leader\",\"children\":[{\"name\":\"Sania Nishtar\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Microgrid manager\",\"children\":[{\"name\":\"Iman Ghaleb AlHamli\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Minister for Advanced Technologies\",\"children\":[{\"name\":\"Sarah AlAmiri\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Plant virologist\",\"children\":[{\"name\":\"Safaa Kumari\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Public health expert\",\"children\":[{\"name\":\"Nisreen Alwan\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Robotics team leader\",\"children\":[{\"name\":\"Somaya Faruqi\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Teacher\",\"children\":[{\"name\":\"Rima Sultana Rimu\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Writer\",\"children\":[{\"name\":\"Fang Fang\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"}],\"colname\":\"X2\"},{\"name\":\"Leadership\",\"children\":[{\"name\":\"Athlete\",\"children\":[{\"name\":\"Manasi Joshi\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Climate activist\",\"children\":[{\"name\":\"Ridhima Pandey\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Disability activist\",\"children\":[{\"name\":\"Gulnaz Zhuzbaeva\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Environmental campaigner\",\"children\":[{\"name\":\"Salsabila Khairunnisa\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Human rights activist\",\"children\":[{\"name\":\"Nasrin Sotoudeh\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"KDCA Commissioner\",\"children\":[{\"name\":\"Jeong Eunkyeong\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Prodemocracy activist\",\"children\":[{\"name\":\"Agnes Chow\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Protest leader\",\"children\":[{\"name\":\"Bilkis\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"},{\"name\":\"Student activist\",\"children\":[{\"name\":\"Panusaya Sithijirawattanakul\",\"size\":1,\"colname\":\"X4\"}],\"colname\":\"X3\"}],\"colname\":\"X2\"}],\"colname\":\"X1\"}],\"name\":\"root\"},\"options\":{\"legendOrder\":null,\"colors\":[\"#e6e0ae\",\"#dfbc5e\",\"#ee6146\",\"#d73c37\",\"#b51f09\"],\"valueField\":\"size\",\"percent\":true,\"count\":false,\"explanation\":null,\"breadcrumb\":[],\"legend\":false,\"sortFunction\":null,\"sumNodes\":true}},\"evals\":[],\"jsHooks\":[]} And that’s it for visualizing the BBC’s top 100 influential women in 2020 with the sunburstR package.\nFor more content on data science, visualization, in R and Python, find me on Twitter.\n ","date":1608163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608211452,"objectID":"3eaead33fdd85122241f2d85a29588d1","permalink":"/post/sunburst/","publishdate":"2020-12-17T00:00:00Z","relpermalink":"/post/sunburst/","section":"post","summary":"Using the {sunburstR} package to visualize nested data.","tags":["Data Viz","R Markdown","Statistics","RStats"],"title":"BBC Women of 2020","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Central Limit Theorem  Hypothesis Testing  p-Values  Confidence Intervals  Connecting dots with Python  Overview This is a continuation of my progress through Data Science from Scratch by Joel Grus. We\u0026rsquo;ll use a classic coin-flipping example in this post because it is simple to illustrate with both concept and code. The goal of this post is to connect the dots between several concepts including the Central Limit Theorem, Hypothesis Testing, p-Values and confidence intervals, using python to build our intuition.\nCentral_Limit_Theorem Terms like \u0026ldquo;null\u0026rdquo; and \u0026ldquo;alternative\u0026rdquo; hypothesis are used quite frequently, so let\u0026rsquo;s set some context. The \u0026ldquo;null\u0026rdquo; is the default position. The \u0026ldquo;alternative\u0026rdquo;, alt for short, is something we\u0026rsquo;re comparing to the default (null).\nThe classic coin-flipping exercise is to test the fairness off a coin. If a coin is fair, it\u0026rsquo;ll land on heads 50% of the time (and tails 50% of the time). Let\u0026rsquo;s translate into hypothesis testing language:\nNull Hypothesis: Probability of landing on Heads = 0.5.\nAlt Hypothesis: Probability of landing on Heads != 0.5.\nEach coin flip is a Bernoulli trial, which is an experiment with two outcomes - outcome 1, \u0026ldquo;success\u0026rdquo;, (probability p) and outcome 0, \u0026ldquo;fail\u0026rdquo; (probability p - 1). The reason it\u0026rsquo;s a Bernoulli trial is because there are only two outcome with a coin flip (heads or tails). Read more about Bernoulli here.\nHere\u0026rsquo;s the code for a single Bernoulli Trial:\ndef bernoulli_trial(p: float) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot;Returns 1 with probability p and 0 with probability 1-p\u0026quot;\u0026quot;\u0026quot; return 1 if random.random() \u0026lt; p else 0  When you sum the independent Bernoulli trials, you get a Binomial(n,p) random variable, a variable whose possible values have a probability distribution. The central limit theorem says as n or the number of independent Bernoulli trials get large, the Binomial distribution approaches a normal distribution.\nHere\u0026rsquo;s the code for when you sum all the Bernoulli Trials to get a Binomial random variable:\ndef binomial(n: int, p: float) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot;Returns the sum of n bernoulli(p) trials\u0026quot;\u0026quot;\u0026quot; return sum(bernoulli_trial(p) for _ in range(n))  Note: A single \u0026lsquo;success\u0026rsquo; in a Bernoulli trial is \u0026lsquo;x\u0026rsquo;. Summing up all those x\u0026rsquo;s into X, is a Binomial random variable. Success doesn\u0026rsquo;t imply desirability, nor does \u0026ldquo;failure\u0026rdquo; imply undesirability. They\u0026rsquo;re just terms to count the cases we\u0026rsquo;re looking for (i.e., number of heads in multiple coin flips to assess a coin\u0026rsquo;s fairness).\nGiven that our null is (p = 0.5) and alt is (p != 0.5), we can run some independent bernoulli trials, then sum them up to get a binomial random variable.\nEach bernoulli_trial is an experiment with either 0 or 1 as outcomes. The binomial function sums up n bernoulli(0.5) trails. We ran both twice and got different results. Each bernoulli experiment can be a success(1) or faill(0); summing up into a binomial random variable means we\u0026rsquo;re taking the probability p(0.5) that a coin flips head and we ran the experiment 1,000 times to get a random binomial variable.\nThe first 1,000 flips we got 510. The second 1,000 flips we got 495. We can repeat this process many times to get a distribution. We can plot this distribution to reinforce our understanding. To this we\u0026rsquo;ll use binomial_histogram function. This function picks points from a Binomial(n,p) random variable and plots their histogram.\nfrom collections import Counter import matplotlib.pyplot as plt def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2 def binomial_histogram(p: float, n: int, num_points: int) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Picks points from a Binomial(n, p) and plots their histogram\u0026quot;\u0026quot;\u0026quot; data = [binomial(n, p) for _ in range(num_points)] # use a bar chart to show the actual binomial samples histogram = Counter(data) plt.bar([x - 0.4 for x in histogram.keys()], [v / num_points for v in histogram.values()], 0.8, color='0.75') mu = p * n sigma = math.sqrt(n * p * (1 - p)) # use a line chart to show the normal approximation xs = range(min(data), max(data) + 1) ys = [normal_cdf(i + 0.5, mu, sigma) - normal_cdf(i - 0.5, mu, sigma) for i in xs] plt.plot(xs, ys) plt.title(\u0026quot;Binomial Distribution vs. Normal Approximation\u0026quot;) plt.show() # call function binomial_histogram(0.5, 1000, 10000)  This plot is then rendered:\nWhat we did was sum up independent bernoulli_trial(s) of 1,000 coin flips, where the probability of head is p = 0.5, to create a binomial random variable. We then repeated this a large number of times (N = 10,000), then plotted a histogram of the distribution of all binomial random variables. And because we did it so many times, it approximates the standard normal distribution (smooth bell shape curve).\nJust to demonstrate how this works, we can generate several binomial random variables:\nIf we do this 10,000 times, we\u0026rsquo;ll generate the above histogram. You\u0026rsquo;ll notice that because we are testing whether the coin is fair, the probability of heads (success) should be at 0.5 and, from 1,000 coin flips, the mean(mu) should be a 500.\nWe have another function that can help us calculate normal_approximation_to_binomial:\nimport random from typing import Tuple import math def normal_approximation_to_binomial(n: int, p: float) -\u0026gt; Tuple[float, float]: \u0026quot;\u0026quot;\u0026quot;Return mu and sigma corresponding to a Binomial(n, p)\u0026quot;\u0026quot;\u0026quot; mu = p * n sigma = math.sqrt(p * (1 - p) * n) return mu, sigma # call function # (500.0, 15.811388300841896) normal_approximation_to_binomial(1000, 0.5)  When calling the function with our parameters, we get a mean mu of 500 (from 1,000 coin flips) and a standard deviation sigma of 15.8114. Which means that 68% of the time, the binomial random variable will be 500 +/- 15.8114 and 95% of the time it\u0026rsquo;ll be 500 +/- 31.6228 (see 68-95-99.7 rule)\nHypothesis_Testing Now that we have seen the results of our \u0026ldquo;coin fairness\u0026rdquo; experiment plotted on a binomial distribution (approximately normal), we will be, for the purpose of testing our hypothesis, be interested in the probability of its realized value (binomial random variable) lies within or outside a particular interval.\nThis means we\u0026rsquo;ll be interested in questions like:\n What\u0026rsquo;s the probability that the binomial(n,p) is below a threshold? Above a threshold? Between an interval? Outside an interval?  First, the normal_cdf (normal cummulative distribution function), which we learned in a previous post, is the probability of a variable being below a certain threshold.\nHere, the probability of X (success or heads for a \u0026lsquo;fair coin\u0026rsquo;) is at 0.5 (mu = 500, sigma = 15.8113), and we want to find the probability that X falls below 490, which comes out to roughly 26%\nnormal_probability_below = normal_cdf # probability that binomal random variable, mu = 500, sigma = 15.8113, is below 490 # 0.26354347477247553 normal_probability_below(490, 500, 15.8113)  On the other hand, the normal_probability_above, probability that X falls above 490 would be 1 - 0.2635 = 0.7365 or roughly 74%.\ndef normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;The probability that an N(mu, sigma) is greater than lo.\u0026quot;\u0026quot;\u0026quot; return 1 - normal_cdf(lo, mu, sigma) # 0.7364565252275245 normal_probability_above(490, 500, 15.8113)  To make sense of this we need to recall the binomal distribution, that approximates the normal distribution, but we\u0026rsquo;ll draw a vertical line at 490.\nWe\u0026rsquo;re asking, given the binomal distribution with mu 500 and sigma at 15.8113, what is the probability that a binomal random variable falls below the threshold (left of the line); the answer is approximately 26% and correspondingly falling above the threshold (right of the line), is approximately 74%.\nBetween interval We may also wonder what the probability of a binomial random variable falling between 490 and 520:\nHere is the function to calculate this probability and it comes out to approximately 63%. note: Bear in mind the full area under the curve is 1.0 or 100%.\ndef normal_probability_between(lo: float, hi: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;The probability that an N(mu, sigma) is between lo and hi.\u0026quot;\u0026quot;\u0026quot; return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma) # 0.6335061861416337 normal_probability_between(490, 520, 500, 15.8113)  Finally, the area outside of the interval should be 1 - 0.6335 = 0.3665:\ndef normal_probability_outside(lo: float, hi: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;The probability that an N(mu, sigma) is not between lo and hi.\u0026quot;\u0026quot;\u0026quot; return 1 - normal_probability_between(lo, hi, mu, sigma) # 0.3664938138583663 normal_probability_outside(490, 520, 500, 15.8113)  In addition to the above, we may also be interested in finding (symmetric) intervals around the mean that account for a certain level of likelihood, for example, 60% probability centered around the mean.\nFor this operation we would use the inverse_normal_cdf:\ndef inverse_normal_cdf(p: float, mu: float = 0, sigma: float = 1, tolerance: float = 0.00001) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Find approximate inverse using binary search\u0026quot;\u0026quot;\u0026quot; # if not standard, compute standard and rescale if mu != 0 or sigma != 1: return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance) low_z = -10.0 # normal_cdf(-10) is (very close to) 0 hi_z = 10.0 # normal_cdf(10) is (very close to) 1 while hi_z - low_z \u0026gt; tolerance: mid_z = (low_z + hi_z) / 2 # Consider the midpoint mid_p = normal_cdf(mid_z) # and the CDF's value there if mid_p \u0026lt; p: low_z = mid_z # Midpoint too low, search above it else: hi_z = mid_z # Midpoint too high, search below it return mid_z  First we\u0026rsquo;d have to find the cutoffs where the upper and lower tails each contain 20% of the probability. We calculate normal_upper_bound and normal_lower_bound and use those to calculate the normal_two_sided_bounds.\ndef normal_upper_bound(probability: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Returns the z for which P(Z \u0026lt;= z) = probability\u0026quot;\u0026quot;\u0026quot; return inverse_normal_cdf(probability, mu, sigma) def normal_lower_bound(probability: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Returns the z for which P(Z \u0026gt;= z) = probability\u0026quot;\u0026quot;\u0026quot; return inverse_normal_cdf(1 - probability, mu, sigma) def normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -\u0026gt; Tuple[float, float]: \u0026quot;\u0026quot;\u0026quot; Returns the symmetric (about the mean) bounds that contain the specified probability \u0026quot;\u0026quot;\u0026quot; tail_probability = (1 - probability) / 2 # upper bound should have tail_probability above it upper_bound = normal_lower_bound(tail_probability, mu, sigma) # lower bound should have tail_probability below it lower_bound = normal_upper_bound(tail_probability, mu, sigma) return lower_bound, upper_bound  So if we wanted to know what the cutoff points were for a 60% probability around the mean and standard deviation (mu = 500, sigma = 15.8113), it would be between 486.69 and 513.31.\nSaid differently, this means roughly 60% of the time, we can expect the binomial random variable to fall between 486 and 513.\n# (486.6927811021805, 513.3072188978196) normal_two_sided_bounds(0.60, 500, 15.8113)  Significance and Power Now that we have a handle on the binomial normal distribution, thresholds (left and right of the mean), and cut-off points, we want to make a decision about significance. Probably the most important part of statistical significance is that it is a decision to be made, not a standard that is externally set.\nSignificance is a decision about how willing we are to make a type 1 error (false positive), which we explored in a previous post. The convention is to set it to a 5% or 1% willingness to make a type 1 error. Suppose we say 5%.\nWe would say that out of 1,000 coin flips, 95% of the time, we\u0026rsquo;d get between 469 and 531 heads on a \u0026ldquo;fair coin\u0026rdquo; and 5% of the time, outside of this 469-531 range.\n# (469.0104394712448, 530.9895605287552) normal_two_sided_bounds(0.95, 500, 15.8113)  If we recall our hypotheses:\nNull Hypothesis: Probability of landing on Heads = 0.5 (fair coin)\nAlt Hypothesis: Probability of landing on Heads != 0.5 (biased coin)\nEach binomial distribution (test) that consist of 1,000 bernoulli trials, each test where the number of heads falls outside the range of 469-531, we\u0026rsquo;ll reject the null that the coin is fair. And we\u0026rsquo;ll be wrong (false positive), 5% of the time. It\u0026rsquo;s a false positive when we incorrectly reject the null hypothesis, when it\u0026rsquo;s actually true.\nWe also want to avoid making a type-2 error (false negative), where we fail to reject the null hypothesis, when it\u0026rsquo;s actually false.\nNote: Its important to keep in mind that terms like significance and power are used to describe tests, in our case, the test of whether a coin is fair or not. Each test is the sum of 1,000 independent bernoulli trials.\nFor a \u0026ldquo;test\u0026rdquo; that has a 95% significance, we\u0026rsquo;ll assume that out of a 1,000 coin flips, it\u0026rsquo;ll land on heads between 469-531 times and we\u0026rsquo;ll determine the coin is fair. For the 5% of the time it lands outside of this range, we\u0026rsquo;ll determine the coin to be \u0026ldquo;unfair\u0026rdquo;, but we\u0026rsquo;ll be wrong because it actually is fair.\nTo calculate the power of the test, we\u0026rsquo;ll take the assumed mu and sigma with a 95% bounds (based on the assumption that the probability of the coin landing on heads is 0.5 or 50% - a fair coin). We\u0026rsquo;ll determine the lower and upper bounds:\nlo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0) lo # 469.01026640487555 hi # 530.9897335951244  And if the coin was actually biased, we should reject the null, but we fail to. Let\u0026rsquo;s suppose the actual probability that the coin lands on heads is 55% ( biased towards head):\nmu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55) mu_1 # 550.0 sigma_1 # 15.732132722552274  Using the same range 469 - 531, where the coin is assumed \u0026lsquo;fair\u0026rsquo; with mu at 500 and sigma at 15.8113:\nIf the coin, in fact, had a bias towards head (p = 0.55), the distribution would shift right, but if our 95% significance test remains the same, we get:\nThe probability of making a type-2 error is 11.345%. This is the probability that we\u0026rsquo;re see that the coin\u0026rsquo;s distribution is within the previous interval 469-531, thinking we should accept the null hypothesis (that the coin is fair), but in actuality, failing to see that the distribution has shifted to the coin having a bias towards heads.\n# 0.11345199870463285 type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)  The other way to arrive at this is to find the probability, under the new mu and sigma (new distribution), that X (number of successes) will fall below 531.\n# 0.11357762975476304 normal_probability_below(531, mu_1, sigma_1)  So the probability of making a type-2 error or the probability that the new distribution falls below 531 is approximately 11.3%.\nThe power to detect a type-2 error is 1.00 minus the probability of a type-2 error (1 - 0.113 = 0.887), or 88.7%.\npower = 1 - type_2_probability # 0.8865480012953671  Finally, we may be interested in increasing power to detect a type-2 error. Instead of using a normal_two_sided_bounds function to find the cut-off points (i.e., 469 and 531), we could use a one-sided test that rejects the null hypothesis (\u0026lsquo;fair coin\u0026rsquo;) when X (number of heads on a coin-flip) is much larger than 500.\nHere\u0026rsquo;s the code, using normal_upper_bound:\n# 526.0073585242053 hi = normal_upper_bound(0.95, mu_0, sigma_0)  This means shifting the upper bounds from 531 to 526, providing more probability in the upper tail. This means the probability of a type-2 error goes down from 11.3 to 6.3.\n# previous probability of type-2 error # 0.11357762975476304 normal_probability_below(531, mu_1, sigma_1) # new probability of type-2 error # 0.06356221447122662 normal_probability_below(526, mu_1, sigma_1)  And the new (stronger) power to detect type-2 error is 1.0 - 0.064 = 0.936 or 93.6% (up from 88.7% above).\np_values p-Values represent another way of deciding whether to accept or reject the Null Hypothesis. Instead of choosing bounds, thresholds or cut-off points, we could compute the probability, assuming the Null Hypothesis is true, that we would see a value as extreme as the one we just observed.\nHere is the code:\ndef two_sided_p_values(x: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot; How likely are we to see a value at least as extreme as x (in either direction) if our values are from an N(mu, sigma)? \u0026quot;\u0026quot;\u0026quot; if x \u0026gt;= mu: # x is greater than the mean, so the tail is everything greater than x return 2 * normal_probability_above(x, mu, sigma) else: # x is less than the mean, so the tail is everything less than x return 2 * normal_probability_below(x, mu, sigma)  If we wanted to compute, assuming we have a \u0026ldquo;fair coin\u0026rdquo; (mu = 500, sigma = 15.8113), what is the probability of seeing a value like 530? (note: We use 529.5 instead of 530 below due to continuity correction)\nAnswer: approximately 6.2%\n# 0.06207721579598835 two_sided_p_values(529.5, mu_0, sigma_0)  The p-value, 6.2% is higher than our (hypothetical) 5% significance, so we don\u0026rsquo;t reject the null. On the other hand, if X was slightly more extreme, 532, the probability of seeing that value would be approximately 4.3%, which is less than 5% significance, so we would reject the null.\n# 0.04298479507085862 two_sided_p_values(532, mu_0, sigma_0)  For one-sided tests, we would use the normal_probability_above and normal_probability_below functions created above:\nupper_p_value = normal_probability_above lower_p_value = normal_probability_below  Under the two_sided_p_values test, the extreme value of 529.5 had a probability of 6.2% of showing up, but not low enough to reject the null hypothesis.\nHowever, with a one-sided test, upper_p_value for the same threshold is now 3.1% and we would reject the null hypothesis.\n# 0.031038607897994175 upper_p_value(529.5, mu_0, sigma_0)  Confidence_Intervals A third approach to deciding whether to accept or reject the null is to use confidence intervals. We\u0026rsquo;ll use the 530 as we did in the p-Values example.\np_hat = 530/1000 mu = p_hat sigma = math.sqrt(p_hat * (1 - p_hat) / 1000) # 0.015782902141241326 # (0.4990660982192851, 0.560933901780715) normal_two_sided_bounds(0.95, mu, sigma)  The confidence interval for a coin flipping heads 530 (out 1,000) times is (0.4991, 0.5609). Since this interval contains the p = 0.5 (probability of heads 50% of the time, assuming a fair coin), we do not reject the null.\nIf the extreme value were more extreme at 540, we would arrive at a different conclusion:\np_hat = 540/1000 mu = p_hat sigma = math.sqrt(p_hat * (1 - p_hat) / 1000) (0.5091095927295919, 0.5708904072704082) normal_two_sided_bounds(0.95, mu, sigma)  Here we would be 95% confident that the mean of this distribution is contained between 0.5091 and 0.5709 and this does not contain 0.500 (albiet by a slim margin), so we reject the null hypothesis that this is a fair coin.\nnote: Confidence intervals are about the interval not probability p. We interpret the confidence interval as, if you were to repeat the experiment many times, 95% of the time, the \u0026ldquo;true\u0026rdquo; parameter, in our example p = 0.5, would lie within the observed confidence interval.\nConnecting_Dots We used several python functions to build intuition around statistical hypothesis testing. To higlight this \u0026ldquo;from scratch\u0026rdquo; aspect of the book here is a diagram tying together the various python function used in this post:\nThis post is part of an ongoing series where I document my progress through Data Science from Scratch by Joel Grus.\nFor more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"ee1cb59d8492fbc16233cc8475c095fc","permalink":"/post/dsfs_7/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/post/dsfs_7/","section":"post","summary":"An overview of statistical Hypothesis Testing, Estimation and Bayesian Inference","tags":["Python","Data Science","Probability","Statistics"],"title":"Data Science from Scratch (ch7) - Hypothesis and Inference","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Permutations  Overview Itertools are a core set of fast, memory efficient tools for creating iterators for efficient looping (read the documentation here).\nItertools Permutations One (of many) uses for itertools is to create a permutations() function that will return all possible combinations of items in a list.\nI was working on a project that involved user funnels with different stages and we were wondering how many different \u0026ldquo;paths\u0026rdquo; a user could take, so this was naturally a good fit for using permutations.\nSample Funnel\nIn our hypothetical example, we\u0026rsquo;re looking at a funnel with three stages for a total of 6 permutations. Here\u0026rsquo;s the formula:\nIf you\u0026rsquo;re using a sales/marketing funnel, you\u0026rsquo;ll have in mind what your funnel would look like so you may not want all possible paths, but if you\u0026rsquo;re interested in exploring potentially overlooked paths, read on.\nHere\u0026rsquo;s the python documentation for itertools, and permutations specifically. We\u0026rsquo;ll break down the code to better understand what\u0026rsquo;s going on in this function.\nnote: I found a clearer alternative after the fact. Feel free to skip to the final section below, although there is value in comparing the two versions.\nWe\u0026rsquo;ll start off with the iterable which is a list with three strings. The permutations function takes in two parameters, the iterable and r which is the number of items from the list that we\u0026rsquo;re interested in finding the combination of. If we have three items in the list, we generally want to find all possible combinations of those three items.\nHere is the code, and subsequent breakdown:\n# list of length 3 list1 = ['stage 1', 'stage 2', 'stage 3'] # iterable is the list # r = number of items from the list to find combinations of def permutations(iterable, r=None): \u0026quot;\u0026quot;\u0026quot;Find all possible order of a list of elements\u0026quot;\u0026quot;\u0026quot; # permutations('ABCD',2)--\u0026gt; AB AC AD BA BC BD CA CB CD DA DB DC # permutations(range(3))--\u0026gt; 012 021 102 120 201 210 # permutations(list1, 6)--\u0026gt; ...720 permutations pool = tuple(iterable) n = len(pool) r = n if r is None else r if r \u0026gt; n: return indices = list(range(n)) # [0, 1, 2] cycles = list(range(n, n-r, -1)) # [3, 2, 1] yield tuple(pool[i] for i in indices[:r]) print(\u0026quot;Now entering while-loop \\n\u0026quot;) while n: for i in reversed(range(r)): cycles[i] -= 1 if cycles[i] == 0: indices[i:] = indices[i+1:] + indices[i:i+1] cycles[i] = n - i else: j = cycles[i] indices[i], indices[-j] = indices[-j], indices[i] yield tuple(pool[i] for i in indices[:r]) print(\u0026quot;indices[:r]\u0026quot;, indices[:r]) print(\u0026quot;pool[i]:\u0026quot;, tuple(pool[i] for i in indices[:r])) print(\u0026quot;n:\u0026quot;, n) break else: print(\u0026quot;return:\u0026quot;) return #permutations(list1, 6) perm = permutations(list1, 3) count = 0 for p in perm: count += 1 print(p) print(\u0026quot;there are:\u0026quot;, count, \u0026quot;permutations.\u0026quot;)  The first thing we do is take the iterable input parameter is turn it from a list into a tuple.\npool = tuple(iterable)  There are several reasons to do this. First, tuples are faster than lists; the permutations() function will do several operations to the input so changing it to a tuple allows faster operations and because tuples are immutable, we can do a bunch of different operations without fear that we might inadvertently change the list.\nWe then create n from the length of pool (in our case it\u0026rsquo;s 3) and the additional r parameter, which defaults to None is also 3 as we\u0026rsquo;re interested in seeing all combinations of a list of three elements.\nWe also have a line that ensures that r can never be greater than the number of elements in the iterable (list).\nif r \u0026gt; n: return  Next, we create indices and cycles. Indices are basically the index of each item, starting with 0 to 2, for three items. Cycles uses range(n, n-r, -1), which in our case is range(3, 3-3, -1); this means start at three and end at zero, in -1 steps.\nThe next chunk of code is a while-loop that will continue for the length of the list, n (note the break at the bottom to exit out of this loop).\nAfter each if-else cycle, a new set of indices are created, which then gets looped through with pool, the interable parameter input, which changes the order of the elements in the list.\nYou\u0026rsquo;ll note in the commented code above, cycles start off at [3,2,1] and indices start off at [0,1,2]. Each loop through the code changes the indices where indices[i:] successively gets longer [2], then [1,2], then [1,2,3]. While cycles changes as it trends toward [1,1,1], which point the code breaks out of the loop.\nwhile n: for i in reversed(range(r)): cycles[i] -= 1 if cycles[i] == 0: indices[i:] = indices[i+1:] + indices[i:i+1] cycles[i] = n - i else: j = cycles[i] indices[i], indices[-j] = indices[-j], indices[i] yield tuple(pool[i] for i in indices[:r]) print(\u0026quot;indices[:r]\u0026quot;, indices[:r]) print(\u0026quot;pool[i]:\u0026quot;, tuple(pool[i] for i in indices[:r])) print(\u0026quot;n:\u0026quot;, n) break else: print(\u0026quot;return:\u0026quot;)  The permutations(iterable, r) function actually creates a generator so we need to loop through it again to print out all the permutations of the list.\n\u0026lt;generator object permutations at 0x7fe19400fdd0\u0026gt;  We add another for-loop at the bottom to print out all the permutations:\nperm = permutations(list1, 3) count = 0 for p in perm: count += 1 print(p) print(\u0026quot;there are:\u0026quot;, count, \u0026quot;permutations.\u0026quot;)  Here is our result:\nA Clearer Alternative: Permutation Using Recursion As is often the case, there is a better way I found in retrospect from this stack overflow (h/t to Eric O Lebigot):\ndef all_perms(elements): if len(elements) \u0026lt;= 1: yield elements # Only permutation possible = no permutation else: # Iteration over the first element in the result permutation: for (index, first_elmt) in enumerate(elements): other_elmts = elements[:index] + elements[index+1:] for permutation in all_perms(other_elmts): yield [first_elmt] + permutation  The enumerate built-in function obviates the need to separately create cycles and indices. The local variable other_elmts separates the other elements in the list from the first_elmt, then the second for-loop recursively finds the permutation of the other elements before adding with the first_elmt on the final line, yielding all possible permutations of a list. As with the previous case, the result of this function is a generator which requires looping through and printing the permutations.\nI found this much easier to digest than the documentation version.\nPermutations can be useful when you have varied user journeys through your product and you want to figure out all the possible paths. With this short python script, you can easily print out all options for consideration.\nTake Aways From the perspective of a user funnel, permutations allow us to explore all possible paths a user might take. For our hypothetical example, a three-step funnel yields six possible paths a user could navigate from start to finish.\nKnowing permutations should also give us pause when deciding whether to add another \u0026ldquo;step\u0026rdquo; to a funnel. Going from a three-step funnel to a four-step funnel increases the number of possible paths from six to 24 - a quadruple increase.\nNot only does this increase friction between your user and the \u0026lsquo;end goal\u0026rsquo; (conversion), whatever that may be for your product, but it also increases complexity (and potentially confusion) in the user experience.\nFor more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1607472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607472000,"objectID":"4e2d07c6c50aa7c2f5c00f3deabfea17","permalink":"/post/statistics_probability/","publishdate":"2020-12-09T00:00:00Z","relpermalink":"/post/statistics_probability/","section":"post","summary":"Learning Statistics and Probability Concepts through Code Snippets","tags":["Python","Data Science","Probability","Statistics"],"title":"Statistics \u0026 Probability in Code","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Challenge  Marginal and Joint Probability  Conditional Probability  Bayes' Theorem  Applying Bayes' Theorem  Distributions  Overview Challenge The first challenge in this section is distinguishing between two conditional probability statements.\nHere\u0026rsquo;s the setup. We have a family with two (unknown) children with two assumptions. First, each child is equally likely to be a boy or a girl. Second, the gender of the second child is independent of the gender of the first child.\n Outcome 1: What is the probability of the event \u0026ldquo;both children are girls\u0026rdquo; (B) conditional on the event \u0026ldquo;the older child is a girl\u0026rdquo; (G)?\n The probability for statement one is roughly 50% or (1/2).\n Outcome 2: What is the probability of the event \u0026ldquo;both children are girls\u0026rdquo; (B) conditional on the event \u0026ldquo;at least one of the children is a girl\u0026rdquo; (L)?\n The probability for statement two is roughly 33% or (1/3).\nBut at first glance, they look similar.\nMarginal_and_Joint_Probabilities The book jumps straight to conditional probabilities, but first, we\u0026rsquo;ll have to look at marginal and joint probabilities. Then we\u0026rsquo;ll create a joint probabilities table and sum probabilities to help us figure out the differences. We\u0026rsquo;ll then resume with conditional probabilities.\nBefore anything, we need to realize the situation we have is one of independence. The gender of one child is independent of a second child.\nThe intuition for this scenario will be different from a dependent situation. For example, if we draw two cards from a deck (without replacement), the probabilities are different. The probability of drawing one King ♠️ is (4/52) and the probability of drawing a second King ♣️ is now (3/51); the probability of the second event (a second King) is dependent on the result of the first draw.\nOk back to the two unknown children.\nWe can say the probability of the first child being either a boy or a girl is 50/50. Moreover, the probability of the second child, which is independent of the first, is also 50/50. Remember, our first assumption is that each child is equally likely to be a boy or a girl.\nLet\u0026rsquo;s put these numbers in a table. The (1/2) probabilities shown here are called marginal probabilities (note how they\u0026rsquo;re at the margins of the table).\nSince we have two gender (much like two sides of a flipped coin), we can intuitively figure out all possible outcomes:\n first child (Boy), second child (Boy) first child (Boy), second child (Girl) first child (Girl), second child (Boy) first child (Girl), second child (Girl)  There are 4 possible outcomes so the probability of getting any one of the four outcomes is (1/4). We can actually write these probabilities in the middle of the table, the joint probabilities:\nTo recap, the probability of the first child being either boy or girl is 50/50, simple enough. The probability of the second child being either boy or girl is also 50/50. When put in a table, this yielded the marginal probability.\nNow we want to know the probability of say, \u0026lsquo;first child being a boy and second child being a girl\u0026rsquo;. This is a joint probability because is is the probability that the first child take a specific gender (boy) AND the second child take a specific gender (girl).\nIf two event are independent, and in this case they are, their joint probabilities are the product of the probabilities of each one happening.\nThe probability of the first child being a Boy (1/2) and second child being a Girl (1/2); The product of each marginal probability is the joint probability (1/2 * 1/2 = 1/4).\nThis can be repeated for the other three joint probabilities.\nConditional_Probability Now we get into conditional probability which is the probability of one event happening (i.e., second child being a Boy or Girl) given that or on conditional that another event happened (i.e., first child being a Boy).\nAt this point, it might be a good idea to get familiar with notation.\nA joint probability is the product of each individual event happening (assuming they are independent events). For example we might have two individual events:\n P(1st Child = Boy): 1/2 P(2nd Child = Boy): 1/2  Here is their joint probability:\n P(1st Child = Boy, 2nd Child = Boy) =\u0026gt; P(1st Child = Boy) * P(2nd Child = Boy) =\u0026gt; (1/2 * 1/2 = 1/4)  There is a relationship between conditional probabilities and joint probabilities.\n P(1st Child = Boy | 2nd Child = Boy) = P(1st Child = Boy, 2nd Child = Boy) / P(2nd Child = Boy)  Namely, the conditional probability is equal to the joint probability divided by the conditional.\nThie works out to:\n P(1st Child = Boy | 2nd Child = Boy) = (1/4) / (1/2) or (1/4) * (2/1) = 1/2  In other words, the probability that the second child is a boy, given that the first child is a boy is still 50% (this implies that with respect to conditional probability, if the events are independent it is not different from a single event).\nNow we\u0026rsquo;re ready to tackle the two challenges posed at the beginning of this post.\n Challenge 1: What is the probability of the event \u0026ldquo;both children are girls\u0026rdquo; (B) conditional on the event \u0026ldquo;the older child is a girl\u0026rdquo; (G)?\n Let\u0026rsquo;s break it down. First we want the probability of the event that \u0026ldquo;both children are girls\u0026rdquo;. We\u0026rsquo;ll take the product of two events; the probability that the first child is a girl (1/2) and the probability that the second child is a girl (1/2). So the joint probability of both child being girls is 1/2 * 1/2 = 1/4\n P(1st Child = Girl, 2nd Child = Girl) = 1/4  Second, we want that to be given that the \u0026ldquo;older child is a girl\u0026rdquo;.\n P(1st Child = Girl) = 1/2  Conditional probability:\n  P(Both Child = Girls | 1st Child = Girl) = P(1st Child = Girl, 2nd Child = Girl) / P(1st Child = Girl)\n  P(Both Child = Girls | 1st Child = Girl) = (1/4) / (1/2)\n  (1/4) * (2/1) = 1/2 or roughly 50%\n  Now let\u0026rsquo;s break down the second challenge:\n Challenge 2: What is the probability of the event \u0026ldquo;both children are girls\u0026rdquo; (B) conditional on the event \u0026ldquo;at least one of the children is a girl\u0026rdquo; (L)?\n Again, we start with \u0026ldquo;both children are girls\u0026rdquo;:\n P(1st Child = Girl, 2nd Child = Girl) = 1/4  Then, we have \u0026ldquo;on condition that at least one of the children is a girl\u0026rdquo;. We\u0026rsquo;ll reference a joint probability table. We see that when trying to figure out the probability that \u0026ldquo;at least one of the children is a girl\u0026rdquo;, we rule out the scenario where both children are boys. The remaining 3 out of 4 probabilities, fit the condition.\nThe probability of at least one children being a girl is:\n (1/4) + (1/4) + (1/4) = 3/4  So (introducing notation):\n  P(B) = \u0026ldquo;probability of both child being girls\u0026rdquo; (i.e., 1st Child = Girl, 2nd Child = Girl)\n  P(L) = \u0026ldquo;probability of at least one child being a girl\u0026rdquo;\n  P(B|L) = P(B,L) / P(L)\n  P(B|L) = (1/4) / (3/4) = (1/4) * (4/3) = 1/3 or roughly 33%\n  Key Take-away When two events are independent, their joint probability is the product of each event:\n P(E,F) = P(E) * P(F)  Their conditional probability is the joint probability divided by the conditional (i.e., P(F)).\n P(E|F) = P(E,F) / P(F)  And so for our two challenge scenarios, we have:\nChallenge 1:\n B = probability that both children are girls G = probability that the older children is a girl  This can be stated as: P(B|G) = P(B,G) / P(G)\nChallenge 2:\n B = probability that both children are girls L = probability that at least one children is a girl  This can be stated as: P(B|L) = P(B,L) / P(L)\nPython Code Now that we have an intuition and have worked out the problem on paper, we can use code to express conditional probability:\nimport enum, random class Kid(enum.Enum): BOY = 0 GIRL = 1 def random_kid() -\u0026gt; Kid: return random.choice([Kid.BOY, Kid.GIRL]) both_girls = 0 older_girl = 0 either_girl = 0 random.seed(0) for _ in range(10000): younger = random_kid() older = random_kid() if older == Kid.GIRL: older_girl += 1 if older == Kid.GIRL and younger == Kid.GIRL: both_girls += 1 if older == Kid.GIRL or younger == Kid.GIRL: either_girl += 1 print(\u0026quot;P(both | older):\u0026quot;, both_girls / older_girl) # 0.5007089325501317 print(\u0026quot;P(both | either):\u0026quot;, both_girls / either_girl) # 0.3311897106109325  We can see that code confirms our intuition.\nWe use a for-loop and range(10000) to randomly simulate 10,000 scenarios. The random_kid function randomly picks either a boy or girl (assumption #1). We set the following variables to start a 0, both_girls (both children are girls); older_girl (first child is a girl); and either_girl (at least one child is a girl).\nThen, each of these variables are incremented by 1 through each of the 10,000 loops if it meets certain conditions. After we finish looping, we can call on each of the three variables to see if they match our calculations above:\neither_girl #7,464 / 10,000 ~ roughly 75% or 3/4 probability that there is at least one girl both_girls #2,472 / 10,000 ~ roughly 25% or 1/4 probability that both children are girls older_girl #4,937 / 10,000 ~ roughly 50% or 1/2 probability that the first child is a girl  We will look at Bayes Theorem next.\nBayes_Theorem Previously, we established an understanding of conditional probability, but building up with marginal and joint probabilities. We explored the conditional probabilities of two outcomes:\n Outcome 1: What is the probability of the event \u0026ldquo;both children are girls\u0026rdquo; (B) conditional on the event \u0026ldquo;the older child is a girl\u0026rdquo; (G)?\n The probability for outcome one is roughly 50% or (1/2).\n Outcome 2: What is the probability of the event \u0026ldquo;both children are girls\u0026rdquo; (B) conditional on the event \u0026ldquo;at least one of the children is a girl\u0026rdquo; (L)?\n The probability for outcome two is roughly 33% or (1/3).\nBayes' Theorem is simply an alternate way of calculating conditional probability.\nPreviously, we used the joint probability to calculate the conditional probability.\nOutcome 1 Here\u0026rsquo;s the conditional probability for outcome 1, using a joint probability:\n  P(G) = \u0026lsquo;Probability that first child is a girl\u0026rsquo; (1/2)\n  P(B) = \u0026lsquo;Probability that both children are girls\u0026rsquo; (1/4)\n  P(B|G) = P(B,G) / P(G)\n  P(B|G) = (1/4) / (1/2) = 1/2 or roughly 50%\n  Technically, we can\u0026rsquo;t use joint probability because the two events are not independent.\nTo clarify, the probability of the older child being a certain gender and the probability of the younger child being a certain gender is independent, but P(B|G) the \u0026lsquo;probability of both child being a girl\u0026rsquo; and \u0026lsquo;the probability of the older child being a girl\u0026rsquo; are not independent; and hence we express it as a conditional probability.\nSo, the joint probability of P(B,G) is just event B,P(B).\nHere\u0026rsquo;s an alternate way to calculate the conditional probability (without joint probability):\n P(B|G) = P(G|B) * P(B) / P(G) This is Bayes Theorem P(B|G) = 1 * (1/4) / (1/2) P(B|G) = (1/4) * (2/1) P(B|G) = 1/2 = 50%  note: P(G|B) is \u0026lsquo;the probability that the first child is a girl, given that both children are girls is a certainty (1.0)\u0026rsquo;\nThe reverse conditional probability, can also be calculated, without joint probability:\n What is the probability of the older child being a girl, given that both children are girls?\n  P(G|B) = P(B|G) * P(G) / P(B) This is Bayes Theorem (reverse case) P(G|B) = (1/2) * (1/2) / (1/4) P(G|B) = (1/4) / (1/4) P(G|B) = 1 = 100%  This is consistent with what we already derived above, namely that P(G|B) is a certainty (probability = 1.0), that the older child is a girl, given that both children are girls.\nWe can point out two additional observations / rules:\n While, joint probabilities are symmetrical: P(B,G) == P(G,B), Conditional probabilities are not symmetrical: P(B|G) != P(G|B)  Bayes' Theorem: Alternative Expression Bayes Theorem is a way of calculating conditional probability without the joint probability, summarized here:\n P(B|G) = P(G|B) * P(B) / P(G) This is Bayes Theorem P(G|B) = P(B|G) * P(G) / P(B) This is Bayes Theorem (reverse case)  You\u0026rsquo;ll note that P(G) is the denominator in the former, and P(B) is the denominator in the latter.\n What if, for some reasons, we don\u0026rsquo;t have access to the denominator?\n We could derive both P(G) and P(B) in another way using the NOT operator:\n P(G) = P(G,B) + P(G,not B) = P(G|B) * P(B) + P(G|not B) * P(not B) P(B) = P(B,G) + P(B,not G) = P(B|G) * P(G) + P(B|not G) * P(not G)  Therefore, the alternative expression of Bayes Theorem for the probability of both children being girls, given that the first child is a girl ( P(B|G) ) is:\n P(B|G) = P(G|B) * P(B) / ( P(G|B) * P(B) + P(G|not B) * P(not B) ) P(B|G) = 1 * 1/4 / (1 * 1/4 + 1/3 * 3/4) P(B|G) = 1/4 / (1/4 + 3/12) P(B|G) = 1/4 / 2/4 = 1/4 * 4/2 P(B|G) = 1/2 or roughly 50%  We can check the result in code:\ndef bayes_theorem(p_b, p_g_given_b, p_g_given_not_b): # calculate P(not B) not_b = 1 - p_b # calculate P(G) p_g = p_g_given_b * p_b + p_g_given_not_b * not_b # calculate P(B|G) p_b_given_g = (p_g_given_b * p_b) / p_g return p_b_given_g #P(B) p_b = 1/4 # P(G|B) p_g_given_b = 1 # P(G|notB) p_g_given_not_b = 1/3 # calculate P(B|G) result = bayes_theorem(p_b, p_g_given_b, p_g_given_not_b) # print result print('P(B|G) = %.2f%%' % (result * 100))  For the probability that the first child is a girl, given that both children are girls ( P(G|B) ) is:\n P(G|B) = P(B|G) * P(G) / ( P(G|B) * P(G) + P(B|not G) * P(not G) ) P(G|B) = 1/2 * 1/2 / ((1/2 * 1/2) + (0 * 1/2)) P(G|B) = 1/4 / 1/4 P(G|B) = 1  Let\u0026rsquo;s unpack Outcome 2.\nOutcome 2  Outcome 2: What is the probability of the event \u0026ldquo;both children are girls\u0026rdquo; (B) conditional on the event \u0026ldquo;at least one of the children is a girl\u0026rdquo; (L)?\n The probability for outcome two is roughly 33% or (1/3).\nWe\u0026rsquo;ll go through the same process as above.\nWe could use joint probability to calculate the conditional probability. As with the previous outcome, the joint probability of P(B,G) is just event B,P(B).\n P(B|L) = P(B,L) / P(L) = 1/3  Or, we could use Bayes' Theorem to figure out the conditional probability without joint probability:\n P(B|L) = P(L|B) * P(B) / P(L) P(B|L) = (1 * 1/4) / (3/4) P(B|L) = 1/3  And, if there\u0026rsquo;s no P(L), we can calculate that indirectly, also using Bayes' Theorem:\n P(L) = P(L|B) * P(B) + P(L|not B) * P(not B) P(L) = 1 * (1/4) + (2/3) * (3/4) P(L) = (1/4) + (2/4) P(L) = 3/4  Then, we can use P(L) in the way Bayes' Theorem is commonly expressed, when we don\u0026rsquo;t have the denominator:\n P(B|L) = P(L|B) * P(B) / ( P(L|B) * P(B) + P(L|not B) * P(not B) ) P(B|L) = 1 * (1/4) / (3/4) P(B|L) = 1/3  Now that we\u0026rsquo;ve gone through the calculation for two conditional probabilities, P(B|G) and P(B|L), using Bayes Theorem, and implemented code for one of the scenarios, let\u0026rsquo;s take a step back and assess what this means.\nBayesian Terminology I think its useful to understand that probability in general shines when we want to describe uncertainty and that Bayes' Theorem allows us to quantify how much the data we observe, should change our beliefs.\nWe have two posteriors, P(B|G) and P(B|L), both with equal priors and likelihood, but with different evidence.\nSaid differently, we want to know the \u0026lsquo;probability that both children are girls`, given different conditions.\nIn the first case, our condition is \u0026lsquo;the first child is a girl\u0026rsquo; and in the second case, our condition is \u0026lsquo;at least one of the child is a girl\u0026rsquo;. The question is which condition will increase the probability that both children are girls?\nBayes\u0026rsquo; Theorem allows us to update our belief about the probability in these two cases, as we incorporate varied data into our framework.\nWhat the calculations tell us is that the evidence that \u0026lsquo;one child is a girl\u0026rsquo; increases the probability that both children are girls more than the other piece of evidence that \u0026lsquo;at least one child is a girl\u0026rsquo; increases that probability.\nAnd our beliefs should be updated accordingly.\nAt the end of the day, understanding conditional probability (and Bayes Theorem) comes down to counting. For our hypothetical scenarios, we only need one hand:\nWhen we look at the probability table for outcome one, P(B|G), we can see how the posterior probability comes out to 1/2:\nWhen we look at the probability table for outcome two, P(B|L), we can see how the posterior probability comes out to 1/3:\nThis is part of an ongoing series documenting my progress through Data Science from Scratch by Joel Grus:\nApplying_Bayes_Theorem Now that we have a basic understanding of Bayes Theorem, let\u0026rsquo;s extend the application to a slightly more complex example. This section was inspired by this tweet from Grant Sanderson (of 3Blue1Brown fame):\nThis is a classic application of Bayes Theorem - the medical diagnostic scenario. The above tweet can be re-stated:\n What is the probability of you actually having the disease, given that you tested positive?\n This happens to be even more relevant as we\u0026rsquo;re living through a generational pandemic.\nLet\u0026rsquo;s start off with a conceptual understanding, using the tools we learned previously. First, we have to keep in mind testing and actually having the disease are not independent events. Therefore, we will use conditional probability to express their joint outcomes.\nThe intuitive visual to illustrate this is the tree diagram:\nThe initial given information contains the information in the tree.\n  P(D): Probability of having the disease (covid-19)\n  P(P): Probability of testing positive\n  *P(D|P): Our objective is to find the probability of having the disease, given a positive test\n  1 in 1,000 actively have covid-19, P(D), this implies\u0026hellip;\n  999 in 1,000 do not actively have covid-19, P(not D)\n  1% or 0.01 false positive (given)\n  10% or 0.1 false negative (given)\n  The false positive is when you don\u0026rsquo;t have the disease, but your test (in error) shows up positive. False negative is when you have the disease, but your test (in error) shows up negative. We are provided this information and have to calculate other values to fill in the tree.\nWe know that all possible events have to add up to 1, so if 1 in 1,000 actively have the disease, we know that 999 in 1,000 do not have it. If the false negative is 10%, then the true positive is 90%. If the false positive is 1%, then the true negative is 99%. From our calculations, the tree can be updated:\nNow that we\u0026rsquo;ve filled out the tree, we can use Bayes' Theorem to find P(D|P). Here\u0026rsquo;s Bayes' Theorem that we discussed in the previous section. We have Bayes' Theorem, the denominator, probability of testing positive P(P) and the second version of Bayes Theorem in cases were we do not know the probability of testing positive (as in the present case):\nThen we can plug-in the denominator to get the alternative version of Bayes' Theorem:\nHere\u0026rsquo;s how the numbers add up:\n P(D|P) = P(P|D) * P(D) / P(P|D) * P(D) + P(P|not D) * P(not D) P(D|P) = 0.9 * 0.001 / 0.9 * 0.001 + 0.01 * 0.999 P(D|P) = 0.0009 / 0.0009 + 0.00999 P(D|P) = 0.0009 / 0.01089 P(D|P) ~ 0.08264 or 8.26%  Interestingly, Andrej Karpathy actually responded in the thread and provided an intuitive way to arrive at the same result using Python.\nHere\u0026rsquo;s his code (with added comments):\nfrom random import random, seed seed(0) pop = 10000000 # 10M people counts = {} for i in range(pop): has_covid = i % 1000 == 0 # one in 1,000 people have covid (priors or prevalence of disease) # The major assumption is that every person gets tested regardless of any symptoms if has_covid: # Has disease tests_positive = True # True positive if random() \u0026lt; 0.1: tests_positive = False # False negative else: # Does not have disease tests_positive = False # True negative if random() \u0026lt; 0.01: tests_positive = True # False positive outcome = (has_covid, tests_positive) counts[outcome] = counts.get(outcome, 0) + 1 for (has_covid, tested_positive), n in counts.items(): print('has covid: %6s, tests positive: %6s, count: %d' % (has_covid, tested_positive, n)) n_positive = counts[(True, True)] + counts[(False, True)] print('number of people who tested positive:', n_positive) print('probability that a test-positive person actually has covid: %.2f' % (100.0 * counts[(True, True)] / n_positive), )  We first build a hypothetical population of 10 million. If the prior or prevalence of disease is 1 in 1,000, a population of 10 million should find 10000 people with covid. You can see how this works with this short snippet:\npop = 10000000 counts = 0 for i in range(pop): has_covid = i % 1000 == 0 if has_covid: counts = counts + 1 print(counts, \u0026quot;people have the disease in a population of 10 million\u0026quot;)  Nested in the for-loop are if-statements that segment the population (10M) into one of four categories True Positive, False Negative, True Negative, False Positive. Each category is counted and stored in a dict called counts. Then another for-loop is used to loop through this dictionary to print out all the categories:\nhas covid: True, tests positive: True, count: 9033 has covid: False, tests positive: False, count: 9890133 has covid: False, tests positive: True, count: 99867 has covid: True, tests positive: False, count: 967 number of people who tested positive: 108900 probability that a test-positive person actually has covid: 8.29  Finally, we want the number of people who have the disease and tested positive (True Positive, 9033) divided by the number of people who tested positive, regardless of whether they actually have the disease (True Positive (9033) + False Positive (99867) = 108,900) and this comes out to approximately 8.29.\nAlthough the code was billed as \u0026ldquo;simple code to build intuition\u0026rdquo;, I found that Bayes' Theorem is the intuition.\nWhat about symptoms? The key to Bayes' Theorem is that it encourages us to update our beliefs when presented with new evidence. But what if there\u0026rsquo;s evidence we missed in the first place?\nIf you look back at the original tweet, there are important details about symptoms that, if we wanted to be more realistic, should be accounted for.\n You feel fatigued and have a slight sore throat.\n Here, instead of assuming that prevalence of the disease (1 in 1,000 people have covid-19) is the prior, we might ask what probability that someone who is symptomatic has the disease?\nLet\u0026rsquo;s suppose we change from 1 in 1,000 to 1 in 100. We could change just one line of code (while everything else remains the same):\nfor i in range(pop): has_covid = i % 100 == 0 # update info: 1/1000 have covid, but 1/100 with symptoms have covid  The probability that someone with a positive test actually has the disease jumps from 8.29% to 47.61%\nhas covid: True, tests positive: True, count: 180224 has covid: False, tests positive: False, count: 19601715 has covid: False, tests positive: True, count: 198285 has covid: True, tests positive: False, count: 19776 number of people who tested positive: 378509 probability that a test-positive person with symptoms actually has covid: 47.61  Thus, being symptomatic means our priors should be adjusted and our beliefs about the likelihood that a positive test means we have the disease (P(D|P)) should be updated accordingly (in this case, it goes way up).\nTake Aways Hypothetically, if we have family or friends living in an area where 1 in 1,000 people have covid-19 and they (god forbid) got tested and got a positive result, you could tell them that their probability of actually having the disease, given a positive test was around 8.26–8.29%.\nHowever, what’s useful about the Bayesian approach is that it encourages us to incorporate new information and update our beliefs accordingly. So if we find out our family or friend is also symptomatic, we could advise them of the higher probability (~47.61%).\nFinally, we may also advise our family/friends to get tested again, because as much as test-positive person would hope they got a ‘false positive’, chances are low. And even lower, is getting a false positive twice.\nDistributions In this post, we\u0026rsquo;ll cover various distributions. This is a broad topic so we\u0026rsquo;ll sample a few concepts to get a feel for it. Borrowing from the previous post, we\u0026rsquo;ll chart our medical diagnostic outcomes.\nYou\u0026rsquo;ll recall that each outcome is the combination of whether someone has a disease, P(D), or not, P(not D). Then, they\u0026rsquo;re given a diagnostic test that returns positive, P(P) or negative, P(not P).\nThese are discrete outcomes so they can be represented with the probability mass function, as opposed to a probability density function, which represent a continuous distribution.\nLet\u0026rsquo;s take another hypothetical scenario of a city where 1 in 10 people have a disease and a diagnostic test has a True Positive of 95% and True Negative of 90%. The probability that a test-positive person actually having the disease is 46.50%.\nHere\u0026rsquo;s the code:\nfrom random import random, seed seed(0) pop = 1000 # 1000 people counts = {} for i in range(pop): has_disease = i % 10 == 0 # one in 10 people have disease # assuming that every person gets tested regardless of any symptoms if has_disease: tests_positive = True # True Positive 95% if random() \u0026lt; 0.05: tests_positive = False # False Negative 5% else: tests_positive = False # True Negative 90% if random() \u0026lt; 0.1: tests_positive = True # False Positive 10% outcome = (has_disease, tests_positive) counts[outcome] = counts.get(outcome, 0) + 1 for (has_disease, tested_positive), n in counts.items(): print('Has Disease: %6s, Test Positive: %6s, count: %d' % (has_disease, tested_positive, n)) n_positive = counts[(True, True)] + counts[(False, True)] print('Number of people who tested positive:', n_positive) print('Probability that a test-positive person actually has disease: %.2f' % (100.0 * counts[(True, True)] / n_positive),)  Given the probability that someone has the disease (1 in 10), also called the \u0026lsquo;prior\u0026rsquo; in Bayesian terms. We modeled four scenarios where people were given a diagnostic test. Again, the big assumption here is that people get randomly tested. With the true positive and true negative rates stated above, here are the outcomes:\nProbability Mass Function Given these discrete events, we can chart a probability mass function, also known as discrete density function. We\u0026rsquo;ll import pandas to help us create DataFrames and matplotlib to chart the probability mass function.\nWe first need to turn the counts of events into a DataFrame and change the column to item_counts. Then, we\u0026rsquo;ll calculate the probability of each event by dividing the count by the total number of people in our hypothetical city (i.e., population: 1000).\nOptional: Create another column with abbreviations for test outcome (i.e., \u0026ldquo;True True\u0026rdquo; becomes \u0026ldquo;TT\u0026rdquo;). We\u0026rsquo;ll call this column item2.\nimport pandas as pd import matplotlib.pyplot as plt df = pd.DataFrame.from_dict(counts, orient='index') df = df.rename(columns={0: 'item_counts'}) df['probability'] = df['item_counts']/1000 df['item2'] = ['TT', 'FF', 'FT', 'TF']  Here is the DataFrame we have so far:\nYou\u0026rsquo;ll note that the numbers in the probability column adds up to 1.0 and that the item_counts numbers are the same as the count above when we had calculated the probability of a test-positive person actually having the disease.\nWe\u0026rsquo;ll use a simple bar chart to chart out the diagnostic probabilities and this is how we\u0026rsquo;d visually represent the probability mass function - probabilities of each discrete event; each \u0026lsquo;discrete event\u0026rsquo; is a conditional (e.g., probability that someone has a positive test, given that they have the disease - TT or probability that someone has a negative test, given that they don\u0026rsquo;t have the disease - FF, and so on).\nHere\u0026rsquo;s the code:\ndf = pd.DataFrame.from_dict(counts, orient='index') df = df.rename(columns={0: 'item_counts'}) df['probability'] = df['item_counts']/1000 df['item2'] = ['TT', 'FF', 'FT', 'TF'] plt.bar(df['item2'], df['probability']) plt.title(\u0026quot;Probability Mass Function\u0026quot;) plt.show()  Cumulative Distribution Function While the probability mass function can tell us the probability of each discrete event (i.e., TT, FF, FT, and TF) we can also represent the same information as a cumulative distribution function which allows us to see how the probability changes as we add events together.\nThe cumulative distribution function simply adds the probability from the previous row in a DataFrame in a cumulative fashion, like in the column probability2:\nWe use the cumsum() function to create the cumsum column which is simply adding the item_counts, with each successive row. When we create the corresponding probability column, probability2, it gets larger until we reach 1.0.\nHere\u0026rsquo;s the chart:\nThis chart tells us that the probability of getting both TT and FF (True, True = True Positive, and False, False = True Negative) is 88.6% which indicates that 11.4% (100 - 88.6) of the time, the diagnostic test will let us down.\nNormal Distribution More often than not, you\u0026rsquo;ll be interested in continuous distributions and you can see better see how the cumulative distribution function works.\nYou\u0026rsquo;re probably familiar with the bell shaped curve or the normal distribution, defined solely by its mean (mu) and standard deviation (sigma). If you have a standard normal distribution of probability values, the average would be 0 and the standard deviation would be 1.\nCode:\nimport math SQRT_TWO_PI = math.sqrt(2 * math.pi) def normal_pdf(x: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (SQRT_TWO_PI * sigma)) # plot xs = [x / 10.0 for x in range(-50, 50)] plt.plot(xs, [normal_pdf(x, sigma=1) for x in xs], '-', label='mu=0, sigma=1') plt.show()  With the standard normal distribution curve, you see the average probability is around 0.4. But if you add up the area under the curve (i.e., all probabilities of every possible outcome), you would get 1.0, just like with the medical diagnostic example.\nAnd if you split the bell in half, then flip over the left half, you\u0026rsquo;ll (visually) get the cumulative distribution function:\nCode:\nimport math def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -\u0026gt; float: return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2 # plot xs = [x / 10.0 for x in range(-50, 50)] plt.plot(xs, [normal_cdf(x, sigma=1) for x in xs], '-', label='mu=0,sigma=1')  In both cases, the area under the curve for the standard normal distribution and the cumulative distribution function is 1.0, thus summing the probabilities of all events is one.\nFor more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1606003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606003200,"objectID":"6adc50b9378b5465bd73d7b306e6953d","permalink":"/post/dsfs_6/","publishdate":"2020-11-22T00:00:00Z","relpermalink":"/post/dsfs_6/","section":"post","summary":"Gaining an intuition for probability using Python","tags":["Python","Data Science","Probability","Statistics"],"title":"Data Science from Scratch (ch6) - Probability","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"2021 Goals One of my goals for 2021 is to build up a portfolio of end-to-end machine learning projects. In this post, I\u0026rsquo;ll keep a running list of resources for inspiration:\n Data Science Portfolio Projects: A Step-by-Step Guide (by Felix Vemmer)\nThis is a clear step-by-step guide. I like the emphasis on web scraping which is where I need to focus my skills on next.\n Full Stack Deep Learning (at Berkeley)\nThis looks to be a promising course that covers: \u0026ldquo;a promising experiment to a shipped product: project structure, useful tooling, data management, best practices for deployment, social responsibility, and finding a job or starting a venture\u0026rdquo;. The course is entirely online. See this tweet thread\n Applied ML in Production by Goku Mohandas\nThis aims to be a \u0026ldquo;guide and code-driven case study on MLOps for software engineers, data scientists and product managers\u0026hellip;developing an end-to-end ML feature, from product \u0026ndash;\u0026gt; ML \u0026ndash;\u0026gt; production, with open source tools\u0026rdquo;. Sounds very promising.\n End-to-End Machine Learning Course Catalog by Brandon Rohrer\n First 30 days of Machine Learning\nThis tweet thread by Pratham Prasoon, as the title suggests, is for newcomers to ML, but I think by the end of the sequence (doesn\u0026rsquo;t have to be 30 days) there\u0026rsquo;s a Kaggle project to complete. note: this is not ML-in-production like some of the other resources, but Kaggle projects are great for learning.\nHe has another thread worth checking out.\n Suggested Project from Jan Giacomelli\nThis is a pretty 🔥 thread. He suggests:\n Build an expense tracker CLI app:  Each expensee should have the following: title (string), amount(float), created_at(date), tags(list of strings)\n2 Add Database\nInstead of storing/reading in/from TXT file, start using SQLite. Write script to copy all of the existing expenses from TXT file to database. Don\u0026rsquo;t use ORM at this point.\nStart using Classes  Represent expense with class Expense having attributes: title(string), amount(float), created_at(date), tags(list of strings).\nRepresent Database with class ExpenseRepository with methods: save, get_by_id, list, delete\nRe-write App to use Commands and Queries  Each command/query is a class with method execute. At initialization you need to provide all required data for execution.\nCommands: AddExpense, EditExpense Queries: GetById, ListAll\nSee this post on Modern Test-Driven Development in Python\nAdd Tests  Add tests for commands and queries\nExample: GIVEN Valid data WHEN execute method is called on AddExpense command THEN record is created in database with same attributes as provided\nSee this post on Modern Test-Driven Development in Python\nFlask  Use Flask to build the web application for your expense tracker. Reuse commands and queries inside views Use Jinja2 for HTML templating Add integration tests for endpoints\nPostgreSQL  Start using PostgreSQL instead of SQLite. You should only edit ExpenseRepository. Create script to copy all existing data from SQLite to Postgres\nAuthentication  Add sign up and login to your Flask application Protect endpoints for expenses to allow only logged in users to use them Allow user to only see own expenses.\nDockerize and Deploy  Dockerize your Flask application Deploy to Heroku (don\u0026rsquo;t use DB in docker, use it on Heroku)\nSee this post on Dockerizing Flask with Postgres, Gunicorn and Nginx\nStart using your application for real  Start tracking your expenses Even the most little ones Don\u0026rsquo;t forget to add them daily\nData Analysis  Use Pandas and Matplotlib to analyze your expenses Check frequency, check biggest amount, smallest amount, average amount, most frequent amount and most used tags\u0026hellip;\nDraw plots: Number of expenses per day, amount spent per day\nML  Build model which will predict tags based on the title of expense Use your existing records Although your data set is small, try to build model as precise as possible\nCongratulate yourself  Don\u0026rsquo;t forget to write a blog post for each of these steps. Don\u0026rsquo;t forget to share your code in a public git repository (GitHub) Don\u0026rsquo;t forget to tweet it out Don\u0026rsquo;t forget to add all the skills to LinkedIn\nFor more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1605916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606262400,"objectID":"e0fc9aeabc9289dae23d5b318dcfcc67","permalink":"/post/end_to_end/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/post/end_to_end/","section":"post","summary":"A collection of inspiration for my own projects","tags":["Data Science","Machine Learning"],"title":"End-to-End Projects","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Describing Data  Finding Relationships in Data  Overview This post is chapter 5 in continuation of my coverage of Data Science from Scratch by Joel Grus.\nIt should be noted upfront that everything covered in this post can be done more expediently and efficiently in libraries like NumPy as well as the statistics module in Python.\nThe primary value of this book, and by extension this post, in my opinion, is the emphasis on learning how Python primitives can be used to build tools from the ground up.\nSpecifically, we\u0026rsquo;ll examine how specific features of the Python language as well as functions we built in a previous post on linear algebra can be used to build tools used to describe data and relationships within data (aka statistics).\nI think this is pretty cool. Hopefully you agree.\nExample Data This chapter continues the narrative of you as a newly hired data scientist at DataScienster, the social network for data scientists, and your job is to describe how many friends members in this social network has. We have two lists of float to work with. We\u0026rsquo;ll work with num_friends first, then daily_minutes later.\nI wanted this post to be self-contained, and in order to do that we\u0026rsquo;ll have to read in a larger than average list of floats. The alternative would be to get the data directly from the book\u0026rsquo;s github repo (statistics.py)\nnum_friends = [100.0,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1] daily_minutes = [1,68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84] daily_hours = [dm / 60 for dm in daily_minutes]  Describing The num_friends list is a list of numbers representing \u0026ldquo;number of friends\u0026rdquo; a person has, so for example, one person has 100 friends. The first thing we do to describe the data is to create a bar chart plotting the number of people who have 100 friends, 49 friends, 41 friends, and so on.\nWe\u0026rsquo;ll import Counter from collections and import matplotlib.pyplot.\nWe\u0026rsquo;ll use Counter to turn num_friends list into a defaultdict(int)-like object mapping keys to counts. For more info, please refer to this previous post on the Counters.\nOnce we use the Counter collection, a high-performance container datatype, we can use methods like most_common to find the keys with the most common values. Here we see that the five most common number of friends are 6, 1, 4, 3 and 9, respectively.\nfrom collections import Counter import matplotlib.pyplot as plt friend_counts = Counter(num_friends) # the five most common values are: 6, 1, 4, 3 and 9 friends # [(6, 22), (1, 22), (4, 20), (3, 20), (9, 18)] friend_counts.most_common(5)  To proceed with plotting, we\u0026rsquo;ll use friend_counts to create a list comprehension that will loop through friends_count and for all keys from 0-101 (xs) and print a corresponding value (if it exists). This becomes the y-axis to num_friends, which is the x-axis:\nxs = range(101) # x-axis: largest num_friend value is 100 ys = [friend_counts[x] for x in xs] # y-axis plt.bar(xs, ys) plt.axis([0, 101, 0, 25]) plt.title(\u0026quot;Histogram of Friend Counts\u0026quot;) plt.xlabel(\u0026quot;# of friends\u0026quot;) plt.ylabel(\u0026quot;# of people\u0026quot;) plt.show()  Here is the plot below. You can see one person with 100 friends.\nYou can also read more about data visualization here.\nAlternatively, we could generate simple statistics to describe the data using built-in Python methods: len, min, max and sorted.\nnum_points = len(num_friends) # number of data points in num_friends: 204 largest_value = max(num_friends) # largest value in num_friends: 100 smallest_value = min(num_friends) # smallest value in num_friends: 1 sorted_values = sorted(num_friends) # sort the values in ascending order second_largest_value = sorted_values[-2] # second largest value from the back: 49  Central Tendencies The most common way of describing a set of data is to find it\u0026rsquo;s mean, which is the sum of all the values, divided by the number of values. note : we\u0026rsquo;ll continue to use type annotations. In my opinion, it helps you be a more deliberate and mindful Python programmer.\nfrom typing import List def mean(xs: List[float]) -\u0026gt; float: return sum(xs) / len(xs) assert 7.3333 \u0026lt; mean(num_friends) \u0026lt; 7.3334  However, the mean is notoriously sensitive to outliers so statisticians often supplement with other measures of central tendencies like median. Because the median is the middle-most value, it matters whether there is an even or odd number of data points.\nHere, we\u0026rsquo;ll create two private functions for both situations - even and odd number of data points - in calculating the median. First, we\u0026rsquo;ll sort the data values. Then, for even number values, we\u0026rsquo;ll find the two middle values and split them. For odd number of values, we\u0026rsquo;ll divide the length of the dataset by 2 (i.e., 50).\nOur median function will return either of the private function _median_even or _median_odd conditionally depending on if the length of a list of numbers is divisible (%2==0) by 2.\ndef _median_even(xs: List[float]) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;If len(xs) is even, it's the average of the middle two elements\u0026quot;\u0026quot;\u0026quot; sorted_xs = sorted(xs) hi_midpoint = len(xs) // 2 # e.g. length 4 =\u0026gt; hi_midpoint 2 return (sorted_xs[hi_midpoint - 1] + sorted_xs[hi_midpoint]) / 2 def _median_odd(xs: List[float]) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;If len(xs) is odd, its the middle element\u0026quot;\u0026quot;\u0026quot; return sorted(xs)[len(xs) // 2] def median(v: List[float]) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Finds the 'middle-most' value of v\u0026quot;\u0026quot;\u0026quot; return _median_even(v) if len(v) % 2 == 0 else _median_odd(v) assert median([1,10,2,9,5]) == 5 assert median([1, 9, 2, 10]) == (2 + 9) / 2  Because the median is the middle-most value, it does not fully depend on every value in the data. For illustration, hypothetically if we have a another list num_friends2 where one person had 10,000 friends, the mean would be much more sensitive to that change than the median would be.\nnum_friends2 = [10000.0,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14 ,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9 ,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7 ,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5 ,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3 ,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1 ,1,1,1,1,1,1,1,1,1,1,1,1] mean(num_friends2) # more sensitive to outliers: 7.333 =\u0026gt; 55.86274509803921 median(num_friends2) # less sensitive to outliers: 6.0 =\u0026gt; 6.0  You may also used quantiles to describe your data. Whenever you\u0026rsquo;ve heard \u0026ldquo;X percentile\u0026rdquo;, that is a description of quantiles relative to 100. In fact, the median is the 50th percentile (where 50% of the data lies below this point and 50% lies above).\nBecause quantile is a position from 0-100, the second argument is a float from 0.0 to 1.0. We\u0026rsquo;ll use that float to multiply with the length of the list. Then we\u0026rsquo;ll wrap in int to create an integer index which we\u0026rsquo;ll use on a sorted xs to find the quantile.\ndef quantile(xs: List[float], p: float) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Returns the pth-percentile value in x\u0026quot;\u0026quot;\u0026quot; p_index = int(p * len(xs)) return sorted(xs)[p_index] assert quantile(num_friends, 0.10) == 1 assert quantile(num_friends, 0.25) == 3 assert quantile(num_friends, 0.75) == 9 assert quantile(num_friends, 0.90) == 13  Finally, we have the mode, which looks at the most common values. First, we use the Counter method on our list parameter and since Counter is a subclass of dict we have access to methods like values() to find all the values and items() to find key value pairs.\nWe define max_count to find the max value (22), then the function returns a list comprehension which loops through counts.items() to find the key associated with the max_count (22). That is 1 and 6, meaning twenty-two people (the mode) had one or six friends.\ndef mode(x: List[float]) -\u0026gt; List[float]: \u0026quot;\u0026quot;\u0026quot;Returns a list, since there might be more than one mode\u0026quot;\u0026quot;\u0026quot; counts = Counter(x) max_count = max(counts.values()) return [x_i for x_i, count in counts.items() if count == max_count] assert set(mode(num_friends)) == {1, 6}  Because we had already used Counter on num_friends previously (see friend_counts), we could have just called the most_common(2) method to get the same results:\nmode(num_friends) # [6, 1] friend_counts.most_common(2) # [(6, 22), (1, 22)]  Dispersion Aside from our data\u0026rsquo;s central tendencies, we\u0026rsquo;ll also want to understand it\u0026rsquo;s spread or dispersion. The tools to do this are data_range, variance, standard deviation and interquartile range.\nRange is a straightforward max value minus min value.\nVariance measures how far a set of numbers is from their average value. What\u0026rsquo;s more interesting, for our purpose, is how we need to borrow the functions we had previously built in the linear algebra post to create the variance function.\nIf you look at its wikipedia page, variance is the squared deviation of a variable from its mean.\nFirst, we\u0026rsquo;ll need to create the de_mean function that takes a list of numbers and subtract from all numbers in the list, the mean value (this gives us the deviation from the mean).\nThen, we\u0026rsquo;ll sum_of_squares all those deviations, which means we\u0026rsquo;ll take all the values, multiply them with itself (square it), then add the values (and divide by length of the list minus one) to get the variance.\nRecall that the sum_of_squares is a special case of the dot product function.\n# variance from typing import List Vector = List[float] # see vectors.py in chapter 4 for dot and sum_of_squares def dot(v: Vector, w: Vector) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Computes v_1 * w_1 + ... + v_n * w_n\u0026quot;\u0026quot;\u0026quot; assert len(v) == len(w), \u0026quot;vectors must be the same length\u0026quot; return sum(v_i * w_i for v_i, w_i in zip(v,w)) def sum_of_squares(v: Vector) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Returns v_1 * v_1 + ... + v_n * v_n\u0026quot;\u0026quot;\u0026quot; return dot(v,v) def de_mean(xs: List[float]) -\u0026gt; List[float]: \u0026quot;\u0026quot;\u0026quot;Translate xs by subtracting its mean (so the result has mean 0)\u0026quot;\u0026quot;\u0026quot; x_bar = mean(xs) return [x - x_bar for x in xs] def variance(xs: List[float]) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Almost the average squared deviation from the mean\u0026quot;\u0026quot;\u0026quot; assert len(xs) \u0026gt;= 2, \u0026quot;variance requires at least two elements\u0026quot; n = len(xs) deviations = de_mean(xs) return sum_of_squares(deviations) / (n - 1) assert 81.54 \u0026lt; variance(num_friends) \u0026lt; 81.55  The variance is sum_of_squares deviations, which can be tricky to interpret. For example, we have a num_friends with values ranging from 0 to 100.\n What does a variance of 81.54 mean?\n A more common alternative is the standard deviation. Here we take the square root of the variance using Python\u0026rsquo;s math module.\nWith a standard deviation of 9.03, and we know the mean of num_friends is 7.3, anything below 7 + 9 = 16 or 7 - 9 (0 friends) friends is still within a standard deviation of the mean. And we can check by running friend_counts that most people are within a standard deviation of the mean.\nOn the other hand, we know that someone with 20 friends is more than one standard deviation away from the mean.\nimport math def standard_deviation(xs: List[float]) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;The standard deviation is the square root of the variance\u0026quot;\u0026quot;\u0026quot; return math.sqrt(variance(xs)) assert 9.02 \u0026lt; standard_deviation(num_friends) \u0026lt; 9.04  However, because the standard deviation builds on the variance, which is dependent on the mean, we know that just like the mean, it can be sensitive to outliers, we can use an alternative called the interquartile range, which is based on the median and less sensitive to outliers.\nSpecifically, the interquartile range can be used to examine num_friends between the 25th and 75th percentile. A large chunk of people are going to have around 6 friends.\ndef interquartile_range(xs: List[float]) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Returns the difference between the 75%-ile and the 25%-ile\u0026quot;\u0026quot;\u0026quot; return quantile(xs, 0.75) - quantile(xs, 0.25) assert interquartile_range(num_friends) == 6  Now that we describe a single list of data, we\u0026rsquo;ll also want to look at potential relationship between two data sources. For example, we may have a hypothesis that the amount of time spent on the DataScienster social network is somehow related to the number of friends someone has.\nWe\u0026rsquo;ll examine covariance and correlations next.\nCorrelation If variance is how much a single set of numbers deviates from its mean (i.e., see de_mean above), then covariance measures how two sets of numbers vary from their means. With the idea that if they co-vary the same amount, then they could be related.\nHere we\u0026rsquo;ll borrow the dot production function we developed in the linear algebra post.\nMoreover, we\u0026rsquo;ll examine if there\u0026rsquo;s a relationship between num_friends and daily_minutes and daily_hours (see above).\ndef covariance(xs: List[float], ys: List[float]) -\u0026gt; float: assert len(xs) == len(ys), \u0026quot;xs and ys must have same number of elements\u0026quot; return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1) assert 22.42 \u0026lt; covariance(num_friends, daily_minutes) \u0026lt; 22.43 assert 22.42 / 60 \u0026lt; covariance(num_friends, daily_hours) \u0026lt; 22.43 / 60  As with variance, a similar critique can be made of covariance, you have to do extra steps to interpret it. For example, the covariance of num_friends and daily_minutes is 22.43.\n What does that mean? Is that considered a strong relationship?\n A more intuitive measure would be a correlation:\ndef correlation(xs: List[float], ys: List[float]) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Measures how much xs and ys vary in tandem about their means\u0026quot;\u0026quot;\u0026quot; stdev_x = standard_deviation(xs) stdev_y = standard_deviation(ys) if stdev_x \u0026gt; 0 and stdev_y \u0026gt; 0: return covariance(xs,ys) / stdev_x / stdev_y else: return 0 # if no variation, correlation is zero assert 0.24 \u0026lt; correlation(num_friends, daily_minutes) \u0026lt; 0.25 assert 0.24 \u0026lt; correlation(num_friends, daily_hours) \u0026lt; 0.25  By dividing out the standard deviation of both input variables, correlation is always between -1 (perfect (anti) correlation) and 1 (perfect correlation). A correlation of 0.24 is relatively weak correlation (although what is considered weak, moderate, strong depends on the context of the data).\nOne thing to keep in mind is simpson\u0026rsquo;s paradox or when the relationship between two variables change when accounting for a third, confounding variable. Moreover, we should keep this cliché in mind (it\u0026rsquo;s a cliché for a reason): correlation does not imply causation.\nSummary We are just five chapters in and we can begin to see how we\u0026rsquo;re building the tools now, that we\u0026rsquo;ll use later on. Here\u0026rsquo;s a visual summary of what we\u0026rsquo;ve covered in this post and how it connects to previous posts, namely linear algebra and the python crash course.\nFor more content on data science, machine learning, R, Python, SQL and more, find me on Twitter.\n","date":1605744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605744000,"objectID":"71b2b86d50d5b1763f69b8add66d88aa","permalink":"/post/dsfs_5/","publishdate":"2020-11-19T00:00:00Z","relpermalink":"/post/dsfs_5/","section":"post","summary":"Building tools to describe a data and find relationships","tags":["Python","Data Science"],"title":"Data Science from Scratch (ch5) - Statistics","type":"post"},{"authors":[],"categories":[],"content":" Datasaurus Introduction I recently came across the Datasaurus dataset by Alberto Cairo on #TidyTuesday and wanted to create a series of charts illustrating the lessons associated with this dataset, primarily to: never trust summary statistics alone.\nFirst, some context. Here’s Alberto’s original tweet from years ago when he created this dataset:\npng\n This tweet alone doesn’t communicate why we shouldn’t trust summary statistics alone, so let’s unpack this. First we’ll load the various packages and data we’ll use.\n Load Packages library(tidyverse) ## ── Attaching packages ─────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.1 ## ✓ tidyr 1.1.1 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(ggcorrplot) library(ggridges)  Load Data note : datasaurus and datasaurus_dozen are identical. The former is provided via #TidyTuesday, the latter from this research paper discussing more advanced concepts beyond the scope of this document (i.e., simulated annealing).\nYou’ll also note that datasaurus_dozen and datasaurus_wide are the same data, organized differently. The former in long format and the latter, in wide format - see here for details.\nFor the most part, we’ll use datasaurus_dozen throughout this document. We’ll use datasaurus_wide when we get to the correlation section.\ndatasaurus \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-13/datasaurus.csv\u0026#39;) ## Parsed with column specification: ## cols( ## dataset = col_character(), ## x = col_double(), ## y = col_double() ## ) datasaurus_dozen \u0026lt;- read_tsv(\u0026#39;./data/DatasaurusDozen.tsv\u0026#39;) ## Parsed with column specification: ## cols( ## dataset = col_character(), ## x = col_double(), ## y = col_double() ## ) datasaurus_wide \u0026lt;- read_tsv(\u0026#39;./data/DatasaurusDozen-wide.tsv\u0026#39;) ## Warning: Duplicated column names deduplicated: \u0026#39;away\u0026#39; =\u0026gt; \u0026#39;away_1\u0026#39; [2], ## \u0026#39;bullseye\u0026#39; =\u0026gt; \u0026#39;bullseye_1\u0026#39; [4], \u0026#39;circle\u0026#39; =\u0026gt; \u0026#39;circle_1\u0026#39; [6], \u0026#39;dino\u0026#39; =\u0026gt; ## \u0026#39;dino_1\u0026#39; [8], \u0026#39;dots\u0026#39; =\u0026gt; \u0026#39;dots_1\u0026#39; [10], \u0026#39;h_lines\u0026#39; =\u0026gt; \u0026#39;h_lines_1\u0026#39; [12], ## \u0026#39;high_lines\u0026#39; =\u0026gt; \u0026#39;high_lines_1\u0026#39; [14], \u0026#39;slant_down\u0026#39; =\u0026gt; \u0026#39;slant_down_1\u0026#39; [16], ## \u0026#39;slant_up\u0026#39; =\u0026gt; \u0026#39;slant_up_1\u0026#39; [18], \u0026#39;star\u0026#39; =\u0026gt; \u0026#39;star_1\u0026#39; [20], \u0026#39;v_lines\u0026#39; ## =\u0026gt; \u0026#39;v_lines_1\u0026#39; [22], \u0026#39;wide_lines\u0026#39; =\u0026gt; \u0026#39;wide_lines_1\u0026#39; [24], \u0026#39;x_shape\u0026#39; =\u0026gt; ## \u0026#39;x_shape_1\u0026#39; [26] ## Parsed with column specification: ## cols( ## .default = col_character() ## ) ## See spec(...) for full column specifications.  Eyeballing the data Here are the first six rows of datasaurus_dozen (long):\n## # A tibble: 6 x 3 ## dataset x y ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 dino 55.4 97.2 ## 2 dino 51.5 96.0 ## 3 dino 46.2 94.5 ## 4 dino 42.8 91.4 ## 5 dino 40.8 88.3 ## 6 dino 38.7 84.9 Here are the first six rows of datasaurus_wide (wide):\n## # A tibble: 6 x 26 ## away away_1 bullseye bullseye_1 circle circle_1 dino dino_1 dots dots_1 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 x y x y x y x y x y ## 2 32.3… 61.41… 51.2038… 83.339776… 55.99… 79.2772… 55.3… 97.17… 51.1… 90.86… ## 3 53.4… 26.18… 58.9744… 85.499817… 50.03… 79.0130… 51.5… 96.02… 50.5… 89.10… ## 4 63.9… 30.83… 51.8720… 85.829737… 51.28… 82.4359… 46.1… 94.48… 50.2… 85.46… ## 5 70.2… 82.53… 48.1799… 85.045116… 51.17… 79.1652… 42.8… 91.41… 50.0… 83.05… ## 6 34.1… 45.73… 41.6832… 84.017940… 44.37… 78.1646… 40.7… 88.33… 50.5… 82.93… ## # … with 16 more variables: h_lines \u0026lt;chr\u0026gt;, h_lines_1 \u0026lt;chr\u0026gt;, high_lines \u0026lt;chr\u0026gt;, ## # high_lines_1 \u0026lt;chr\u0026gt;, slant_down \u0026lt;chr\u0026gt;, slant_down_1 \u0026lt;chr\u0026gt;, slant_up \u0026lt;chr\u0026gt;, ## # slant_up_1 \u0026lt;chr\u0026gt;, star \u0026lt;chr\u0026gt;, star_1 \u0026lt;chr\u0026gt;, v_lines \u0026lt;chr\u0026gt;, v_lines_1 \u0026lt;chr\u0026gt;, ## # wide_lines \u0026lt;chr\u0026gt;, wide_lines_1 \u0026lt;chr\u0026gt;, x_shape \u0026lt;chr\u0026gt;, x_shape_1 \u0026lt;chr\u0026gt; There are 13 variables, each with X- and Y- axes.\n Summary Statistics First, we’ll note that if we just look at summary statistics (i.e., mean and standard deviation), we might conclude that these variables are all the same. Moreover, within each variable, x and y values have very similarly low correlations at ranging from -0.06 to -0.07.\ndatasaurus_dozen %\u0026gt;% group_by(dataset) %\u0026gt;% summarize( x_mean = mean(x), x_sd = sd(x), y_mean = mean(y), y_sd = sd(y), corr = cor(x,y) ) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 13 x 6 ## dataset x_mean x_sd y_mean y_sd corr ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 away 54.3 16.8 47.8 26.9 -0.0641 ## 2 bullseye 54.3 16.8 47.8 26.9 -0.0686 ## 3 circle 54.3 16.8 47.8 26.9 -0.0683 ## 4 dino 54.3 16.8 47.8 26.9 -0.0645 ## 5 dots 54.3 16.8 47.8 26.9 -0.0603 ## 6 h_lines 54.3 16.8 47.8 26.9 -0.0617 ## 7 high_lines 54.3 16.8 47.8 26.9 -0.0685 ## 8 slant_down 54.3 16.8 47.8 26.9 -0.0690 ## 9 slant_up 54.3 16.8 47.8 26.9 -0.0686 ## 10 star 54.3 16.8 47.8 26.9 -0.0630 ## 11 v_lines 54.3 16.8 47.8 26.9 -0.0694 ## 12 wide_lines 54.3 16.8 47.8 26.9 -0.0666 ## 13 x_shape 54.3 16.8 47.8 26.9 -0.0656  Boxplots You could use boxplots to show slight variation in the distribution and median values of these 13 variables. However, the mean values, indicated with the red circles, are identical.\ndatasaurus_dozen %\u0026gt;% ggplot(aes(x = dataset, y = x, fill = dataset)) + geom_boxplot(alpha = 0.6) + stat_summary(fun = mean, geom = \u0026quot;point\u0026quot;, shape = 20, size = 6, color = \u0026quot;red\u0026quot;, fill = \u0026quot;red\u0026quot;) + scale_fill_brewer(palette = \u0026quot;Set3\u0026quot;) + theme_classic() + theme(legend.position = \u0026#39;none\u0026#39;) + labs( y = \u0026#39;13 variables\u0026#39;, x = \u0026#39;X-values\u0026#39;, title = \u0026quot;Boxplots: Slight differences in the distribution and median values (X-axis)\u0026quot;, subtitle = \u0026quot;Identical mean values\u0026quot; ) ## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Set3 is 12 ## Returning the palette you asked for with that many colors Here’s the same plot for y values:\ndatasaurus_dozen %\u0026gt;% ggplot(aes(x = dataset, y = y, fill = dataset)) + geom_boxplot(alpha = 0.6) + stat_summary(fun = mean, geom = \u0026quot;point\u0026quot;, shape = 20, size = 6, color = \u0026quot;red\u0026quot;, fill = \u0026quot;red\u0026quot;) + scale_fill_brewer(palette = \u0026quot;Paired\u0026quot;) + theme_classic() + theme(legend.position = \u0026#39;none\u0026#39;) + labs( y = \u0026#39;13 variables\u0026#39;, x = \u0026#39;Y-values\u0026#39;, title = \u0026quot;Boxplots: Slight differences in the distribution and median values (Y-axis)\u0026quot;, subtitle = \u0026quot;Identical mean values\u0026quot; ) ## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Paired is 12 ## Returning the palette you asked for with that many colors  Ridgeline Plot We can begin to get a sense for how these variables are different if we plot the distribution in different ways. The ridgeline plot begins to reveal aspects of the data that were hidden before.\nWe can begin to see that certain variables have markedly different distribution shapes (i.e., v_lines, dots, x_shape, wide_lines), while having the same mean value.\ndatasaurus_dozen %\u0026gt;% ggplot(aes(x = x, y = dataset, fill = dataset)) + geom_density_ridges_gradient(scale = 3, quantile_lines = T, quantile_fun = mean) + scale_fill_manual(values = c(\u0026#39;#a6cee3\u0026#39;, \u0026#39;#1f78b4\u0026#39;, \u0026#39;#b2df8a\u0026#39;, \u0026#39;#33a02c\u0026#39;, \u0026#39;#fb9a99\u0026#39;, \u0026#39;#e31a1c\u0026#39;, \u0026#39;#fdbf6f\u0026#39;, \u0026#39;#ff7f00\u0026#39;, \u0026#39;#cab2d6\u0026#39;, \u0026#39;#6a3d9a\u0026#39;, \u0026#39;#ffff99\u0026#39;, \u0026#39;#b15928\u0026#39;, \u0026#39;grey\u0026#39;)) + theme_classic() + theme(legend.position = \u0026#39;none\u0026#39;) + labs( x = \u0026quot;X-values\u0026quot;, y = \u0026quot;13 variables\u0026quot;, title = \u0026quot;Ridgeline Plot: More variation in the distribution (X-axis)\u0026quot;, subtitle = \u0026quot;Identical mean values\u0026quot; ) ## Picking joint bandwidth of 5.46 For y values, high_lines, dots, circle and star have obviously different distributions from the rest. Again, the mean values are identical across variables.\ndatasaurus_dozen %\u0026gt;% ggplot(aes(x = y, y = dataset, fill = dataset)) + geom_density_ridges_gradient(scale = 3, quantile_lines = T, quantile_fun = mean) + scale_fill_manual(values = c(\u0026#39;#a6cee3\u0026#39;, \u0026#39;#1f78b4\u0026#39;, \u0026#39;#b2df8a\u0026#39;, \u0026#39;#33a02c\u0026#39;, \u0026#39;#fb9a99\u0026#39;, \u0026#39;#e31a1c\u0026#39;, \u0026#39;#fdbf6f\u0026#39;, \u0026#39;#ff7f00\u0026#39;, \u0026#39;#cab2d6\u0026#39;, \u0026#39;#6a3d9a\u0026#39;, \u0026#39;#ffff99\u0026#39;, \u0026#39;#b15928\u0026#39;, \u0026#39;grey\u0026#39;)) + theme_classic() + theme(legend.position = \u0026#39;none\u0026#39;) + labs( x = \u0026quot;Y-values\u0026quot;, y = \u0026quot;13 variables\u0026quot;, title = \u0026quot;Ridgeline Plot: More variation in the distribution (Y-axis)\u0026quot;, subtitle = \u0026quot;Identical mean values\u0026quot; ) ## Picking joint bandwidth of 9  Correlations If you skip visualizing the distribution and central tendencies and go straight to seeing how the variables correlate with each other, you could also miss some fundamental differences in the data.\nIn particular, the x and y values across all 13 variables are highlight correlated. With just knowledge of the summary statistics, one could be led to believe that these variables are highly similar.\nBelow is an abbreviated correlation matrix.\nlibrary(ggcorrplot) # X-values # selecting rows 2-143 # turning all values from character to numeric datasaurus_wide_x \u0026lt;- datasaurus_wide %\u0026gt;% slice(2:143) %\u0026gt;% select(away, bullseye, circle, dino, dots, h_lines, high_lines, slant_down, slant_up, star, v_lines, wide_lines, x_shape) %\u0026gt;% mutate_if(is.character, as.numeric) # Y-values # selecting rows 2-143 # turning all values from character to numeric datasaurus_wide_y \u0026lt;- datasaurus_wide %\u0026gt;% slice(2:143) %\u0026gt;% select(away_1, bullseye_1, circle_1, dino_1, dots_1, h_lines_1, high_lines_1, slant_down_1, slant_up_1, star_1, v_lines_1, wide_lines_1, x_shape_1) %\u0026gt;% mutate_if(is.character, as.numeric) # correlation matrix for X values corr_x \u0026lt;- round(cor(datasaurus_wide_x), 1) # correlation matrix for Y values corr_y \u0026lt;- round(cor(datasaurus_wide_y), 1) head(corr_x[, 1:6]) ## away bullseye circle dino dots h_lines ## away 1.0 -0.3 -0.3 -0.3 -0.3 -0.3 ## bullseye -0.3 1.0 0.9 0.9 0.9 0.9 ## circle -0.3 0.9 1.0 0.9 0.8 0.9 ## dino -0.3 0.9 0.9 1.0 0.9 1.0 ## dots -0.3 0.9 0.8 0.9 1.0 0.9 ## h_lines -0.3 0.9 0.9 1.0 0.9 1.0 Visualizing the correlation matrix Here is a correlation between the x-values between all 13 variables. You can see that all variables, aside from away, are highly correlated with each other.\n# correlation between X-values ggcorrplot(corr_x, hc.order = TRUE, type=\u0026quot;lower\u0026quot;, outline.color = \u0026quot;white\u0026quot;, ggtheme = ggplot2::theme_gray, colors = c(\u0026quot;#d8b365\u0026quot;, \u0026quot;#f5f5f5\u0026quot;, \u0026quot;#5ab4ac\u0026quot;), lab = TRUE)  Here is a correlation between the ‘y-values’ between all 13 variables. Again, aside from away, all the variables are highly correlated with each other.\n# correlation between Y-values ggcorrplot(corr_y, hc.order = TRUE, type=\u0026quot;lower\u0026quot;, outline.color = \u0026quot;white\u0026quot;, ggtheme = ggplot2::theme_gray, colors = c(\u0026quot;#ef8a62\u0026quot;, \u0026quot;#f7f7f7\u0026quot;, \u0026quot;#67a9cf\u0026quot;), lab = TRUE)   Facets At this point, the boxplots show us variables with similar median and identical mean; the ridgelines begin to show us that some variables have different distributions. And the correlation matrix suggests the variables are more similar than not.\nTo really see their differences, we’ll need to use facet_wrap.\nHere we’ll use facet_wrap to examine the histogram for x and y values of all 13 variables. We started to see the differences in distribution between variables from the ridgeline plots, but overlapping histograms provide another perspective.\n# facet histogram (both-values) datasaurus_dozen %\u0026gt;% group_by(dataset) %\u0026gt;% ggplot() + geom_histogram(aes(x=x, fill=\u0026#39;red\u0026#39;), alpha = 0.5, bins = 30) + geom_histogram(aes(x=y, fill=\u0026#39;green\u0026#39;), alpha = 0.5, bins = 30) + facet_wrap(~dataset) + scale_fill_discrete(labels = c(\u0026#39;y\u0026#39;, \u0026#39;x\u0026#39;)) + theme_classic() + labs( fill = \u0026#39;Axes\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;Count\u0026#39;, title = \u0026#39;Faceted Histogram: x- and y-values\u0026#39; )  Scatter Plot However, if there’s one thing this dataset is trying to communicate its that there’s no subtitute for plotting the actual data points. No amount of summary statistics, central tendency or distribution is going to replace plotting actually data points.\nOnce we create the scatter plot with geom_point, we see the big reveal with this dataset. That despite the similarities in central measures, for the most part similar distributions and high correlations, the 13 variables are wildly different from each other.\ndatasaurus_dozen %\u0026gt;% group_by(dataset) %\u0026gt;% ggplot(aes(x=x, y=y, color=dataset)) + geom_point(alpha = 0.5) + facet_wrap(~dataset) + scale_color_manual(values = c(\u0026#39;#a6cee3\u0026#39;, \u0026#39;#1f78b4\u0026#39;, \u0026#39;#b2df8a\u0026#39;, \u0026#39;#33a02c\u0026#39;, \u0026#39;#fb9a99\u0026#39;, \u0026#39;#e31a1c\u0026#39;, \u0026#39;#fdbf6f\u0026#39;, \u0026#39;#ff7f00\u0026#39;, \u0026#39;#cab2d6\u0026#39;, \u0026#39;#6a3d9a\u0026#39;, \u0026#39;#ffff99\u0026#39;, \u0026#39;#b15928\u0026#39;, \u0026#39;grey\u0026#39;)) + theme_classic() + theme(legend.position = \u0026quot;none\u0026quot;) + labs( x = \u0026#39;X-axis\u0026#39;, y = \u0026#39;Y-axis\u0026#39;, title = \u0026#39;Faceted Scatter Plot\u0026#39; ) There are other less common alternatives to the scatter plot.\n Geom Density 2D While not as clear as the scatter plot, plotting the contours of a 2D density estimate does show how very different the variables are from each other, despite similar summary statistics.\n# contours of a 2D Density estimate datasaurus_dozen %\u0026gt;% ggplot(aes(x=x, y=y)) + geom_density_2d() + theme_classic() + facet_wrap(~dataset) + labs( x = \u0026#39;X-axis\u0026#39;, y = \u0026#39;Y-axis\u0026#39;, title = \u0026#39;Contours of a 2D density estimate\u0026#39; ) This is a slight variation using stat_density_2d:\n# stat density 2d datasaurus_dozen %\u0026gt;% ggplot(aes(x=x, y=y)) + stat_density_2d(aes(fill=y), geom = \u0026quot;polygon\u0026quot;, colour = \u0026#39;white\u0026#39;) + theme_classic() + facet_wrap(~dataset) + labs( x = \u0026#39;X-axis\u0026#39;, y = \u0026#39;Y-axis\u0026#39;, title = \u0026#39;Stat Density 2D estimate\u0026#39; ) Using the density_2d plots are quite effective in showing how different the variables are and serve as a nice alternative to the more familiar scatter plot.\nHopefully this vignette illustrates the importance of never trusting summary statistics (alone). Moreover, when visualizing, we should go beyond simply visualizing the data’s distribution or central tendency, but plotting the actually data points.\n ","date":1605484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605533052,"objectID":"f3528567427912366c839305bbf2168c","permalink":"/post/datasaurus/","publishdate":"2020-11-16T00:00:00Z","relpermalink":"/post/datasaurus/","section":"post","summary":"Demonstrating the importance of not (just) trusting summary statistics","tags":["Data Viz","R Markdown","Statistics","RStats"],"title":"Going beyond summary statistics","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of contents   Vectors  Matrices  Overview We\u0026rsquo;ll see the from scratch aspect of the book play out as we implement several building block functions to help us work towards defining the Euclidean Distance in code:\nWhile we don\u0026rsquo;t see its application immediately, we can expect to see the Euclidean Distance used for K-nearest neighbors (classication) or K-means (clustering) to find the \u0026ldquo;k closest points\u0026rdquo; ( source). (note : there are other types of distance formulas used as well.)\nEn route towards implementing the Euclidean Distance, we also implement the sum of squares which is a crucial piece for how regression works.\nThus, the from scratch aspect of this book works on two levels. Within this chapter, we\u0026rsquo;re building piece by piece up to an important distance and sum of squares formula. But we\u0026rsquo;re also building tools we\u0026rsquo;ll use in subsequent chapters.\nVectors We start off with implementing functions to add and subtract two vectors. We also create a function for component wise sum of a list of vectors, where a new vector is created whose first element is the sum of all the first elements in the list and so on.\nWe then create a function to multiply a vector by scalar, which we use to compute the component wise mean of a list of vectors.\nWe also create the dot product of two vectors or the sum of their component wise product, and this is is the generalize version of the sum of squares. At this point, we have enough to implement the Euclidean distance. Let\u0026rsquo;s take a look at the code:\nExample Vectors Vectors are simply a list of numbers:\nheight_weight_age = [70,170,40] grades = [95,80,75,62]  Add You\u0026rsquo;ll note that we do type annotation on our code throughout. This is a convention advocated by the author (and as a newcomer to Python, I like the idea of being explicit about data type for a function\u0026rsquo;s input and output).\nfrom typing import List Vector = List[float] def add(v: Vector, w: Vector) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Adds corresponding elements\u0026quot;\u0026quot;\u0026quot; assert len(v) == len(w), \u0026quot;vectors must be the same length\u0026quot; return [v_i + w_i for v_i, w_i in zip(v,w)] assert add([1,2,3], [4,5,6]) == [5,7,9]  Here\u0026rsquo;s another view of what\u0026rsquo;s going on with the add function:\nSubtract def subtract(v: Vector, w: Vector) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Subtracts corresponding elements\u0026quot;\u0026quot;\u0026quot; assert len(v) == len(w), \u0026quot;vectors must be the same length\u0026quot; return [v_i - w_i for v_i, w_i in zip(v,w)] assert subtract([5,7,9], [4,5,6]) == [1,2,3]  This is pretty much the same as the previous:\nComponentwise Sum def vector_sum(vectors: List[Vector]) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Sum all corresponding elements (componentwise sum)\u0026quot;\u0026quot;\u0026quot; # Check that vectors is not empty assert vectors, \u0026quot;no vectors provided!\u0026quot; # Check the vectorss are all the same size num_elements = len(vectors[0]) assert all(len(v) == num_elements for v in vectors), \u0026quot;different sizes!\u0026quot; # the i-th element of the result is the sum of every vector[i] return [sum(vector[i] for vector in vectors) for i in range(num_elements)] assert vector_sum([[1,2], [3,4], [5,6], [7,8]]) == [16,20]  Here, a list of vectors becomes one vector. If you go back to the add function, it takes two vectors, so if we tried to give it four vectors, we\u0026rsquo;d get a TypeError. So we wrap four vectors in a list and provide that as the argument for vector_sum:\nMultiply Vector with a Number def scalar_multiply(c: float, v: Vector) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Multiplies every element by c\u0026quot;\u0026quot;\u0026quot; return [c * v_i for v_i in v] assert scalar_multiply(2, [2,4,6]) == [4,8,12]  One number is multiplied with all numbers in the vector, with the vector retaining its length:\nComponentwise Mean This is similar to componentwise sum (see above); a list of vectors becomes one vector.\ndef vector_mean(vectors: List[Vector]) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Computes the element-wise average\u0026quot;\u0026quot;\u0026quot; n = len(vectors) return scalar_multiply(1/n, vector_sum(vectors)) assert vector_mean([ [1,2], [3,4], [5,6] ]) == [3,4]  Dot Product def dot(v: Vector, w: Vector) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Computes v_1 * w_1 + ... + v_n * w_n\u0026quot;\u0026quot;\u0026quot; assert len(v) == len(w), \u0026quot;vectors must be the same length\u0026quot; return sum(v_i * w_i for v_i, w_i in zip(v,w)) assert dot([1,2,3], [4,5,6]) == 32  Here we multiply the elements, then sum their results. Two vectors becomes a single number (float):\nSum of Squares def sum_of_squares(v: Vector) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Returns v_1 * v_1 + ... + v_n * v_n\u0026quot;\u0026quot;\u0026quot; return dot(v,v) assert sum_of_squares([1,2,3]) == 14  In fact, sum_of_squares is a special case of dot product:\nMagnitude def magnitude(v: Vector) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Returns the magnitude (or length) of v\u0026quot;\u0026quot;\u0026quot; return math.sqrt(sum_of_squares(v)) # math.sqrt is the square root function assert magnitude([3,4]) == 5  With magnitude we square root the sum_of_squares. This is none other than the pythagorean theorem.\nSquared Distance def squared_distance(v: Vector, w: Vector) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\u0026quot;\u0026quot;\u0026quot; return sum_of_squares(subtract(v,w))  This is the distance between two vectors, squared.\n(Euclidean) Distance import math def distance(v: Vector, w: Vector) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Also computes the distance between v and w\u0026quot;\u0026quot;\u0026quot; return math.sqrt(squared_distance(v,w))  Finally, we square root the squared_distance to get the (euclidean) distance:\nSummary We literally built from scratch, albeit with some help from Python\u0026rsquo;s math module, the blocks needed for essential functions that we\u0026rsquo;ll expect to use later, namely: the sum_of_squares and distance.\nIt\u0026rsquo;s pretty cool to see these foundational concepts set us up to understand more complex machine learning algorithms like regression, k-nearest neighbors (classification), k-means (clustering) and even touch on the pythagorean theorem.\nWe\u0026rsquo;ll examine matrices next.\nMatrices The first thing to note is that matrices are represented as lists of lists which is explicit with type annotation:\nfrom typing import List Matrix = List[List[float]]  You might bet wondering if a list of lists is somehow different from a list of vectors we saw previously with the vector_sum function. To see, I used type annotation to try to define the arguments differently.\nHere\u0026rsquo;s the vector_sum function we defined previously:\ndef vector_sum(vectors: List[Vector]) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Sum all corresponding elements (componentwise sum)\u0026quot;\u0026quot;\u0026quot; # Check that vectors is not empty assert vectors, \u0026quot;no vectors provided!\u0026quot; # Check the vectorss are all the same size num_elements = len(vectors[0]) assert all(len(v) == num_elements for v in vectors), \u0026quot;different sizes!\u0026quot; # the i-th element of the result is the sum of every vector[i] return [sum(vector[i] for vector in vectors) for i in range(num_elements)] assert vector_sum([[1,2], [3,4], [5,6], [7,8]]) == [16,20]  Here\u0026rsquo;s a new function, vector_sum2 defined differently with type annotation:\ndef vector_sum2(lists: List[List[float]]) -\u0026gt; List: \u0026quot;\u0026quot;\u0026quot;Sum all corresponding list (componentwise sum?)\u0026quot;\u0026quot;\u0026quot; assert lists, \u0026quot;this list is empty!\u0026quot; # check that lists are the same size num_lists = len(lists[0]) assert all(len(l) == num_lists for l in lists), \u0026quot;different sizes!\u0026quot; # the i-th list is the sum of every list[i] return [sum(l[i] for l in lists) for i in range(num_lists)] assert vector_sum2([[1,2], [3,4], [5,6], [7,8]]) == [16,20]  I did a variety of things to see if vector_sum and vector_sum2 behaved differently, but they appear to be identical:\n# both are functions assert callable(vector_sum) == True assert callable(vector_sum2) == True # when taking the same argument, they both return a list type(vector_sum([[1,2], [3,4], [5,6], [7,8]])) #list type(vector_sum2([[1,2], [3,4], [5,6], [7,8]])) #list # the same input yields the same output vector_sum([[1,2],[3,4]]) # [4,6] vector_sum2([[1,2],[3,4]]) # [4,6]  To keep it simple, in the context of matrices, you can think of vectors as the rows of the matrix.\nFor example, if we represent the small dataset below as a matrix, we can think of columns as variables like: height, weight, age; and each row as a person:\nsample_data = [[70, 170, 40], [65, 120, 26], [77, 250, 19]]  By extension of rows and columns, we can write a function for the shape of a matrix. This below shape function takes in a matrix and returns a tuple with two integers, number of rows and number of columns:\nfrom typing import Tuple def shape(A: Matrix) -\u0026gt; Tuple[int, int]: \u0026quot;\u0026quot;\u0026quot;Returns (# of rows of A, # of columns of A)\u0026quot;\u0026quot;\u0026quot; num_rows = len(A) num_cols = len(A[0]) if A else 0 # number of elements in first row return num_rows, num_cols assert shape([[1,2,3], [4,5,6]]) == (2,3) # 2 rows, 3 columns assert shape(sample_data) == (3,3)  We can actually write functions to grab either a specific row or a specific columns :\nVector = List[float] # rows def get_row(A: Matrix, i: int) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Returns the i-th row of A (as a Vector)\u0026quot;\u0026quot;\u0026quot; return A[i] # A[i] is already the ith row # column def get_column(A: Matrix, i: int) -\u0026gt; Vector: \u0026quot;\u0026quot;\u0026quot;Returns the j-th column of A (as a Vector)\u0026quot;\u0026quot;\u0026quot; return [A_i[j] for A_i in A]  Now, going beyond finding the shape, rows and columns of an existing matrix, we\u0026rsquo;ll also want to create matrices and we\u0026rsquo;ll do that using nested list comprehensions:\nfrom typing import Callable def make_matrix(num_rows: int, num_cols: int, entry_fn: Callable[[int, int], float]) -\u0026gt; Matrix: \u0026quot;\u0026quot;\u0026quot; Returns a num_rows x num_cols matrix whose (i,j)-th entry is entry_fn(i, j) \u0026quot;\u0026quot;\u0026quot; return [[entry_fn(i,j) # given i, create a list for j in range(num_cols)] # [entry_fn(i, 0), ...] for i in range(num_rows)] # create one list for each i  Then we\u0026rsquo;ll actually use the make_matrix function to create a special type of matrix called the identity matrix:\ndef identity_matrix(n: int) -\u0026gt; Matrix: \u0026quot;\u0026quot;\u0026quot;Returns the n x n identity matrix\u0026quot;\u0026quot;\u0026quot; return make_matrix(n, n, lambda i, j: 1 if i == j else 0) assert identity_matrix(5) == [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]]  Summary To be sure there are other types of matrices, but in this chapter we\u0026rsquo;re only briefly exploring its construction to prime us.\nWe know matrices can be used to represent data, each row in the dataset being a vector. Because we can also know a matrices' column, we\u0026rsquo;ll use it to represent linear functions that map k-dimensional vectors to n-dimensional vectors.\nFinally, matrices can also be used to map binary relationships.\nFlashback to Ch.1 On our first day at DataScienster™ we were given friendship_pairs data:\nfriendship_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (3,4), (4,5), (5,6), (5,7), (6,8), (7,8), (8,9)]  These friendship_pairs can also be represented in matrix form:\n# user 0 1 2 3 4 5 6 7 8 9 friend_matrix = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0], # user 0 [1, 0, 1, 1, 0, 0, 0, 0, 0, 0], # user 1 [1, 1, 0, 1, 0, 0, 0, 0, 0, 0], # user 2 [0, 1, 1, 0, 1, 0, 0, 0, 0, 0], # user 3 [0, 0, 0, 1, 0, 1, 0, 0, 0, 0], # user 4 [0, 0, 0, 0, 1, 0, 1, 1, 0, 0], # user 5 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # user 6 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # user 7 [0, 0, 0, 0, 0, 0, 1, 1, 0, 1], # user 8 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]] # user 9  This allows us to check very quickly whether two users are friends or not:\nassert friend_matrix[0][2] == 1, \u0026quot;0 and 2 are friends\u0026quot; assert friend_matrix[0][8] == 0, \u0026quot;0 and 8 are not friends\u0026quot;  And if we wanted to check for each user\u0026rsquo;s friend, we could:\nfriends_of_five = [i for i, is_friend in enumerate(friend_matrix[5]) if is_friend] friends_of_zero = [i for i, is_friend in enumerate(friend_matrix[0]) if is_friend] assert friends_of_five == [4,6,7] assert friends_of_zero == [1,2]  ","date":1604966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604966400,"objectID":"89fe48187368e7bec74f1ec8b539a066","permalink":"/post/dsfs_4/","publishdate":"2020-11-10T00:00:00Z","relpermalink":"/post/dsfs_4/","section":"post","summary":"Building tools for working with vectors and matrices from scratch","tags":["Python","Data Science"],"title":"Data Science from Scratch (ch4) - Linear Algebra","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Beyond Collections and Comprehensions A couple days back I wrote a post summarizing how much Collections and Comprehension were used. Data was provided in the form of lists, either lists of dictionaries or tuples. And to answer questions about the data, the author often used list comprehensions - iterating through lists with a for-loop. I am beginning to see this as a very Python-centric way of approaching problems.\nWhile not all data is tabular, so much of it is so its reasonable to assume that, more often that not, you\u0026rsquo;ll be dealing with spreadsheet-like tabular data (note: I\u0026rsquo;m open to other perspectives here, feel free to leave a comment below!).\nIn any case, I had this itch to go back to that chapter and ask:\n How would I approach the same problem using data frames?\n So that\u0026rsquo;s what this post is about. You can reference these previous post for context; also keep in mind, this is a brief detour and deviation from Joel Grus' book (for example, I\u0026rsquo;ll be using pandas here and a jupyter notebook here, both of which are not covered in the book).\nFor review, here\u0026rsquo;s the data you are given as a newly hired data scientist at Data Scienster™\n# users in the network # stored as a list of dictionaries users = [ {\u0026quot;id\u0026quot;: 0, \u0026quot;name\u0026quot;: \u0026quot;Hero\u0026quot;}, {\u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;Dunn\u0026quot;}, {\u0026quot;id\u0026quot;: 2, \u0026quot;name\u0026quot;: \u0026quot;Sue\u0026quot;}, {\u0026quot;id\u0026quot;: 3, \u0026quot;name\u0026quot;: \u0026quot;Chi\u0026quot;}, {\u0026quot;id\u0026quot;: 4, \u0026quot;name\u0026quot;: \u0026quot;Thor\u0026quot;}, {\u0026quot;id\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;Clive\u0026quot;}, {\u0026quot;id\u0026quot;: 6, \u0026quot;name\u0026quot;: \u0026quot;Hicks\u0026quot;}, {\u0026quot;id\u0026quot;: 7, \u0026quot;name\u0026quot;: \u0026quot;Devin\u0026quot;}, {\u0026quot;id\u0026quot;: 8, \u0026quot;name\u0026quot;: \u0026quot;Kate\u0026quot;}, {\u0026quot;id\u0026quot;: 9, \u0026quot;name\u0026quot;: \u0026quot;Klein\u0026quot;} ] # friendship pairings in the network # stored as a list of tuples friendship_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (3,4), (4,5), (5,6), (5,7), (6,8), (7,8), (8,9)] # interests data # stored as another list of tuples interests = [ (0, \u0026quot;Hadoop\u0026quot;), (0, \u0026quot;Big Data\u0026quot;), (0, \u0026quot;HBase\u0026quot;), (0, \u0026quot;Java\u0026quot;), (0, \u0026quot;Spark\u0026quot;), (0, \u0026quot;Storm\u0026quot;), (0, \u0026quot;Cassandra\u0026quot;), (1, \u0026quot;NoSQL\u0026quot;), (1, \u0026quot;MongoDB\u0026quot;), (1, \u0026quot;Cassandra\u0026quot;), (1, \u0026quot;HBase\u0026quot;), (1, \u0026quot;Postgres\u0026quot;), (2, \u0026quot;Python\u0026quot;), (2, \u0026quot;scikit-learn\u0026quot;), (2, \u0026quot;scipy\u0026quot;), (2, \u0026quot;numpy\u0026quot;), (2, \u0026quot;statsmodels\u0026quot;), (2, \u0026quot;pandas\u0026quot;), (3, \u0026quot;R\u0026quot;), (3, \u0026quot;Python\u0026quot;), (3, \u0026quot;statistics\u0026quot;), (3, \u0026quot;regression\u0026quot;), (3, \u0026quot;probability\u0026quot;), (4, \u0026quot;machine learning\u0026quot;), (4, \u0026quot;regression\u0026quot;), (4, \u0026quot;decision trees\u0026quot;), (4, \u0026quot;libsvm\u0026quot;), (5, \u0026quot;Python\u0026quot;), (5, \u0026quot;R\u0026quot;), (5, \u0026quot;Java\u0026quot;), (5, \u0026quot;C++\u0026quot;), (5, \u0026quot;Haskell\u0026quot;), (5, \u0026quot;programming langauges\u0026quot;), (6, \u0026quot;statistics\u0026quot;), (6, \u0026quot;probability\u0026quot;), (6, \u0026quot;mathematics\u0026quot;), (6, \u0026quot;theory\u0026quot;), (7, \u0026quot;machine learning\u0026quot;), (7, \u0026quot;scikit-learn\u0026quot;), (7, \u0026quot;Mahout\u0026quot;), (7, \u0026quot;neural networks\u0026quot;), (8, \u0026quot;neural networks\u0026quot;), (8, \u0026quot;deep learning\u0026quot;), (8, \u0026quot;Big Data\u0026quot;), (8, \u0026quot;artificial intelligence\u0026quot;), (9, \u0026quot;Hadoop\u0026quot;), (9, \u0026quot;Java\u0026quot;), (9, \u0026quot;MapReduce\u0026quot;), (9, \u0026quot;Big Data\u0026quot;) ]  Given just these pieces of data, we can create functions, use for-loops and list comprehensions to answer some questions like:\n Who are each user friends with? What are the total and average number of connections? Which users share the same interest? What are the most popular topics in this network?  However, the chapter ends with lists, functions and comprehension. What about storing data in data frames?\nFirst we\u0026rsquo;ll store users as a data frame:\nimport pandas as pd # convert list of dict into dataframe users_df = pd.DataFrame(users) users_df  Just visually, a data frame looks different from a list of dictionaries:\nYour mileage may vary, but I make sense of the data very differently when I\u0026rsquo;m looking at a list vs a data frame. Rows and columns are ingrained in how I think about data.\nNext, we\u0026rsquo;re given a list of tuples representing friendship pairs and we proceed to turn that into a dictionary by using a dictionary comprehension:\n# list of tuples friendship_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (3,4), (4,5), (5,6), (5,7), (6,8), (7,8), (8,9)] # create a dict, where keys are users id, # dictionary comprehension friendships = {user[\u0026quot;id\u0026quot;]: [] for user in users} for i, j in friendship_pairs: friendships[i].append(j) friendships[j].append(i)  Similar to the previous example, I find that viewing the data as a data frame is different from viewing it as a dictionary:\nFrom this point, I\u0026rsquo;m doing several operations in pandas to join the first two tables, such that I have a column with the user\u0026rsquo;s id, user\u0026rsquo;s name and the id of their first, second or, in some cases, third friends (at most people in this network have 3 direct connections).\nIf you want to know the specific pandas operation, here\u0026rsquo;s the code:\n# The users_df is fine as is with two columns: id and name (see above) # We'll transform the friendships_df # reset_index allows us to add an index column friendships_df.reset_index(inplace=True) # add index column friendships_df = friendships_df.rename(columns = {\u0026quot;id\u0026quot;:\u0026quot;new column name\u0026quot;}) # change index column to 'id' friendships_df = friendships_df.rename(columns = {'index':'id'}) # join with users_df so we get each person's name users_friendships = pd.merge(users_df, friendships_df, on='id')  Once we\u0026rsquo;ve joined users_df and friendships_df, we have:\nSince we have users and friendships data, we could write a function to help us answer \u0026ldquo;how many friends does each user have?\u0026rdquo;. In addition, we\u0026rsquo;ll have to create a list comprehension so we loop through each user within users:\n# function to count how many friend each user has def number_of_friends(user): \u0026quot;\u0026quot;\u0026quot;How many friends does _user_ have?\u0026quot;\u0026quot;\u0026quot; user_id = user[\u0026quot;id\u0026quot;] friend_ids = friendships[user_id] return len(friend_ids) # list comprehension to apply the function for each user num_friends_by_id = [(user[\u0026quot;id\u0026quot;], number_of_friends(user)) for user in users] # this gives us a list of tuples num_friends_by_id [(0, 2), (1, 3), (2, 3), (3, 3), (4, 2), (5, 3), (6, 2), (7, 2), (8, 3), (9, 1)]  Again, viewing the data as a list of tuples is different from a data frame, so let\u0026rsquo;s go ahead and turn that into a pandas data frame:\n# when converting to data frame, we can set the name of the columns to id and num_friends; this sets us up for another join num_friends_by_id = pd.DataFrame(num_friends_by_id, columns = ['id', 'num_friends'])  Because we have an \u0026lsquo;id\u0026rsquo; column, we can join this with our previously created users_friendships data frame:\nOnce joined with users_friendships using the merge function, we get (users_friendships2):\nBy now you\u0026rsquo;re familiar with the process. We have a Python collection, generally a list of dictionaries or tuples and we want to convert them to a data frame.\nWe\u0026rsquo;ll repeat this process for the interests variable which is a long list of tuples (see above). We\u0026rsquo;ll convert to data frame, then join with users_friendships_2 to get a longer data frame with interests as one of the columns (note : picture is cut for space):\nThe nice thing about pandas is that once you have all your data joined together in a data frame, you can query the data.\nFor example, I may want to see all users have an interest in \u0026ldquo;Big Data\u0026rdquo;:\nPreviously, we would have had to create a function that returns a list comprehension:\ndef data_scientists_who_like(target_interest): \u0026quot;\u0026quot;\u0026quot;Find the ids of all users who like the target interests.\u0026quot;\u0026quot;\u0026quot; return [user_id for user_id, user_interest in interests if user_interest == target_interest] data_scientists_who_like(\u0026quot;Big Data\u0026quot;)  The data frame has other advantages, you could also query columns on multiple conditions, here are two ways to query multiple topics:\n# Option One: Use .query() user_friendship_topics.query('topic == \u0026quot;machine learning\u0026quot; | topic == \u0026quot;regression\u0026quot; | topic == \u0026quot;decision trees\u0026quot; | topic == \u0026quot;libsvm\u0026quot;') # Option Two: Use .isin() user_friendship_topics[user_friendship_topics['topic'].isin([\u0026quot;machine learning\u0026quot;, \u0026quot;regression\u0026quot;, \u0026quot;decision trees\u0026quot;, \u0026quot;libsvm\u0026quot;])]  Both options return this data frame:\nBy querying the data frame, we learned:\n all users interested in these four topics users that have interests in common with Thor (if needed) the num_friends that each user has  You can also find out the most popular topics within this network:\n# groupby topic, tally(count), then reset_index(), then sort user_friendship_topics.groupby(['topic']).count().reset_index().sort_values('id', ascending=False)  You can even groupby two columns (name \u0026amp; topic) to see topic of interests listed by each user:\nuser_friendship_topics.groupby(['name', 'topic']).count()  Hopefully you\u0026rsquo;re convinced that data frames are a powerful supplement to the more familiar operations in Python like for-loops and/or list comprehensions; that both are worth knowing well to manipulate data in a variety of formats. (e.g., to access JSON data, Python dictionaries are ideal).\n","date":1604707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604707200,"objectID":"80efaaddbc476d2477b5c2a58084a108","permalink":"/post/list-to-df/","publishdate":"2020-11-07T00:00:00Z","relpermalink":"/post/list-to-df/","section":"post","summary":"Combining for-loops, list comprehensions and data frames","tags":[],"title":"Supplementing lists with data frames","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Data Visualization Chapter 3 of Data Science from Scratch introduces us to visualizing data using matplotlib. This is widely used in the Python ecosystem, although my sense is that people are just as happy, if not more, to use other libraries like seaborn, Altair and bokeh. (note: seaborn is built on top of matplotlib).\nThis chapter is fairly brief and is meant as a quick introduction to matplotlib - to get readers familiar with basic charts. Whole books can be written on data visualization alone, so this is meant more as an appetizer, rather than a full-course.\nThere\u0026rsquo;s a fair amount of detail involved in using matplotlib, so we\u0026rsquo;ll break it down to demystify it.\nBasic Plotting This chapter goes through the main basic charts including Line, Bar, Histograms, and Scatter Plots. At first glance, they follow a similar pattern. Data is provided as a list of numbers (usually more than one list). pyplot is imported from matplotlib as plt. The plt module has several functions which are accessed to create the plot.\nHere\u0026rsquo;s an example line chart visualizing growth in GDP over time:\nHere\u0026rsquo;s the code:\nfrom matplotlib import pyplot as plt # the data years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] # the plot plt.plot(years, gdp, color=\u0026quot;green\u0026quot;, marker='o', linestyle='solid') plt.title(\u0026quot;Nominal GDP\u0026quot;) plt.ylabel(\u0026quot;Billions of $\u0026quot;) plt.xlabel(\u0026quot;Years\u0026quot;) plt.show()  You can somewhat get by with just knowing this. Briefly consulting the documentation will let you see some other chart types like so:\nLet\u0026rsquo;s say we wanted to convert our line chart into a stacked area chart, we can just change one line:\nfrom matplotlib import pyplot as plt years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] plt.stackplot(years, gdp, color=\u0026quot;green\u0026quot;) # this is the only line we changed plt.title(\u0026quot;Nominal GDP\u0026quot;) plt.ylabel(\u0026quot;Billions of $\u0026quot;) plt.xlabel(\u0026quot;Years\u0026quot;) plt.show()  Here\u0026rsquo;s what the stacked area chart version of the previous graph looks like:\nTo keep things simple, we can change the chart type with just one line and we just need to remember that when converting from chart to chart, we have to be mindful of the parameters that each chart type takes. For example, a stacked area chart takes in different parameters than line charts (for example, you\u0026rsquo;ll get an AttributionError if you try to use marker in a stacked area chart.)\nHere\u0026rsquo;s an example bar chart comparing movies by the number of Academy awards they\u0026rsquo;ve won:\nHere\u0026rsquo;s a stem plot version:\nAs with the previous example, changing just one function from plt.bar to plt.stem gave us a different plot:\n#---- Original Bar Chart ----# movies = [\u0026quot;Annie Hall\u0026quot;, \u0026quot;Ben-Hur\u0026quot;, \u0026quot;Casablanca\u0026quot;, \u0026quot;Gandhi\u0026quot;, \u0026quot;West Side Story\u0026quot;] num_oscars = [5,11,3,8,10] plt.bar(range(len(movies)), num_oscars) plt.title(\u0026quot;My Favorite Movies\u0026quot;) plt.ylabel(\u0026quot;# of Academy Awards\u0026quot;) plt.xticks(range(len(movies)), movies) plt.show() # ---- Stem Chart ---- # movies = [\u0026quot;Annie Hall\u0026quot;, \u0026quot;Ben-Hur\u0026quot;, \u0026quot;Casablanca\u0026quot;, \u0026quot;Gandhi\u0026quot;, \u0026quot;West Side Story\u0026quot;] num_oscars = [5,11,3,8,10] plt.stem(range(len(movies)), num_oscars) # the only change plt.title(\u0026quot;My Favorite Movies\u0026quot;) plt.ylabel(\u0026quot;# of Academy Awards\u0026quot;) plt.xticks(range(len(movies)), movies) plt.show()  There are levels to this: Hierarchy I\u0026rsquo;m all for keeping matplotlib as simple as possible but one thing the above examples gloss over is the matplotlib object hierarchy, which is something worth understanding to get a feel for how the various functions operate.\nThis next figure is borrowed from Real Python and it nicely highlights the hierarchy inherent in every plot:\nYou\u0026rsquo;ll note the levels: Figure, Axes and Axis. When digging into matplotlib documentation on axes, these levels are brought to the foreground.\nTo really see this in action, we\u0026rsquo;ll need to code our plot slightly differently. For the last chart this chapter examines the bias-variance tradeoff which is something we\u0026rsquo;ll learn more about in future chapters, but it highlights the trade-off in trying to simultanenously minimize two sources of error so our algorithm generalizes to new situations.\nHere\u0026rsquo;s the code:\n# BOOK version variance = [1,2,4,8,16,32,64,128,256] bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1] total_error = [x + y for x,y in zip(variance, bias_squared)] xs = [i for i, _ in enumerate(variance)] plt.plot(xs, variance, 'g-', label='variance') plt.plot(xs, bias_squared, 'r-', label='bias^2') plt.plot(xs, total_error, 'b:', label='total error') plt.legend(loc=9) plt.xlabel(\u0026quot;model complexity\u0026quot;) plt.xticks([]) plt.title(\u0026quot;The Bias-Variance Tradeoff\u0026quot;) plt.show() # ALTERNATE version variance = [1,2,4,8,16,32,64,128,256] bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1] total_error = [x + y for x,y in zip(variance, bias_squared)] xs = [i for i, _ in enumerate(variance)] fig, ax = plt.subplots(figsize=(8,5)) ax.plot(xs, variance, 'g-', label='variance') ax.plot(xs, bias_squared, 'r-', label='bias^2') ax.plot(xs, total_error, 'b:', label='total error') ax.legend(loc='upper center') ax.set_xlabel(\u0026quot;model complexity\u0026quot;) ax.set_title(\u0026quot;The Bias-Variance Tradeoff: Alt Version\u0026quot;) fig.tight_layout() fig.show()  Instead using the plt module, we use fig and ax, here are their data types:\ntype(fig) # matplotlib.figure.Figure type(ax) # matplotlib.axes._subplots.AxesSubplot  This makes explicit the matplotlib object hierarchy, particularly as we see how we access function at the axes._subplits.AxesSubplot level (the documentation has much more detail).\nHere\u0026rsquo;s the chart:\nIn summary, we learned that matplotlib can be fairly simple to use for static, simple plots, but we\u0026rsquo;re better served having some understanding of matplotlib\u0026rsquo;s object hierarchy. We\u0026rsquo;ll examine more chart types as we proceed with the rest of the chapters.\n","date":1604534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604534400,"objectID":"2227a576f8813556cb2787cb27e14c00","permalink":"/post/dsfs_3/","publishdate":"2020-11-05T00:00:00Z","relpermalink":"/post/dsfs_3/","section":"post","summary":"Visualizing Data in Python","tags":["Python","Data Science"],"title":"Making sense of matplotlib","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of Content:\n  Part 1  Part 2  DataScienster_pt1 Collections and Comprehensions  Data Science from Scratch opens with a narrative motivating example where you, dear reader, are newly hired to lead data science at DataSciencester, a social network exclusively for data scientists.\nJoel Grus, the author, explains:\n Throughout the book, we\u0026rsquo;ll be learning about data science concepts by solving problems that you encounter at work. Sometimes we\u0026rsquo;ll look at data explicitly supplied by users, sometimes we\u0026rsquo;ll look at data generated through their interactions with the site, and sometimes we\u0026rsquo;ll even look at data from experiments that we\u0026rsquo;ll design\u0026hellip;we\u0026rsquo;ll be building our tools from scratch.\n This chapter is meant as a teaser for the rest of the book, but I wanted to revisit this chapter with our python crash course fresh on our minds to highlight some frequently used concepts we can expect to see for the rest of the book.\nYou are just hired as \u0026ldquo;VP of Networking\u0026rdquo; and are tasked with finding out which data scientist is the most well connected in the DataSciencster network, you\u0026rsquo;re giving a data dump 👇. It\u0026rsquo;s a list of users, each with a unique id.\nusers = [ {\u0026quot;id\u0026quot;: 0, \u0026quot;name\u0026quot;: \u0026quot;Hero\u0026quot;}, {\u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;Dunn\u0026quot;}, {\u0026quot;id\u0026quot;: 2, \u0026quot;name\u0026quot;: \u0026quot;Sue\u0026quot;}, {\u0026quot;id\u0026quot;: 3, \u0026quot;name\u0026quot;: \u0026quot;Chi\u0026quot;}, {\u0026quot;id\u0026quot;: 4, \u0026quot;name\u0026quot;: \u0026quot;Thor\u0026quot;}, {\u0026quot;id\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;Clive\u0026quot;}, {\u0026quot;id\u0026quot;: 6, \u0026quot;name\u0026quot;: \u0026quot;Hicks\u0026quot;}, {\u0026quot;id\u0026quot;: 7, \u0026quot;name\u0026quot;: \u0026quot;Devin\u0026quot;}, {\u0026quot;id\u0026quot;: 8, \u0026quot;name\u0026quot;: \u0026quot;Kate\u0026quot;}, {\u0026quot;id\u0026quot;: 9, \u0026quot;name\u0026quot;: \u0026quot;Klein\u0026quot;} ]  Of note here is that the users variable is a list of dict (dictionaries).\nMoving along, we also receive \u0026ldquo;friendship\u0026rdquo; data. Of note here that this is a list of tuples:\nfriendship_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (3,4), (4,5), (5,6), (5,7), (6,8), (7,8), (8,9)]  I had initially (and erroneously) thought of list, dict and tuple as data types (like int64, float64, string).\nThey\u0026rsquo;re rather collections, and somewhat unique to Python and more importantly, informs the way Pythonistas approach and solve problems.\nYou may feel that having \u0026ldquo;friendship\u0026rdquo; data in a list of tuple is not the easiest way to work with data (nor may it be the best way to represent data, but we\u0026rsquo;ll suspend those thoughts for now). Our first task is to convert this list of tuple into a form that\u0026rsquo;s more workable; the author proposes we turn it into a dict where the keys are user_ids and the values are list of friends.\nThe argument is that its faster to look things up in a dict rather than a list of tuple (where we\u0026rsquo;d have to iterate over every tuple). Here\u0026rsquo;s how we\u0026rsquo;d do that:\n# Initialize the dict with an empty list for each user id friendships = { user[\u0026quot;id\u0026quot;]: [] for user in users } # Loop over friendship pairs # This operation grabs the first, then second integer in each tuple # It then appends each integer to the newly initialized friendships dict for i, j in friendship_pairs: friendships[i].append(j) friendships[j].append(i)  We\u0026rsquo;re initializing a dict (called friendships), then looping over friendship_pairs to populate friendships. This is the outcome:\nfriendships { 0: [1, 2], 1: [0, 2, 3], 2: [0, 1, 3], 3: [1, 2, 4], 4: [3, 5], 5: [4, 6, 7], 6: [5, 8], 7: [5, 8], 8: [6, 7, 9], 9: [8] }  Each key in friendships is matched with a value that is initially an empty list, which then gets populated as we loop over friendship_pairs and systematically append the user_id that is paired together.\nTo understand how the looping happends and, specifically how each pair of user_ids are connected to each other, I created my own mini-toy example. Let\u0026rsquo;s say we\u0026rsquo;re just going to focus on looping through friendship_pairs for the user Hero whose id is 0.\n# we'll set hero to an empty list hero = [] # for every friendship_pair, if the first integer is 0, which is Hero's id, # then append the second integer for x, y in friendship_pairs: if x == 0: hero.append(y) # outcome: we can confirm that Hero is connected to Dunn and Sue hero # [1,2]  The above gave me better intuition for how this works:\nfor i, j in friendship_pairs: friendships[i].append(j) # Add j as a friend of user i friendships[j].append(i) # Add i as a friend of user j  Here are some other questions we may be interested in:\nWhat is the total number of connections? Look at how the problem is solved. What\u0026rsquo;s notable to me is how we first define a function number_of_friends(user) that returns the number of friends for a particular user.\nThen, total_connections is calculated using a comprehension (tuple?):\ndef number_of_friends(user): \u0026quot;\u0026quot;\u0026quot;How many friends does _user_ have?\u0026quot;\u0026quot;\u0026quot; user_id = user[\u0026quot;id\u0026quot;] friend_ids = friendships[user_id] return len(friend_ids) total_connections = sum(number_of_friends(user) for user in users)  To be clear, the (tuple) comprehension is a pattern where a function is applied over a for-loop, in one line:\n# (2, 3, 3, 3, 2, 3, 2, 2, 3, 1) tuple((number_of_friends(user) for user in users)) # you can double check by calling friendships dict and counting the number of friends each user has friendships { 0: [1, 2], 1: [0, 2, 3], 2: [0, 1, 3], 3: [1, 2, 4], 4: [3, 5], 5: [4, 6, 7], 6: [5, 8], 7: [5, 8], 8: [6, 7, 9], 9: [8] }  This pattern of using a one-line for-loop (aka comprehension) will come up often. If we add up all the connections, we get 24 and to find the average, we simply divide by the number of users (10) for 2.4, this part is straight-forward.\nCan we sort who has most-to-least friends to find the most connected individuals? To answer this question, again, a list comprehension is used. The cool thing is that we re-use functions we had previously created (number_of_friends(user)).\n# Create a list that loops over users dict, applying a previously defined function num_friends_by_id = [(user[\u0026quot;id\u0026quot;], number_of_friends(user)) for user in users] # Then sort num_friends_by_id.sort( # Sort the list key=lambda id_and_friends: id_and_friends[1], # by number friends reverse=True) # descending order  We have just identified how central an individual is to the network, and we can expect to explore degree centrality and networks more in future chapters, but for the purposes of this post, we have identified the central role that collections (lists, dictionaries, tuples) as well as comprehensions play in Python operations.\nIn the next post, we\u0026rsquo;ll examing how friendship connections may or may not overlap with interests.\nDataScienster_pt2 In the previous section, we began examining a toy data set see what kind of Python concepts from the crash course we\u0026rsquo;d see in action.\nWhat stands out is the use of collections and comprehension. We\u0026rsquo;ll see this trend continue as data is given to us in the form of a list of dict or tuples.\nOften time, we\u0026rsquo;re manipulating the data to make it faster and more efficient to iterate through the data. The tool that comes up quite often is using defaultdict to initialize an empty list. Followed by list comprehensions to iterate through data.\nIndeed, either we\u0026rsquo;re seeing how the author, specifically, approaches problem or how problems are approached in Python, in general.\nWhat I\u0026rsquo;m keeping in mind is that there are more than one way to approach data science problems and this is one of them.\nWith that said, let\u0026rsquo;s pick up where the previous section left off.\nFriends you may know We have a sense of the total number of connections and a sorting of the most connected individuals. Now, we may want to design a \u0026ldquo;people you may know\u0026rdquo; suggester.\nQuick recap, here\u0026rsquo;s what the friendship dictionary looks like.\nfriendships { 0: [1, 2], 1: [0, 2, 3], 2: [0, 1, 3], 3: [1, 2, 4], 4: [3, 5], 5: [4, 6, 7], 6: [5, 8], 7: [5, 8], 8: [6, 7, 9], 9: [8] }  Again, the first step is to iterate over friends and collect friends' friend. The following function returns a list comprehension. Let\u0026rsquo;s examine this function line-by-line to understand how it works. It returns friend_of_a_friend (foaf) id for each of the individuals' id, then grabing the id of their friends.\nWe\u0026rsquo;ll break it down in code below this function:\ndef foaf_ids_bad(user): \u0026quot;\u0026quot;\u0026quot;foaf is short for 'friend of a friend' \u0026quot;\u0026quot;\u0026quot; return [foaf_id for friend_id in friendships[user[\u0026quot;id\u0026quot;]] for foaf_id in friendships[friend_id]] # Let's take Hero, to see Hero's friends # we'll call the first key of the friendships dict # Hero has two friends with ids 1 and 2 friendships[0] # [1,2] # then we'll loop over *each* of the friends friendships[1] # [0, 2, 3] friendships[2] # [0, 1, 3] # assert that function works assert foaf_ids_bad(users[0]) == [0, 2, 3, 0, 1, 3]  Can we count mutual friends? To answer this we\u0026rsquo;ll use a Counter, which we learned is a dict subclass. Moreover, the function friends_of_friends(user),\nfrom collections import Counter def friends_of_friends(user): user_id = user[\u0026quot;id\u0026quot;] return Counter( foaf_id for friend_id in friendships[user_id] # for each of my friends, for foaf_id in friendships[friend_id] # find their friends if foaf_id != user_id # who aren't me and foaf_id not in friendships[user_id] # and aren't my friends ) # lets look at Hero # he has two common friends with Chi # Chi is neither Hero nor his direct friends friends_of_friends(users[0]) # Counter({3: 2})  In addition to friendship data, we also have interest data. Here we see a list of tuples, containing a user_id and a string representing a specific of technology.\ninterests = [ (0, \u0026quot;Hadoop\u0026quot;), (0, \u0026quot;Big Data\u0026quot;), (0, \u0026quot;HBase\u0026quot;), (0, \u0026quot;Java\u0026quot;), (0, \u0026quot;Spark\u0026quot;), (0, \u0026quot;Storm\u0026quot;), (0, \u0026quot;Cassandra\u0026quot;), (1, \u0026quot;NoSQL\u0026quot;), (1, \u0026quot;MongoDB\u0026quot;), (1, \u0026quot;Cassandra\u0026quot;), (1, \u0026quot;HBase\u0026quot;), (1, \u0026quot;Postgres\u0026quot;), (2, \u0026quot;Python\u0026quot;), (2, \u0026quot;scikit-learn\u0026quot;), (2, \u0026quot;scipy\u0026quot;), (2, \u0026quot;numpy\u0026quot;), (2, \u0026quot;statsmodels\u0026quot;), (2, \u0026quot;pandas\u0026quot;), (3, \u0026quot;R\u0026quot;), (3, \u0026quot;Python\u0026quot;), (3, \u0026quot;statistics\u0026quot;), (3, \u0026quot;regression\u0026quot;), (3, \u0026quot;probability\u0026quot;), (4, \u0026quot;machine learning\u0026quot;), (4, \u0026quot;regression\u0026quot;), (4, \u0026quot;decision trees\u0026quot;), (4, \u0026quot;libsvm\u0026quot;), (5, \u0026quot;Python\u0026quot;), (5, \u0026quot;R\u0026quot;), (5, \u0026quot;Java\u0026quot;), (5, \u0026quot;C++\u0026quot;), (5, \u0026quot;Haskell\u0026quot;), (5, \u0026quot;programming langauges\u0026quot;), (6, \u0026quot;statistics\u0026quot;), (6, \u0026quot;probability\u0026quot;), (6, \u0026quot;mathematics\u0026quot;), (6, \u0026quot;theory\u0026quot;), (7, \u0026quot;machine learning\u0026quot;), (7, \u0026quot;scikit-learn\u0026quot;), (7, \u0026quot;Mahout\u0026quot;), (7, \u0026quot;neural networks\u0026quot;), (8, \u0026quot;neural networks\u0026quot;), (8, \u0026quot;deep learning\u0026quot;), (8, \u0026quot;Big Data\u0026quot;), (8, \u0026quot;artificial intelligence\u0026quot;), (9, \u0026quot;Hadoop\u0026quot;), (9, \u0026quot;Java\u0026quot;), (9, \u0026quot;MapReduce\u0026quot;), (9, \u0026quot;Big Data\u0026quot;) ]  First thing we\u0026rsquo;ll do is find users with a specific interest. This is function returns a list comprehension. It first split each tuple into user_id (integer) and user_interest (string), then conditionally check if the string in the tuple matches the input parameter.\ndef data_scientists_who_like(target_interest): \u0026quot;\u0026quot;\u0026quot;Find the ids of all users who like the target interests.\u0026quot;\u0026quot;\u0026quot; return [user_id for user_id, user_interest in interests if user_interest == target_interest] # let's see all user_id who likes \u0026quot;statistics\u0026quot; data_scientists_who_like(\u0026quot;statistics\u0026quot;) # [3, 6]  We may also want to count the number of times a specific interest comes up. Here\u0026rsquo;s a function for that. We use a basic for-loop and if-statement to check truthiness of user_interest == target_interest.\ndef num_user_with_interest_in(target_interest): interest_count = 0 for user_id, user_interest in interests: if user_interest == target_interest: interest_count += 1 return interest_count  A concern is having to examine a whole list of interests for every search. The author proposes building an index from interests to users. Here, a defaultdict is imported, then populated with user_id\nfrom collections import defaultdict # user_ids matched to specific interest user_ids_by_interest = defaultdict(list) for user_id, interest in interests: user_ids_by_interest[interest].append(user_id) # three users interested in Python assert user_ids_by_interest[\u0026quot;Python\u0026quot;] == [2,3,5] # list of interests by user_id interests_by_user_id = defaultdict(list) for user_id, interest in interests: interests_by_user_id[user_id].append(interest) # check all of Hero's interests assert interests_by_user_id[0] == ['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']  We can find who has the most interests in common with a given user. Looks like Klein (#9) has the most common interests with Hero (#0). Here we return a Counter with for-loops and an if-statement.\ndef most_common_interests_with(user): return Counter( interested_user_id for interest in interests_by_user_id[user[\u0026quot;id\u0026quot;]] for interested_user_id in user_ids_by_interest[interest] if interested_user_id != user[\u0026quot;id\u0026quot;] ) # let's check to see who has the most common interest with Hero most_common_interests_with(users[0]) # Counter({9: 3, 8: 1, 1: 2, 5: 1})  Finally, we can also find which topics are most popular among the network. Previously, we calculated the number of users interested in a particular topic, but now we want to compare the whole list.\nwords_and_counts = Counter(word for user, interest in interests for word in interest.lower().split())  Salaries and Experience Data We\u0026rsquo;re also given anonymous salary and tenure (number of years work experience) data, let\u0026rsquo;s see what we can do with that information. First we\u0026rsquo;ll find the average salary. Again, we\u0026rsquo;ll start by creating a list (defaultdict), then loop through salaries_and_tenures.\nsalaries_and_tenures = [(83000, 8.7), (88000, 8.1), (48000, 0.7), (76000, 6), (69000, 6.5), (76000, 7.5), (60000, 2.5), (83000, 10), (48000, 1.9), (63000, 4.2)] salary_by_tenure = defaultdict(list) for salary, tenure in salaries_and_tenures: salary_by_tenure[tenure].append(salary) # find average salary by tenure average_salary_by_tenure = { tenure: sum(salaries) / len(salaries) for tenure, salaries in salary_by_tenure.items() }  The problem is that this is not terribly informative as each tenure value has a different salary. Not even the average_salary_by_tenure is informative, so our next move is to group similar tenure values together.\nFirst, we\u0026rsquo;ll create the groupings/categories using a control-flow, then we\u0026rsquo;ll create a list(defaultdict), and loop through salaries_and_tenures to populate the newly created salary_by_tenure_bucket. Finally calculate the average.\ndef tenure_bucket(tenure): if tenure \u0026lt; 2: return \u0026quot;less than two\u0026quot; elif tenure \u0026lt; 5: return \u0026quot;between two and five\u0026quot; else: return \u0026quot;more than five\u0026quot; salary_by_tenure_bucket = defaultdict(list) for salary, tenure in salaries_and_tenures: bucket = tenure_bucket(tenure) salary_by_tenure_bucket[bucket].append(salary) # finally calculate average average_salary_by_bucket = { tenure_bucket: sum(salaries) / len(salaries) for tenure_bucket, salaries in salary_by_tenure_bucket.items() }  One thing to note is that the \u0026ldquo;given\u0026rdquo; data, in this hypothetical toy example is either in a list of dictionaries or tuples, which may be atypical if we\u0026rsquo;re used to working with tabular data in dataFrame (pandas) or native data.frame in R.\nAgain, we are reminded that the higher purpose of this book - Data Science from Scratch (by Joel Grus; 2nd Ed) is to eschew libraries in favor of plain python to build everything from the ground up.\nShould your goal be to learn how various algorithms work by building them up from scratch, and in the process learn how data problems can be solved with python and minimal libraries, this is your book.\nJoel Grus does make clear that you would use libraries and frameworks (pandas, scikit-learn, matplotlib etc), rather than coded-from-scratch algorithms when working in production environments and will point out resource for further reading at the end of the chapters.\nIn the next post, we\u0026rsquo;ll get into visualizing data.\n","date":1604361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604361600,"objectID":"ad81417d06ffa18f71798a6567596730","permalink":"/post/dsfs_1/","publishdate":"2020-11-03T00:00:00Z","relpermalink":"/post/dsfs_1/","section":"post","summary":"Problem solving in Python","tags":["Python","Data Science"],"title":"Data Science from Scratch (ch1)","type":"post"},{"authors":[],"categories":[],"content":"       Preparing the Data We’ll start off with the raw data from 538. The data accompanies this Rmarkdown file or, alternatively, can be downloaded directly from their github repository. The file we’re using is called ‘historical_RAPTOR_by_player.csv’. Background information on the RAPTOR metric can be found here.\nWe’ll load the libraries, then the raw data.\nWe are primarily using the dplyr and readr packages from the Tidyverse. We’ll create our table with the reactable package and use htmltools for when we need to use html elements for table customization.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.1 ## ✓ tidyr 1.1.1 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(reactable) library(htmltools) # read data df \u0026lt;- read_csv(\u0026quot;./data/historical_RAPTOR_by_player.csv\u0026quot;) ## Parsed with column specification: ## cols( ## player_name = col_character(), ## player_id = col_character(), ## season = col_double(), ## poss = col_double(), ## mp = col_double(), ## raptor_offense = col_double(), ## raptor_defense = col_double(), ## raptor_total = col_double(), ## war_total = col_double(), ## war_reg_season = col_double(), ## war_playoffs = col_double(), ## predator_offense = col_double(), ## predator_defense = col_double(), ## predator_total = col_double(), ## pace_impact = col_double() ## ) df %\u0026gt;% head() ## # A tibble: 6 x 15 ## player_name player_id season poss mp raptor_offense raptor_defense ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Alaa Abdel… abdelal01 1991 640 303 -3.94 -0.510 ## 2 Alaa Abdel… abdelal01 1992 1998 959 -2.55 -0.198 ## 3 Alaa Abdel… abdelal01 1993 2754 1379 -2.37 -2.07 ## 4 Alaa Abdel… abdelal01 1994 320 159 -6.14 -2.75 ## 5 Alaa Abdel… abdelal01 1995 984 506 -3.85 -1.27 ## 6 Kareem Abd… abdulka01 1977 7674 3483 4.54 3.10 ## # … with 8 more variables: raptor_total \u0026lt;dbl\u0026gt;, war_total \u0026lt;dbl\u0026gt;, ## # war_reg_season \u0026lt;dbl\u0026gt;, war_playoffs \u0026lt;dbl\u0026gt;, predator_offense \u0026lt;dbl\u0026gt;, ## # predator_defense \u0026lt;dbl\u0026gt;, predator_total \u0026lt;dbl\u0026gt;, pace_impact \u0026lt;dbl\u0026gt;  Objective Here’s what the original 538 table looks like (note: we’ll add an extra column not shown in this picture.) This table features 538’s most updated NBA statistic, RAPTOR, which stands for Robust Algorithm (using) Player Tracking (and) On/Off Ratings.\nIt attempts to rank indiviual (player’s) seasons, rather than individual players themselves, because a player’s career, like any career, has ebbs and flows.\nOur objective is to re-create this table and give it a fresh makeover.\n538 Raptor Table\n  Data Wrangling We will wrangle the data to be close to what we need before using the reactable package. We’ll first select the columns we’re interested in. Then we’ll filter for players who played for more than 1000 minutes (mp), as done in the original. We’ll arrange the data in descending order by WAR - wins above replacement.\nNext, we’ll rename the columns to match the names used by 538. Then we’ll format all columns with decimal numbers to be rounded to one decimal place. Finally, we’ll choose the top 100 rows (after filtering) to keep our data manageable.\nWe’ll save this to a variable called raptor_table.\nraptor_table \u0026lt;- df %\u0026gt;% select(player_name, season, mp, raptor_offense, raptor_defense, raptor_total, war_total, war_playoffs) %\u0026gt;% filter(mp \u0026gt; 1000) %\u0026gt;% arrange(desc(war_total)) %\u0026gt;% rename( NAME = player_name, SEASON = season, MIN_PLAYED = mp, OFF = raptor_offense, DEF = raptor_defense, TOTAL = raptor_total, WAR = war_total, PLAYOFF_WAR = war_playoffs ) %\u0026gt;% mutate_at(4:8, funs(round(., 1))) %\u0026gt;% head(100)  ## Warning: `funs()` is deprecated as of dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated.  Introduction to {reactable} The reactable package has a set of major functions for creating beautiful tables.\nThe primary function is reactable() which we’ll use the wrap our raptor_table variable. This function immediately provides base features for the table. In addition, we’ll use several parameters that come with this function to create our table. For example, we’ll define the table height, minRows for number of rows in one view and pagination behavior for longer tables that do not fit on one view, as well as several other parameters.\nNext, we’ll use a set of functions to customize columns including: colDef() for individual columns and colGroup() for grouping columns together.\nFinally, we’ll use reactableTheme() for overall table styling. That’s all there is to creating beautiful tables with reactable.\n Creating a Basic Table First, we’ll wrap our raptor_table within the reactable() function to create a basic table. Just doing this gives us individual column sorting, which is neat. You’ll note in the original table that OFF, DEF and TOTAL are grouped under RAPTOR; this is achieved using colGroup().\nreactable( raptor_table, columns = list( OFF = colDef(name = \u0026quot;OFF.\u0026quot;), DEF = colDef(name = \u0026quot;DEF.\u0026quot;), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;) ), columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;)) ) )  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\"},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\"},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN_PLAYED\",\"type\":\"numeric\"},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\"},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\"},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\"},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\"},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"PLAYOFF_WAR\",\"type\":\"numeric\"}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"aad7a838f6c8709939555796669bc423\",\"key\":\"aad7a838f6c8709939555796669bc423\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  Add Basic Table Features Next, we’ll add basic table features including table height, mininum rows (10) and making it compact. In addition, we’ll enable the sort icon, showSortIcon, when any column name is clicked.\nWe won’t disable pagination because that will over write our column grouping (i.e., RAPTOR).\nreactable( raptor_table, height = 600, minRows = 10, showSortIcon = TRUE, compact = TRUE, pagination = TRUE, showPageInfo = TRUE, columns = list( OFF = colDef(name = \u0026quot;OFF.\u0026quot;), DEF = colDef(name = \u0026quot;DEF.\u0026quot;), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;) ), columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;)) ) )  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\"},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\"},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN_PLAYED\",\"type\":\"numeric\"},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\"},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\"},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\"},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\"},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"PLAYOFF_WAR\",\"type\":\"numeric\"}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":10,\"compact\":true,\"height\":\"600px\",\"dataKey\":\"aad7a838f6c8709939555796669bc423\",\"key\":\"aad7a838f6c8709939555796669bc423\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  Add Search Box Next, we’ll add a search box with the searchable parameter, as well as placeholder text for readability (langauge). You can type in a fake name in the search box and if there are no matches, the text will render “No matches”.\nreactable( raptor_table, height = 600, minRows = 10, showSortIcon = TRUE, compact = TRUE, pagination = TRUE, showPageInfo = TRUE, searchable = TRUE, language = reactableLang(searchPlaceholder = \u0026quot;Search...\u0026quot;, noData = \u0026quot;No matches\u0026quot;), columns = list( OFF = colDef(name = \u0026quot;OFF.\u0026quot;), DEF = colDef(name = \u0026quot;DEF.\u0026quot;), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;) ), columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;)) ) )  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\"},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\"},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN_PLAYED\",\"type\":\"numeric\"},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\"},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\"},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\"},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\"},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"PLAYOFF_WAR\",\"type\":\"numeric\"}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"searchable\":true,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":10,\"compact\":true,\"height\":\"600px\",\"language\":{\"searchPlaceholder\":\"Search...\",\"noData\":\"No matches\"},\"dataKey\":\"aad7a838f6c8709939555796669bc423\",\"key\":\"aad7a838f6c8709939555796669bc423\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  Format Individual Columns Next, we’ll add some formatting and styling to each of the individual columns. This will include column width and font used within the columns. The bulk of this process will be within the columns parameter. In addition to the OFF, DEF and TOTAL columns, we’ll add formating for the rest of the columns.\nThis includes customizing name, customizing digits (i.e., making sure commas are placed for minutes played), font and font sizes.\nWe’ll also add a “+” prefix for the TOTAL column. Althought we had previously formatted our data to one decimal place, we’ll need to re-do this with format = colFormat(digits = 1) to make sure all digits line up properly. You’ll note that the style for PLAYOFF_WAR has a whiteSpace parameter set to pre - this will be apparent why when we add visuals to this particular column below.\nWe will style OFF, DEF and TOTAL separately in the next section.\nreactable( raptor_table, height = 600, minRows = 10, showSortIcon = TRUE, compact = TRUE, pagination = TRUE, showPageInfo = TRUE, searchable = TRUE, language = reactableLang(searchPlaceholder = \u0026quot;Search...\u0026quot;, noData = \u0026quot;No matches\u0026quot;), columns = list( NAME = colDef(minWidth = 120, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), SEASON = colDef(minWidth = 60, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14), align = \u0026quot;left\u0026quot;), MIN_PLAYED = colDef(name = \u0026#39;MIN. PLAYED\u0026#39;, format = colFormat(separators = TRUE), minWidth = 60, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14), align = \u0026#39;right\u0026#39;), OFF = colDef(name = \u0026quot;OFF.\u0026quot;, format = colFormat(digits = 1), minWidth = 60, align = \u0026#39;right\u0026#39;), DEF = colDef(name = \u0026quot;DEF.\u0026quot;, format = colFormat(digits = 1), minWidth = 60, align = \u0026#39;right\u0026#39;), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;, format = colFormat(prefix = \u0026quot;+\u0026quot;, digits = 1), minWidth = 60, align = \u0026#39;right\u0026#39;), WAR = colDef(format = colFormat(digits = 1), minWidth = 60, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14), align = \u0026quot;right\u0026quot;), PLAYOFF_WAR = colDef(name = \u0026quot;P/O WAR\u0026quot;, format = colFormat(digits = 1), minWidth = 100, align = \u0026#39;center\u0026#39;, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, whiteSpace = \u0026quot;pre\u0026quot;, fontSize = 14)) ), columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;)) ) )  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\",\"minWidth\":120,\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\",\"minWidth\":60,\"align\":\"left\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN. PLAYED\",\"type\":\"numeric\",\"format\":{\"cell\":{\"separators\":true},\"aggregated\":{\"separators\":true}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\"},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\"},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\",\"format\":{\"cell\":{\"prefix\":\"+\",\"digits\":1},\"aggregated\":{\"prefix\":\"+\",\"digits\":1}},\"minWidth\":60,\"align\":\"right\"},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"P/O WAR\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":100,\"align\":\"center\",\"style\":{\"fontFamily\":\"liberation mono\",\"whiteSpace\":\"pre\",\"fontSize\":14}}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"searchable\":true,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":10,\"compact\":true,\"height\":\"600px\",\"language\":{\"searchPlaceholder\":\"Search...\",\"noData\":\"No matches\"},\"dataKey\":\"2ffde0b748400e9d77bdb1e6de47c425\",\"key\":\"2ffde0b748400e9d77bdb1e6de47c425\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  Code Edit Let’s pause to make the code more readable by refactoring and adding comments to better structure the code.\nreactable( # data raptor_table, # styling for entire table height = 600, minRows = 10, showSortIcon = TRUE, compact = TRUE, pagination = TRUE, showPageInfo = TRUE, searchable = TRUE, language = reactableLang(searchPlaceholder = \u0026quot;Search...\u0026quot;, noData = \u0026quot;No matches\u0026quot;), # styling individual columns # columns start columns = list( NAME = colDef(minWidth = 120, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), SEASON = colDef(minWidth = 60, align = \u0026quot;left\u0026quot;, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), MIN_PLAYED = colDef(name = \u0026#39;MIN. PLAYED\u0026#39;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(separators = TRUE), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), OFF = colDef(name = \u0026quot;OFF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1)), DEF = colDef(name = \u0026quot;DEF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1)), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(prefix = \u0026quot;+\u0026quot;, digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), WAR = colDef(minWidth = 60, align = \u0026quot;right\u0026quot;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), PLAYOFF_WAR = colDef(name = \u0026quot;P/O WAR\u0026quot;, minWidth = 100, align = \u0026#39;center\u0026#39;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, whiteSpace = \u0026quot;pre\u0026quot;, fontSize = 14)) ), #columns end # grouping OFF,DEF,TOTAL under RAPTOR columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;)) ) )  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\",\"minWidth\":120,\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\",\"minWidth\":60,\"align\":\"left\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN. PLAYED\",\"type\":\"numeric\",\"format\":{\"cell\":{\"separators\":true},\"aggregated\":{\"separators\":true}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\"},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\"},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\",\"format\":{\"cell\":{\"prefix\":\"+\",\"digits\":1},\"aggregated\":{\"prefix\":\"+\",\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"P/O WAR\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":100,\"align\":\"center\",\"style\":{\"fontFamily\":\"liberation mono\",\"whiteSpace\":\"pre\",\"fontSize\":14}}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"searchable\":true,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":10,\"compact\":true,\"height\":\"600px\",\"language\":{\"searchPlaceholder\":\"Search...\",\"noData\":\"No matches\"},\"dataKey\":\"d6a2b2574bfd22a6b7a12870582c537e\",\"key\":\"d6a2b2574bfd22a6b7a12870582c537e\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  Add Color for OFF, DEF \u0026amp; TOTAL We’ll visualize differences in offensive (OFF) and defensive (DEF) ratings for each NBA player for the regular season and playoffs by creating a color_palette, then inserting it in the style for specific columns (i.e., OFF, DEF).\n# note: For contrast, we use two colors shade (blue \u0026amp; red). # this function color_palette \u0026lt;- function(x) rgb(colorRamp(c(\u0026quot;#edfeff\u0026quot;, \u0026quot;#ff2c0f\u0026quot;))(x), maxColorValue = 255) # Now we\u0026#39;ll add color to the OFF and DEF columns reactable( # data raptor_table, # styling for entire table height = 600, minRows = 10, showSortIcon = TRUE, compact = TRUE, pagination = TRUE, showPageInfo = TRUE, searchable = TRUE, language = reactableLang(searchPlaceholder = \u0026quot;Search...\u0026quot;, noData = \u0026quot;No matches\u0026quot;), # styling individual columns # columns start columns = list( NAME = colDef(minWidth = 120, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), SEASON = colDef(minWidth = 60, align = \u0026quot;left\u0026quot;, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), MIN_PLAYED = colDef(name = \u0026#39;MIN. PLAYED\u0026#39;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(separators = TRUE), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), OFF = colDef(name = \u0026quot;OFF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1), style = function(value){ # normalization is based on a cell\u0026#39;s value in relation to the range of values in the column normalized \u0026lt;- (value - min(raptor_table$OFF)) / (max(raptor_table$OFF) - min(raptor_table$OFF)) color \u0026lt;- color_palette(normalized) # with color set based on the color_palette function, we can feed it to the background parameter list(background = color, fontWeight = \u0026quot;bold\u0026quot;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14) } ), DEF = colDef(name = \u0026quot;DEF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1), style = function(value){ # normalization is based on a cell\u0026#39;s value in relation to the range of values in the column normalized \u0026lt;- (value - min(raptor_table$DEF)) / (max(raptor_table$DEF) - min(raptor_table$DEF)) color \u0026lt;- color_palette(normalized) # with color set based on the color_palette function, we can feed it to the background parameter list(background = color, fontWeight = \u0026quot;bold\u0026quot;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14) } ), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(prefix = \u0026quot;+\u0026quot;, digits = 1), style = list(backgroundColor = \u0026#39;#F5F5F5\u0026#39;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), WAR = colDef(minWidth = 60, align = \u0026quot;right\u0026quot;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), PLAYOFF_WAR = colDef(name = \u0026quot;P/O WAR\u0026quot;, minWidth = 100, align = \u0026#39;center\u0026#39;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, whiteSpace = \u0026quot;pre\u0026quot;, fontSize = 14)) ), #columns end # grouping OFF,DEF,TOTAL under RAPTOR columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;)) ) )  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\",\"minWidth\":120,\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\",\"minWidth\":60,\"align\":\"left\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN. PLAYED\",\"type\":\"numeric\",\"format\":{\"cell\":{\"separators\":true},\"aggregated\":{\"separators\":true}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":[{\"background\":\"#FC482F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC442A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5943\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FF2C0F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC462D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB553E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA644F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD3D23\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4A32\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87866\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA624D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5943\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F77F6E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4F37\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC442A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87A69\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F5998B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4A32\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3ADA2\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE3E1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D2CD\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A89D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59789\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59B8E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59789\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59F93\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFEFF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87866\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA604B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87C6B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3ADA2\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA604B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F77F6E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A295\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C2BB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F5998B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D2CD\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA644F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B1A7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69587\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96F5C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F9715E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D0CA\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFDFDC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A69B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}]},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":[{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CDC7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5842\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD391E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59A8C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA634D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CDC7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDF3F3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96651\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59A8C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1C6C0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE2DF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDF7F7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BCB4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B2A8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5842\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEE9E7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59D90\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FF2C0F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD4026\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEF0EF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFEFF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68F81\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B2A8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA634D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFAFB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD3D22\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE5E3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE5E3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78879\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87B69\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78879\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEE9E7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78271\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FE361A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE2DF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BCB4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96955\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AEA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}]},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\",\"format\":{\"cell\":{\"prefix\":\"+\",\"digits\":1},\"aggregated\":{\"prefix\":\"+\",\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"backgroundColor\":\"#F5F5F5\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"P/O WAR\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":100,\"align\":\"center\",\"style\":{\"fontFamily\":\"liberation mono\",\"whiteSpace\":\"pre\",\"fontSize\":14}}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"searchable\":true,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":10,\"compact\":true,\"height\":\"600px\",\"language\":{\"searchPlaceholder\":\"Search...\",\"noData\":\"No matches\"},\"dataKey\":\"245656a358215004a59fa52aaeb06b70\",\"key\":\"245656a358215004a59fa52aaeb06b70\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  Add Bars to P/O WAR Now we’ll style the P/O WAR column to contrast with WAR, which includes regular season and playoffs. Basketball is a completely different game between the regular season and playoffs, so it’s worth knowing wins above replacement from the playoffs, if only to see which NBA star takes their game up another level during the playoffs.\n# define a function to define the attributes of bar charts bar_chart \u0026lt;- function(label, width = \u0026quot;100%\u0026quot;, height = \u0026quot;14px\u0026quot;, fill = \u0026quot;#00bfc4\u0026quot;, background = NULL) { bar \u0026lt;- div(style = list(background = fill, width = width, height = height)) chart \u0026lt;- div(style = list(flexGrow = 1, marginLeft = \u0026quot;6px\u0026quot;, background = background), bar) div(style = list(display = \u0026quot;flex\u0026quot;, alignItems = \u0026quot;center\u0026quot;), label, chart) } reactable( # data raptor_table, # styling for entire table height = 600, minRows = 10, showSortIcon = TRUE, compact = TRUE, pagination = TRUE, showPageInfo = TRUE, searchable = TRUE, language = reactableLang(searchPlaceholder = \u0026quot;Search...\u0026quot;, noData = \u0026quot;No matches\u0026quot;), # styling individual columns # columns start columns = list( NAME = colDef(minWidth = 120, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), SEASON = colDef(minWidth = 60, align = \u0026quot;left\u0026quot;, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), MIN_PLAYED = colDef(name = \u0026#39;MIN. PLAYED\u0026#39;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(separators = TRUE), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), OFF = colDef(name = \u0026quot;OFF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1), style = function(value){ # normalization is based on a cell\u0026#39;s value in relation to the range of values in the column normalized \u0026lt;- (value - min(raptor_table$OFF)) / (max(raptor_table$OFF) - min(raptor_table$OFF)) color \u0026lt;- color_palette(normalized) # with color set based on the color_palette function, we can feed it to the background parameter list(background = color, fontWeight = \u0026quot;bold\u0026quot;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14) } ), DEF = colDef(name = \u0026quot;DEF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1), style = function(value){ # normalization is based on a cell\u0026#39;s value in relation to the range of values in the column normalized \u0026lt;- (value - min(raptor_table$DEF)) / (max(raptor_table$DEF) - min(raptor_table$DEF)) color \u0026lt;- color_palette(normalized) # with color set based on the color_palette function, we can feed it to the background parameter list(background = color, fontWeight = \u0026quot;bold\u0026quot;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14) } ), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(prefix = \u0026quot;+\u0026quot;, digits = 1), style = list(backgroundColor = \u0026#39;#F5F5F5\u0026#39;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), WAR = colDef(minWidth = 60, align = \u0026quot;right\u0026quot;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), PLAYOFF_WAR = colDef(name = \u0026quot;P/O WAR\u0026quot;, minWidth = 100, align = \u0026#39;center\u0026#39;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, whiteSpace = \u0026quot;pre\u0026quot;, fontSize = 14), # render bar chart cell = function(value){ width \u0026lt;- paste0(value * 100 / max(raptor_table$PLAYOFF_WAR), \u0026quot;%\u0026quot;) value \u0026lt;- format(value, width = 9, justify = \u0026quot;right\u0026quot;) bar_chart(value, width = width, fill = \u0026quot;#3fc1c9\u0026quot;) } ) ), #columns end # grouping OFF,DEF,TOTAL under RAPTOR columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;)) ) )  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\",\"minWidth\":120,\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\",\"minWidth\":60,\"align\":\"left\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN. PLAYED\",\"type\":\"numeric\",\"format\":{\"cell\":{\"separators\":true},\"aggregated\":{\"separators\":true}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":[{\"background\":\"#FC482F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC442A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5943\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FF2C0F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC462D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB553E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA644F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD3D23\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4A32\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87866\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA624D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5943\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F77F6E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4F37\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC442A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87A69\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F5998B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4A32\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3ADA2\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE3E1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D2CD\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A89D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59789\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59B8E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59789\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59F93\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFEFF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87866\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA604B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87C6B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3ADA2\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA604B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F77F6E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A295\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C2BB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F5998B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D2CD\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA644F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B1A7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69587\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96F5C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F9715E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D0CA\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFDFDC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A69B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}]},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":[{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CDC7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5842\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD391E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59A8C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA634D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CDC7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDF3F3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96651\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59A8C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1C6C0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE2DF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDF7F7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BCB4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B2A8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5842\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEE9E7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59D90\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FF2C0F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD4026\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEF0EF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFEFF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68F81\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B2A8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA634D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFAFB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD3D22\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE5E3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE5E3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78879\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87B69\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78879\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEE9E7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78271\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FE361A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE2DF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BCB4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96955\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AEA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}]},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\",\"format\":{\"cell\":{\"prefix\":\"+\",\"digits\":1},\"aggregated\":{\"prefix\":\"+\",\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"backgroundColor\":\"#F5F5F5\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"P/O WAR\",\"type\":\"numeric\",\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"cell\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"85.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"70%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"75.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"77.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"64.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"4.28571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"91.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"72.8571428571428%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"97.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"51.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"64.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"12.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"100%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"20%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"50%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"55.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"44.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"20%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"27.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"2.85714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"40%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"30%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"31.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"44.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"41.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"12.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"61.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"75.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"52.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"52.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"68.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"28.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"68.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"70%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"22.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"77.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"30%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"47.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"55.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"60%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"28.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"25.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"18.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"58.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"47.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"51.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"37.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]}],\"minWidth\":100,\"align\":\"center\",\"style\":{\"fontFamily\":\"liberation mono\",\"whiteSpace\":\"pre\",\"fontSize\":14}}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"searchable\":true,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":10,\"compact\":true,\"height\":\"600px\",\"language\":{\"searchPlaceholder\":\"Search...\",\"noData\":\"No matches\"},\"dataKey\":\"e45ff651bf84abff9f2d529400297898\",\"key\":\"e45ff651bf84abff9f2d529400297898\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  Column Name Styling We’re putting the finishing touches on this table. We’ll change all column header font to “liberation mono” and we’ll set the defaultPageSize to 15.\nreactable( # data raptor_table, # header styling defaultColDef = colDef(headerStyle = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 16)), # styling for entire table height = 600, minRows = 10, showSortIcon = TRUE, compact = TRUE, pagination = TRUE, showPageInfo = TRUE, searchable = TRUE, language = reactableLang(searchPlaceholder = \u0026quot;Search...\u0026quot;, noData = \u0026quot;No matches\u0026quot;), # styling individual columns # columns start columns = list( NAME = colDef(minWidth = 120, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), SEASON = colDef(minWidth = 60, align = \u0026quot;left\u0026quot;, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), MIN_PLAYED = colDef(name = \u0026#39;MIN. PLAYED\u0026#39;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(separators = TRUE), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), OFF = colDef(name = \u0026quot;OFF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1), style = function(value){ # normalization is based on a cell\u0026#39;s value in relation to the range of values in the column normalized \u0026lt;- (value - min(raptor_table$OFF)) / (max(raptor_table$OFF) - min(raptor_table$OFF)) color \u0026lt;- color_palette(normalized) # with color set based on the color_palette function, we can feed it to the background parameter list(background = color, fontWeight = \u0026quot;bold\u0026quot;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14) } ), DEF = colDef(name = \u0026quot;DEF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1), style = function(value){ # normalization is based on a cell\u0026#39;s value in relation to the range of values in the column normalized \u0026lt;- (value - min(raptor_table$DEF)) / (max(raptor_table$DEF) - min(raptor_table$DEF)) color \u0026lt;- color_palette(normalized) # with color set based on the color_palette function, we can feed it to the background parameter list(background = color, fontWeight = \u0026quot;bold\u0026quot;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14) } ), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(prefix = \u0026quot;+\u0026quot;, digits = 1), style = list(backgroundColor = \u0026#39;#F5F5F5\u0026#39;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), WAR = colDef(minWidth = 60, align = \u0026quot;right\u0026quot;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), PLAYOFF_WAR = colDef(name = \u0026quot;P/O WAR\u0026quot;, minWidth = 100, align = \u0026#39;center\u0026#39;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, whiteSpace = \u0026quot;pre\u0026quot;, fontSize = 14), # render bar chart cell = function(value){ width \u0026lt;- paste0(value * 100 / max(raptor_table$PLAYOFF_WAR), \u0026quot;%\u0026quot;) value \u0026lt;- format(value, width = 9, justify = \u0026quot;right\u0026quot;) bar_chart(value, width = width, fill = \u0026quot;#3fc1c9\u0026quot;) } ) ), #columns end # grouping OFF,DEF,TOTAL under RAPTOR columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;), headerStyle = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 16)) ), defaultPageSize = 15 )  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"minWidth\":120,\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"minWidth\":60,\"align\":\"left\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN. PLAYED\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"separators\":true},\"aggregated\":{\"separators\":true}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":[{\"background\":\"#FC482F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC442A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5943\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FF2C0F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC462D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB553E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA644F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD3D23\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4A32\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87866\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA624D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5943\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F77F6E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4F37\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC442A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87A69\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F5998B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4A32\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3ADA2\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE3E1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D2CD\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A89D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59789\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59B8E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59789\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59F93\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFEFF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87866\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA604B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87C6B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3ADA2\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA604B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F77F6E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A295\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C2BB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F5998B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D2CD\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA644F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B1A7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69587\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96F5C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F9715E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D0CA\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFDFDC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A69B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}]},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":[{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CDC7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5842\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD391E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59A8C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA634D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CDC7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDF3F3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96651\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59A8C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1C6C0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE2DF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDF7F7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BCB4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B2A8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5842\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEE9E7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59D90\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FF2C0F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD4026\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEF0EF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFEFF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68F81\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B2A8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA634D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFAFB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD3D22\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE5E3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE5E3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78879\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87B69\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78879\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEE9E7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78271\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FE361A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE2DF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BCB4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96955\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AEA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}]},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"prefix\":\"+\",\"digits\":1},\"aggregated\":{\"prefix\":\"+\",\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"backgroundColor\":\"#F5F5F5\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"P/O WAR\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"cell\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"85.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"70%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"75.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"77.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"64.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"4.28571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"91.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"72.8571428571428%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"97.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"51.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"64.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"12.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"100%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"20%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"50%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"55.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"44.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"20%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"27.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"2.85714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"40%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"30%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"31.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"44.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"41.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"12.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"61.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"75.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"52.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"52.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"68.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"28.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"68.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"70%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"22.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"77.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"30%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"47.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"55.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"60%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"28.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"25.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"18.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"58.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"47.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"51.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"37.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]}],\"minWidth\":100,\"align\":\"center\",\"style\":{\"fontFamily\":\"liberation mono\",\"whiteSpace\":\"pre\",\"fontSize\":14}}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"searchable\":true,\"defaultPageSize\":15,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":10,\"compact\":true,\"height\":\"600px\",\"language\":{\"searchPlaceholder\":\"Search...\",\"noData\":\"No matches\"},\"dataKey\":\"9653e073ca42b9f5c6e75035f46c6137\",\"key\":\"9653e073ca42b9f5c6e75035f46c6137\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  Table Title and Subtitle To add a title/subtitle, we’ll save our table to a variable tbl, then wrap it around a html div. This allows us to add title and subtitle to the table that is technically external to the table itself.\nWe’ll also change the background color of the table itself using the reactableTheme() function.\noptions(reactable.theme = reactableTheme( backgroundColor = \u0026quot;#fffef0\u0026quot; )) tbl \u0026lt;- reactable( # data raptor_table, # header styling defaultColDef = colDef(headerStyle = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 16)), # styling for entire table height = 600, minRows = 10, showSortIcon = TRUE, compact = TRUE, pagination = TRUE, showPageInfo = TRUE, searchable = TRUE, language = reactableLang(searchPlaceholder = \u0026quot;Search...\u0026quot;, noData = \u0026quot;No matches\u0026quot;), # styling individual columns # columns start columns = list( NAME = colDef(minWidth = 120, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), SEASON = colDef(minWidth = 60, align = \u0026quot;left\u0026quot;, style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), MIN_PLAYED = colDef(name = \u0026#39;MIN. PLAYED\u0026#39;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(separators = TRUE), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), OFF = colDef(name = \u0026quot;OFF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1), style = function(value){ # normalization is based on a cell\u0026#39;s value in relation to the range of values in the column normalized \u0026lt;- (value - min(raptor_table$OFF)) / (max(raptor_table$OFF) - min(raptor_table$OFF)) color \u0026lt;- color_palette(normalized) # with color set based on the color_palette function, we can feed it to the background parameter list(background = color, fontWeight = \u0026quot;bold\u0026quot;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14) } ), DEF = colDef(name = \u0026quot;DEF.\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(digits = 1), style = function(value){ # normalization is based on a cell\u0026#39;s value in relation to the range of values in the column normalized \u0026lt;- (value - min(raptor_table$DEF)) / (max(raptor_table$DEF) - min(raptor_table$DEF)) color \u0026lt;- color_palette(normalized) # with color set based on the color_palette function, we can feed it to the background parameter list(background = color, fontWeight = \u0026quot;bold\u0026quot;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14) } ), TOTAL = colDef(name = \u0026quot;TOTAL\u0026quot;, minWidth = 60, align = \u0026#39;right\u0026#39;, format = colFormat(prefix = \u0026quot;+\u0026quot;, digits = 1), style = list(backgroundColor = \u0026#39;#F5F5F5\u0026#39;, fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), WAR = colDef(minWidth = 60, align = \u0026quot;right\u0026quot;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 14)), PLAYOFF_WAR = colDef(name = \u0026quot;P/O WAR\u0026quot;, minWidth = 100, align = \u0026#39;center\u0026#39;, format = colFormat(digits = 1), style = list(fontFamily = \u0026quot;liberation mono\u0026quot;, whiteSpace = \u0026quot;pre\u0026quot;, fontSize = 14), # render bar chart cell = function(value){ width \u0026lt;- paste0(value * 100 / max(raptor_table$PLAYOFF_WAR), \u0026quot;%\u0026quot;) value \u0026lt;- format(value, width = 9, justify = \u0026quot;right\u0026quot;) bar_chart(value, width = width, fill = \u0026quot;#3fc1c9\u0026quot;) } ) ), #columns end # grouping OFF,DEF,TOTAL under RAPTOR columnGroups = list( colGroup(name = \u0026quot;RAPTOR\u0026quot;, columns = c(\u0026quot;OFF\u0026quot;, \u0026quot;DEF\u0026quot;, \u0026quot;TOTAL\u0026quot;), headerStyle = list(fontFamily = \u0026quot;liberation mono\u0026quot;, fontSize = 16)) ), defaultPageSize = 15 ) After we’ve saved our work with the reactable function into the tbl variable, we can wrap that in a div and create the title and subtitle.\ndiv(class = \u0026quot;table-wrap\u0026quot;, div(class = \u0026quot;table-header\u0026quot;, div(class = \u0026quot;table-title\u0026quot;, \u0026quot;Top 100 Vintage NBA Seasons\u0026quot;), \u0026quot;This table highlights 538\u0026#39;s new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes.\u0026quot;), tbl ) Top 100 Vintage NBA Seasons This table highlights 538's new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes.   {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"NAME\":[\"Michael Jordan\",\"LeBron James\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Michael Jordan\",\"Stephen Curry\",\"LeBron James\",\"LeBron James\",\"Chris Paul\",\"Kevin Garnett\",\"Dwyane Wade\",\"Draymond Green\",\"John Stockton\",\"Larry Bird\",\"Tim Duncan\",\"John Stockton\",\"James Harden\",\"Chris Paul\",\"Michael Jordan\",\"Chris Paul\",\"Larry Bird\",\"John Stockton\",\"John Stockton\",\"Shaquille O'Neal\",\"LeBron James\",\"Magic Johnson\",\"Magic Johnson\",\"Kevin Garnett\",\"Larry Bird\",\"LeBron James\",\"LeBron James\",\"James Harden\",\"Kobe Bryant\",\"Magic Johnson\",\"Stephen Curry\",\"Dwyane Wade\",\"LeBron James\",\"Larry Bird\",\"Charles Barkley\",\"David Robinson\",\"John Stockton\",\"Magic Johnson\",\"Scottie Pippen\",\"LeBron James\",\"Hakeem Olajuwon\",\"David Robinson\",\"John Stockton\",\"John Stockton\",\"Larry Bird\",\"LeBron James\",\"Ray Allen\",\"Charles Barkley\",\"Kevin Durant\",\"Jason Kidd\",\"Magic Johnson\",\"Clyde Drexler\",\"Paul George\",\"Michael Jordan\",\"Scottie Pippen\",\"James Harden\",\"Hakeem Olajuwon\",\"Chris Paul\",\"David Robinson\",\"LeBron James\",\"Scottie Pippen\",\"John Stockton\",\"Kobe Bryant\",\"Kevin Durant\",\"Chris Paul\",\"Dirk Nowitzki\",\"Dirk Nowitzki\",\"John Stockton\",\"Mookie Blaylock\",\"Tim Duncan\",\"Kareem Abdul-Jabbar\",\"Tracy McGrady\",\"Anfernee Hardaway\",\"Magic Johnson\",\"Julius Erving\",\"Scottie Pippen\",\"Manu Ginobili\",\"Gary Payton\",\"Kobe Bryant\",\"Nikola Jokic\",\"David Robinson\",\"Stephen Curry\",\"Michael Jordan\",\"Scottie Pippen\",\"Dwyane Wade\",\"Kawhi Leonard\",\"Dwyane Wade\",\"Kevin Garnett\",\"Jason Kidd\",\"Karl Malone\",\"Tim Duncan\",\"Tim Hardaway\"],\"SEASON\":[1991,2009,1989,1990,2016,1988,1993,1992,1996,2015,2010,2013,2009,2004,2006,2016,1992,1986,2003,1991,2019,2008,1997,2015,1987,1988,1989,2000,2012,1987,1991,2003,1988,2011,2007,2018,2008,1989,2017,2009,2008,1985,1990,1994,1994,1990,1992,2006,1993,1995,1990,1997,1984,2016,2001,1993,2014,2003,1982,1992,2019,1987,1996,2015,1994,2014,1996,2017,1997,1995,2009,2013,2011,2006,2003,1996,1997,2002,1977,2003,1996,1983,1982,1991,2005,1996,2003,2019,1991,2014,1998,1995,2011,2017,2010,2008,2002,1996,2007,1997],\"MIN_PLAYED\":[3723,3634,3973,3871,3314,3738,3850,4022,3823,3439,3426,3837,3203,4014,3851,3687,3625,3883,4202,3476,3291,3492,3910,3302,4020,3320,3310,4163,3309,3570,3756,3586,3728,3985,4083,3172,4055,3404,3239,3333,3579,3976,3504,3387,3566,3313,4063,3965,3760,3697,3109,3635,3989,3531,3897,3885,3937,3841,3553,3598,3045,3409,3567,3617,4266,2643,3372,3538,3848,3060,3900,3604,3130,4072,3839,3594,3497,3709,3483,3262,3488,3550,3569,3718,2965,4073,3932,3061,3261,3142,4053,3410,3651,2903,3002,3315,3859,3838,3462,3837],\"OFF\":[9.1,9.3,8.3,8.6,10.4,7.5,7.6,7,7.6,8.6,9.2,8.5,8.2,4.4,6.4,3.9,7.7,6.4,3.9,7.8,9.6,9,6.9,8.6,6.5,7.6,7.5,4.4,7.9,8.2,8.3,4.9,6.6,6,4.6,8.8,6.1,8.6,9.3,7.1,6.8,5.4,7.5,4.6,7,9,4.5,6.4,2,2.8,8.2,7.1,4.7,6,7.1,5.5,7.1,4.4,6.1,6.4,5.3,5.5,5.1,7.7,0.8,7.7,3.6,6.9,4.9,8,5.9,6.5,6.7,5.9,4.6,7,4.6,3.6,4.5,8,6.6,6.5,5,3.9,5.9,3.5,5.4,6.1,2.8,7.8,4.3,3.6,5.6,7.3,7.2,2.9,4.1,3.9,2.2,4.8],\"DEF\":[3.2,3.2,2.7,2.2,2.1,3.7,2.8,2.7,2.7,2.4,2.3,1.1,3.7,4.5,2.8,5.4,2.2,2.6,4.2,2.2,1.1,1,1.8,2.1,1.7,2.8,2.8,3.2,2.1,0.8,0,4.1,1.8,1.5,2.6,1.3,1.2,0.5,-0.1,2.3,1.6,1.9,1.2,4.5,1.4,0.3,2.5,0.8,5.8,5.2,1.7,0.9,2.3,2.2,0.1,1.7,-0.3,2.9,1.9,1.4,4.2,2.8,2.7,-0.2,5.3,3.7,5,0.9,2.1,1.5,0.8,1,2.4,0.4,2.2,0.4,3.1,3.5,3.1,0.3,1,0.9,2.4,3,3.3,2.4,0.9,2.7,5.5,0.5,1.6,4,1.2,2,1.7,5,2.1,2.3,5,1.4],\"TOTAL\":[12.3,12.6,11,10.8,12.5,11.2,10.4,9.6,10.3,11,11.4,9.6,11.8,8.8,9.2,9.4,9.9,9,8.1,10.1,10.7,10,8.6,10.7,8.2,10.4,10.2,7.6,10,9,8.4,8.9,8.4,7.6,7.3,10.1,7.2,9.2,9.2,9.4,8.4,7.3,8.6,9.1,8.4,9.3,7,7.3,7.8,7.9,9.9,8,7.1,8.2,7.2,7.2,6.8,7.3,8.1,7.8,9.5,8.4,7.8,7.5,6.1,11.4,8.5,7.8,7,9.5,6.7,7.5,9,6.3,6.8,7.5,7.7,7.1,7.6,8.4,7.6,7.4,7.3,6.8,9.2,6,6.3,8.7,8.2,8.3,6,7.6,6.8,9.3,8.9,7.8,6.2,6.2,7.2,6.2],\"WAR\":[28.8,28.5,28,27,26.7,26.6,26,25.5,25.5,25.1,24.8,24.2,23.8,23.6,23.5,23.5,23.4,23.3,23.3,22.8,22.8,22.8,22.7,22.6,22.4,22.3,21.9,21.8,21.6,21.3,21.3,21.2,21.1,21.1,21,20.9,20.8,20.7,20.7,20.6,20.5,20.4,20.4,20.4,20.3,20.3,20.2,20.2,20.2,20.1,20.1,20,20,19.9,19.8,19.7,19.7,19.7,19.6,19.6,19.4,19.4,19.4,19.3,19.3,19.3,19.2,19.2,19.2,19,19,18.9,18.8,18.8,18.8,18.8,18.6,18.5,18.5,18.5,18.5,18.4,18.4,18.3,18.3,18.3,18.2,18.2,18.2,18.1,18.1,18,17.9,17.9,17.9,17.9,17.7,17.6,17.6,17.6],\"PLAYOFF_WAR\":[6,5.7,4.9,5.3,3,2.3,5.7,5.4,4.5,5.2,3,5.8,0.3,3.4,6.4,5.7,3.4,5.1,6.8,2.7,2.7,3.6,4.5,3.2,4.4,3.4,0.9,4.4,7,4.4,4.6,1.4,3.5,4.6,5,3.9,4.7,3.1,5,1.4,3,3.4,1.9,0.2,2.8,2.1,4.7,2.2,3.1,2.9,0.9,4.3,5.7,5.7,4.6,5.3,3.7,4,3.7,4.8,2,0.7,4.8,3.2,4.9,4,1.6,5.8,3.2,0.7,5.8,2.3,1.7,5.4,3,3.4,2.1,2.3,3,1.7,2.7,3.3,3.9,4,5.2,4.2,1.7,5.2,0.7,2,4.7,1.8,4.4,4,1.3,5,4.1,3.3,3.6,2.6]},\"columns\":[{\"accessor\":\"NAME\",\"name\":\"NAME\",\"type\":\"character\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"minWidth\":120,\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"SEASON\",\"name\":\"SEASON\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"minWidth\":60,\"align\":\"left\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"MIN_PLAYED\",\"name\":\"MIN. PLAYED\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"separators\":true},\"aggregated\":{\"separators\":true}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"OFF\",\"name\":\"OFF.\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":[{\"background\":\"#FC482F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC442A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5943\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FF2C0F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC462D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB553E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA644F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD3D23\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4A32\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87866\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96954\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA624D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5943\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F77F6E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4F37\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB533C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC442A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87A69\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F5998B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96B57\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC4A32\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3ADA2\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE3E1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D2CD\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA5C46\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A89D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59789\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AFA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78372\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59B8E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59789\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59F93\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFEFF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96752\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87866\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA604B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87C6B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87663\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4AAA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3ADA2\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA604B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F77F6E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78170\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A295\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68E7F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C2BB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F5998B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68A7A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D2CD\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA644F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B1A7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B9\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69587\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96F5C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F9715E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D0CA\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BAB1\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFDFDC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A69B\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}]},{\"accessor\":\"DEF\",\"name\":\"DEF.\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":[{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CDC7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5842\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD391E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59A8C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA634D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CDC7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78575\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDF3F3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96651\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B5AC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59A8C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1C6C0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE2DF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDF7F7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BCB4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B2A8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FB5842\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEE9E7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59D90\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FF2C0F\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD4026\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEF0EF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFEFF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68F81\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3B2A8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FA634D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F69385\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EDFAFB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FD3D22\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87461\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C0B8\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D8D3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE5E3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A79C\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE5E3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78879\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F87B69\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78879\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EEE9E7\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D1CB\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F68C7D\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F78271\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A194\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F0D4CF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F59688\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FE361A\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#EFE2DF\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2BCB4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F96955\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F1CAC3\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F3AEA4\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2B9B0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4ABA0\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F4A498\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#FC472E\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14},{\"background\":\"#F2C3BC\",\"fontWeight\":\"bold\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}]},{\"accessor\":\"TOTAL\",\"name\":\"TOTAL\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"prefix\":\"+\",\"digits\":1},\"aggregated\":{\"prefix\":\"+\",\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"backgroundColor\":\"#F5F5F5\",\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"WAR\",\"name\":\"WAR\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"minWidth\":60,\"align\":\"right\",\"style\":{\"fontFamily\":\"liberation mono\",\"fontSize\":14}},{\"accessor\":\"PLAYOFF_WAR\",\"name\":\"P/O WAR\",\"type\":\"numeric\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"format\":{\"cell\":{\"digits\":1},\"aggregated\":{\"digits\":1}},\"cell\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"85.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"70%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"75.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"77.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"64.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"4.28571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"91.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"72.8571428571428%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 6.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"97.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"51.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"64.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"12.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"100%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"20%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"50%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"55.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"44.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"20%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"27.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"2.85714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"40%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"30%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"31.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"44.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"41.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"12.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"61.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"81.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"65.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"75.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"52.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"52.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"68.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"28.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"68.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"70%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"22.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"45.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"82.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"77.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"48.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"30%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"32.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"42.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"38.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"47.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.9\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"55.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"60%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"24.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5.2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"74.2857142857143%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 0.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"10%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"28.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.7\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"67.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.8\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"25.7142857142857%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"62.8571428571429%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"57.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 1.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"18.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 5\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"71.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 4.1\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"58.5714285714286%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.3\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"47.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 3.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"51.4285714285714%\",\"height\":\"14px\"}},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"style\":{\"display\":\"flex\",\"alignItems\":\"center\"}},\"children\":[\" 2.6\",{\"name\":\"div\",\"attribs\":{\"style\":{\"flexGrow\":1,\"marginLeft\":\"6px\",\"background\":null}},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"background\":\"#3fc1c9\",\"width\":\"37.1428571428571%\",\"height\":\"14px\"}},\"children\":[]}]}]}],\"minWidth\":100,\"align\":\"center\",\"style\":{\"fontFamily\":\"liberation mono\",\"whiteSpace\":\"pre\",\"fontSize\":14}}],\"columnGroups\":[{\"name\":\"RAPTOR\",\"headerStyle\":{\"fontFamily\":\"liberation mono\",\"fontSize\":16},\"columns\":[\"OFF\",\"DEF\",\"TOTAL\"]}],\"searchable\":true,\"defaultPageSize\":15,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":10,\"compact\":true,\"height\":\"600px\",\"theme\":{\"backgroundColor\":\"#fffef0\"},\"language\":{\"searchPlaceholder\":\"Search...\",\"noData\":\"No matches\"},\"dataKey\":\"9653e073ca42b9f5c6e75035f46c6137\",\"key\":\"9653e073ca42b9f5c6e75035f46c6137\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}  The cool thing is that we can style the table with css directly in Rmarkdown (just make sure your code chunk has css instead of r just for the css parts). We’ll use css to style our title and subtitle to make it consistent with the table itself.\n.table-title { font-size: 20px; font-weight: 600; font-family: \u0026quot;liberation mono\u0026quot;; padding: 10px 10px 10px 3px; } .table-header { font-size: 14px; font-weight: 300; font-family: \u0026quot;liberation mono\u0026quot;; padding: 10px 10px 10px 10px; } .table-wrap { box-shadow: 2px 3px 20px black; background-color: #fffef0; } .table-title { font-size: 20px; font-weight: 600; font-family: \"liberation mono\"; padding: 10px 10px 10px 3px; } .table-header { font-size: 14px; font-weight: 300; font-family: \"liberation mono\"; padding: 10px 10px 10px 10px; } .table-wrap { box-shadow: 2px 3px 20px black; background-color: #fffef0; }   ","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604150652,"objectID":"5081704b1fd505d8a210604f3179f8ce","permalink":"/post/vintage-nba/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/post/vintage-nba/","section":"post","summary":"RStudio Table Competition 2020 entry in the tutorial category","tags":["Data Viz","RStats","reactable"],"title":"Vintage NBA Seasons","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Table of Content:\n  Set Up  Functions  Strings  Exceptions  Lists  Tuples  Dictionaries  defaultdict  Counters  Sets  Control Flow  Truthiness  Sorting  List Comprehensions  Assert  Object-Oriented Programming  Iterables \u0026amp; Generators  Pseudorandomness  Regular Expression  Chapter 2: A Crash Course in Python This is the first of many chapters i\u0026rsquo;ll be covering from Joel Grus' Data Science from Scratch book (2nd edition). This chapter provides a quick survey of python features needed for \u0026ldquo;doing\u0026rdquo; data science from scratch, including essential setup of virtual environments and other tooling.\nWhile the chapter is not meant to be comprehensive, I may supplement certain sections with external content for greater detail in certain parts.\nMy goal is twofold. First, to go through this book and, as a byproduct, learn python. Second, to look out for and highlight the areas where the pythonic way of doing things is necessary to accomplish something in the data science process.\nAt several sections throughout this chapter, the author emphasises how much a particular feature will be used later in the book (e.g., functions, dictionaries, list, list comprehensions (and for-loops), assert, iterables and generators, randomness, type annotations). Things not used as much (e.g., sets, automated test, subclasses that inherit functionality from a parent class, zip and argument unpacking, args, kwargs).\nAdditional code can be found in this repo\nSetup Installation, Virtual Environment and Modules These section takes the reader through installing a virtual environment using Anaconda Python distribution. The author points out a best practice, \u0026ldquo;you should always work in a virtual environment and never use \u0026lsquo;base\u0026rsquo; Python installation\u0026rdquo;. Moreover, the author favors IPython over jupyter notebooks (he\u0026rsquo;s a noted critic of the notebook)\nJeremy Howard of fast.ai offers a contrasting perspective. He does like notebooks.\nThe first time I installed Python, it took me awhile to get things right and eventually I relied on jupyter notebooks through Anaconda. As we go through this book, I\u0026rsquo;ll be using virtual environments and IPython as much as I can (although I may sprinkle in a notebook here and there). My IDE for interacting with the conda virtual environment and IPython will be VSCode.\nFortunately, I had a relatively painless process setting up a virtual environment and IPython, although I had to take a slight detour to setup the code command for VSCode.\nHere\u0026rsquo;s a summary of the commands I used for setup:\nFunctions Three things are emphasized here: passing functions as arguments for other functions, lambda functions and default parameter values.\nThe illustration of functions being passed as arguments is demonstrated below. A function double is created. A function apply_to_one is created. The double function is pointed at my_double. We pass my_double into the apply_to_one function and set that to x.\nWhatever function is passed to apply_to_one, its argument is 1. So passing my_double means we are doubling 1, so x is 2.\nBut the important thing is that a function got passed to another function (aka higher order functions).\ndef double(x): \u0026quot;\u0026quot;\u0026quot; this function doubles and returns the argument \u0026quot;\u0026quot;\u0026quot; return x * 2 def apply_to_one(f): \u0026quot;\u0026quot;\u0026quot;Calls the function f with 1 as its argument\u0026quot;\u0026quot;\u0026quot; return f(1) my_double = double # x is 2 here x = apply_to_one(my_double) # extending this example def apply_five_to(e): \u0026quot;\u0026quot;\u0026quot;returns the function e with 5 as its argument\u0026quot;\u0026quot;\u0026quot; return e(5) # doubling 5 is 10 w = apply_five_to(my_double)  Since functions are going to be used extensively, here\u0026rsquo;s another more complicated example. I found this from Trey Hunner\u0026rsquo;s site. Two functions are defined - square and cube. Both functions are saved to a list called operations. Another list, numbers is created.\nFinally, a for-loop is used to iterate through numbers, and the enumerate property allows access to both index and item in numbers. That\u0026rsquo;s used to find whether the action is a square or cube (operations[0] is square, operations[1] is cube), which is then given as its argument, the items inside the numbers list.\n# create two functions def square(n): return n**2 def cube(n): return n**3 # store those functions inside a list, operations, to reference later operations = [square, cube] # create a list of numbers numbers = [2,1,3,4,7,11,18,29] # loop through the numbers list # using enumerate the identify index and items # [i % 2] results in either 0 or 1, that's pointed at action # using the dunder, name, retrieves the name of the function - either square or cube - from the operations list # print __name__ along with the item from the numbers list # action is either a square or cube for i, n in enumerate(numbers): action = operations[i % 2] print(f\u0026quot;{action.__name__}({n}):\u0026quot;, action(n)) # print square(2): 4 cube(1): 1 square(3): 9 cube(4): 64 square(7): 49 cube(11): 1331 square(18): 324 cube(29): 24389 # more explicit, yet verbose way to write the for-loop for index, num in enumerate(numbers): action = operations[index % 2] print(f\u0026quot;{action.__name__}({num}):\u0026quot;, action(num))  This section also introduces lambda functions (aka anonymous functions) to demonstrate how functions, being first-class in Python, can, like any variable, be passed into the argument of another function. However, with lambda instead of defining functions with def, it is defined inside another function. Here\u0026rsquo;s an illustration:\n# we'll reuse apply_five_to, which takes in a function and provides '5' as the argument def apply_five_to(e): \u0026quot;\u0026quot;\u0026quot;returns the function e with 5 as its argument\u0026quot;\u0026quot;\u0026quot; return e(5) # this lambda function adds '4' to any argument # when passing this lambda function to apply_five_to # you get y = 5 + 4 y = apply_five_to(lambda x: x + 4) # we can also change what the lambda function does without defining a separate function # here the lambda function multiplies the argument by 4 # y = 20 y = apply_five_to(lambda x: x * 4)  Lambda functions are convenient in that you can pass it into another function immediately without having to define it separately, but the consensus seems to be that you should just use def.\nHere\u0026rsquo;s an external example of lambda functions from Trey Hunner. In this example, a lambda function is used within a filter function that takes in two arguments.\n# calling help(filter) displays an explanation class filter(object) | filter(function or None, iterable) --\u0026gt; filter object # create a list of numbers numbers = [2,1,3,4,7,11,18,29] # the lambda function will return n if it is an even number # we filter the numbers list using the lambda function # wrapped in a list, this returns [2,4,18] list(filter(lambda n: n % 2 == 0, numbers))  There are whole books, or at least whole chapters, that can be written about Python functions, but we\u0026rsquo;ll limit our discussion for now to the idea that functions can be passed as arguments to other functions. I\u0026rsquo;ll report back on this section as we progress through the book.\nStrings Strings may not be terribly exciting for data science or machine learning, unless you\u0026rsquo;re getting into natural language processing, so we\u0026rsquo;ll keep it brief here. The key take aways are that backslashes encode special characters and that f-strings is the most updated way to do string interpolation. Here are some examples:\n# point strings to variables (we'll use my name) first_name = \u0026quot;Paul\u0026quot; last_name = \u0026quot;Apivat\u0026quot; # f-string (recommended), 'Paul Apivat' f_string = f\u0026quot;{first_name} {last_name}\u0026quot; # string addition, 'Paul Apivat' string_addition = first_name + \u0026quot; \u0026quot; + last_name # string format, 'Paul Apivat' string_format = \u0026quot;{0} {1}\u0026quot;.format(first_name, last_name) # percent format (NOT recommended), 'Paul Apivat' pct_format = \u0026quot;%s %s\u0026quot; %('Paul','Apivat')  Exceptions The author covers exceptions to make the point that they\u0026rsquo;re not all that bad in Python and it\u0026rsquo;s worth handling exceptions yourself to make code more readable. Here\u0026rsquo;s my own example that\u0026rsquo;s slightly different from the book:\ninteger_list = [1,2,3] heterogeneous_list = [\u0026quot;string\u0026quot;, 0.1, True] # you can sum a list of integers, here 1 + 2 + 3 = 6 sum(integer_list) # but you cannot sum a list of heterogeneous data types # doing so raises a TypeError sum(heterogeneous_list) # the error crashes your program and is not fun to look at --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-12-3287dd0c6c22\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 sum(heterogeneous_list) TypeError: unsupported operand type(s) for +: 'int' and 'str' # so the idea is to handle the exceptions with your own messages try: sum(heterogeneous_list) except TypeError: print(\u0026quot;cannot add objects of different data types\u0026quot;)  At this point, the primary benefits to handling exceptions yourself is for code readability, so we\u0026rsquo;ll come back to this section if we see more useful examples.\nLists Lists are fundamental to Python so I\u0026rsquo;m going to spend some time exploring their features. For data science, NumPy arrays are used frequently, so I thought it\u0026rsquo;d be good to implement all list operations covered in this section in Numpy arrays to tease apart their similarities and differences.\nBelow are the similarities.\nThis implies that whatever can be done in python lists can also be done in numpy arrays, including: getting the nth element in the list/array with square brackets, slicing the list/array, iterating through the list/array with start, stop, step, using the in operator to find list/array membership, checking length and unpacking list/arrays.\n# setup import numpy as np # create comparables python_list = [1,2,3,4,5,6,7,8,9] numpy_array = np.array([1,2,3,4,5,6,7,8,9]) # bracket operations # get nth element with square bracket python_list[0] # 1 numpy_array[0] # 1 python_list[8] # 9 numpy_array[8] # 9 python_list[-1] # 9 numpy_array[-1] # 9 # square bracket to slice python_list[:3] # [1, 2, 3] numpy_array[:3] # array([1, 2, 3]) python_list[1:5] # [2, 3, 4, 5] numpy_array[1:5] # array([2, 3, 4, 5]) # start, stop, step python_list[1:8:2] # [2, 4, 6, 8] numpy_array[1:8:2] # array([2, 4, 6, 8]) # use in operator to check membership 1 in python_list # true 1 in numpy_array # true 0 in python_list # false 0 in numpy_array # false # finding length len(python_list) # 9 len(numpy_array) # 9 # unpacking x,y = [1,2] # now x is 1, y is 2 w,z = np.array([1,2]) # now w is 1, z is 2  Now, here are the differences.\nThese tasks can be done in python lists, but require a different approach for NumPy array including: modification (extend in list, append for array). Finally, lists can store mixed data types, while NumPy array will convert to string.\n# python lists can store mixed data types heterogeneous_list = ['string', 0.1, True] type(heterogeneous_list[0]) # str type(heterogeneous_list[1]) # float type(heterogeneous_list[2]) # bool # numpy arrays cannot store mixed data types # numpy arrays turn all data types into strings homogeneous_numpy_array = np.array(['string', 0.1, True]) # saved with mixed data types type(homogeneous_numpy_array[0]) # numpy.str_ type(homogeneous_numpy_array[1]) # numpy.str_ type(homogeneous_numpy_array[2]) # numpy.str_ # modifying list vs numpy array # lists can use extend to modify list in place python_list.extend([10,12,13]) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13] numpy_array.extend([10,12,13]) # AttributeError: 'numpy.ndarray' # numpy array must use append, instead of extend numpy_array = np.append(numpy_array,[10,12,13]) # python lists can be added with other lists new_python_list = python_list + [14,15] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15] numpy_array + [14,15] # ValueError # numpy array cannot be added (use append instead) # array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15]) new_numpy_array = np.append(numpy_array, [14,15]) # python lists have the append attribute python_list.append(0) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 0] # the append attribute for numpy array is used differently numpy_array = np.append(numpy_array, [0])  Python lists and NumPy arrays have much in common, but there are meaningful differences as well.\nPython Lists vs NumPy Arrays: What\u0026rsquo;s the difference Now that we know that there are meaningful differences, what can we attribute these differences to? This explainer from UCF highlights performance differences including:\n Size Performance Functionality  I\u0026rsquo;m tempted to go down this 🐇 🕳️ of further lists vs array comparisons, but we\u0026rsquo;ll hold off for now.\nTuples Similar to lists, but tuples are immutable.\nmy_list = [1,2] # check type(my_list) my_tuple = (1,2) # check type(my_tuple) other_tuple = 3,4 # tuples don't require parentheses my_list[1] = 3 # lists ARE mutable, my_list is now [1,3] # exception handling when trying to change tuple try: my_tuple[1] = 3 except TypeError: print(\u0026quot;tuples are immutable\u0026quot;)  Tuples are good at returning multiple values from functions:\n# use tuple to return multiple values def sum_and_product(x,y): \u0026quot;\u0026quot;\u0026quot;you can return multiple values from functions using tuples\u0026quot;\u0026quot;\u0026quot; return (x + y), (x * y) sp = sum_and_product(4,5) # sp is (9,20), a tuple  However, lists can also be used to return multiple values:\ndef sum_and_product_list(x,y): return [(x + y), (x * y)] spl = sum_and_product_list(5,6) # [11, 30] type(spl) # list  Finally, both tuples and lists can be used for multiple assignments, here\u0026rsquo;s a pythonic way to swap variables:\nx, y = 1,2 x,y = y,x  Tuples, for the most part, seem to be redundant with lists, but we\u0026rsquo;ll see if there are special use-cases for immutability down the line.\nDictionaries Dictionaries are good for storing structured data. They have a key/value pair so you can look up values of certain keys. The author provides some ways to initialize a dictionary, with comments about what is more or less pythonic (I\u0026rsquo;ll take the author\u0026rsquo;s word for it, but open to other perspectives).\nSome of the things you can do with dictionaries are query keys, values, assign new key/value pairs, check for existence of keys or retrieve certain values.\nempty_dict = {} # most pythonic empty_dict2 = dict() # less pythonic grades = {\u0026quot;Joel\u0026quot;: 80, \u0026quot;Grus\u0026quot;: 99} # dictionary literal type(grades) # type check, dict # use bracket to look up values grades[\u0026quot;Grus\u0026quot;] # 99 grades[\u0026quot;Joel\u0026quot;] # 80 # KeyError for looking up non-existent keys try: kate_grades = grades[\u0026quot;Kate\u0026quot;] except KeyError: print(\u0026quot;That key doesn't exist\u0026quot;) # use in operator to check existence of key joe_has_grade = \u0026quot;Joel\u0026quot; in grades joe_has_grade # true kate_does_not = \u0026quot;Kate\u0026quot; in grades kate_does_not # false # use 'get' method to get values in dictionaries grades.get(\u0026quot;Joel\u0026quot;) # 80 grades.get(\u0026quot;Grus\u0026quot;) # 99 grades.get(\u0026quot;Kate\u0026quot;) # default: None # assign new key/value pair using brackets grades[\u0026quot;Tim\u0026quot;] = 93 grades # {'Joel': 80, 'Grus': 99, 'Tim': 93}  Dictionaries are good for representing structured data that can be queries. The key take-away here is that in order to iterate through dictionaries to get either keys, values or both, we\u0026rsquo;ll need to use specific methods likes keys(), values() or items().\ntweet = { \u0026quot;user\u0026quot;: \u0026quot;paulapivat\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Reading Data Science from Scratch\u0026quot;, \u0026quot;retweet_count\u0026quot;: 100, \u0026quot;hashtags\u0026quot;: [\u0026quot;#66daysofdata\u0026quot;, \u0026quot;datascience\u0026quot;, \u0026quot;machinelearning\u0026quot;, \u0026quot;python\u0026quot;, \u0026quot;R\u0026quot;] } # query specific values tweet[\u0026quot;retweet_count\u0026quot;] # 100 # query values within a list tweet[\u0026quot;hashtags\u0026quot;] # ['#66daysofdata', 'datascience', 'machinelearning', 'python', 'R'] tweet[\u0026quot;hashtags\u0026quot;][2] # 'machinelearning' # retrieve ALL keys tweet_keys = tweet.keys() tweet_keys # dict_keys(['user', 'text', 'retweet_count', 'hashtags']) type(tweet_keys) # different data type: dict != dict_keys # retrieve ALL values tweet_values = tweet.values() tweet_values # dict_values(['paulapivat', 'Reading Data Science from Scratch', 100, ['#66daysofdata', 'datascience', 'machinelearning', 'python', 'R']]) type(tweet_values) # different data type: dict != dict_values # create iterable for Key-Value pairs (in tuple) tweet_items = tweet.items() # iterate through tweet_items() for key,value in tweet_items: print(\u0026quot;These are the keys:\u0026quot;, key) print(\u0026quot;These are the values:\u0026quot;, value) # cannot iterate through original tweet dictionary # ValueError: too many values to unpack (expected 2) for key, value in tweet: print(key) # cannot use 'enumerate' because that only provides index and key (no value) for key, value in enumerate(tweet): print(key) # print 0 1 2 3 - index values print(value) # user text retweet_count hashtags (incorrectly print keys)  Just like in lists and tuples, you can use the in operator to find membership. The one caveat is you cannot look up values that are in lists, unless you use bracket notation to help.\n# search keys \u0026quot;user\u0026quot; in tweet # true \u0026quot;bball\u0026quot; in tweet # false \u0026quot;paulapivat\u0026quot; in tweet_values # true 'python' in tweet_values # false (python is nested in 'hashtags') \u0026quot;hashtags\u0026quot; in tweet # true # finding values inside a list requires brackets to help 'python' in tweet['hashtags'] # true  What is or is not hashable?\nDictionary keys must be hashable.\nStrings are hashable. So we can use strings as dictionary keys, but we cannot use lists because they are not hashable.\npaul = \u0026quot;paul\u0026quot; type(paul) # check type, str hash(paul) # -3897810863245179227 ; strings are hashable paul.__hash__() # -3897810863245179227 ; another way to find the hash jake = ['jake'] # this is a list type(jake) # check type, list # lists are not hashable - cannot be used as dictionary keys try: hash(jake) except TypeError: print('lists are not hashable')  defaultdict defaultdict is a subclass of dictionaries (dict, see previous post), so it inherits most of its behavior from dict with additional features. To understand how those features make it different, and more convenient in some cases, we\u0026rsquo;ll need to run into some errors.\nIf we try to count words in a document, the general approach is to create a dictionary where the dictionary keys are words and the dictionary values are counts of those words.\nLet\u0026rsquo;s try do do this with a regular dictionary.\nFirst, to setup, we\u0026rsquo;ll take a list of words and split() into individual words. I took this paragraph from another project i\u0026rsquo;m working on and artificially added some extra words to ensure that certain words appeared more than once (it\u0026rsquo;ll be apparent why soon).\n# paragraph lines = [\u0026quot;This table highlights 538's new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes minutes the table Wins NBA NBA RAPTOR more players\u0026quot;] # split paragraphy into individual words lines = \u0026quot; \u0026quot;.join(lines).split() type(lines) # list  Now that we have our lines list, we\u0026rsquo;ll create an empty dict called word_counts and have each word be the key and each value be the count of that word.\n# empty list word_counts = {} # loop through lines to count each word for word in lines: word_counts[word] += 1 # KeyError: 'This'  We received a KeyError for the very first word in lines (i.e. \u0026lsquo;This\u0026rsquo;) because the list tried to count a key that didn\u0026rsquo;t exist. We\u0026rsquo;ve learned to handle exceptions so we can use try and except.\nHere, we\u0026rsquo;re looping through lines and when we try to count a key that doesn\u0026rsquo;t exist, like we did previously, we\u0026rsquo;re now anticipating a KeyError and will set the initial count to 1, then it can continue to loop-through and count the word, which now exists, so it can be incremented up.\n# empty list word_counts = {} # exception handling for word in lines: try: word_counts[word] += 1 except KeyError: word_counts[word] = 1 # call word_counts # abbreviated for space word_counts {'This': 1, 'table': 3, 'highlights': 1, \u0026quot;538's\u0026quot;: 1, 'new': 1, 'NBA': 3, 'statistic,': 1, 'RAPTOR,': 1, 'in': 2, 'addition': 1, 'to': 3, 'the': 5, 'more': 2, ... 'top-100': 1, 'players': 2, 'who': 1, 'have': 1, 'played': 1, 'at': 1, 'least': 1, '1,000': 1, 'minutes': 2, 'RAPTOR': 1}  Now, there are other ways to achieve the above:\n# use conditional flow word_counts = {} for word in lines: if word in word_counts: word_counts[word] += 1 else: word_counts[word] = 1 # use get for word in lines: previous_count = word_counts.get(word, 0) word_counts[word] = previous_count + 1  Here\u0026rsquo;s where the author makes the case for defaultdict, arguing that the two aforementioned approaches are unweildy. We\u0026rsquo;ll come back full circle to try our first approach, using defaultdict instead of the traditional dict.\ndefaultdict is a subclass of dict and must be imported from collections:\nfrom collections import defaultdict word_counts = defaultdict(int) for word in lines: word_counts[word] += 1 # we no longer get a KeyError # abbreviated for space defaultdict(int, {'This': 1, 'table': 3, 'highlights': 1, \u0026quot;538's\u0026quot;: 1, 'new': 1, 'NBA': 3, 'statistic,': 1, 'RAPTOR,': 1, 'in': 2, 'addition': 1, 'to': 3, 'the': 5, 'more': 2, ... 'top-100': 1, 'players': 2, 'who': 1, 'have': 1, 'played': 1, 'at': 1, 'least': 1, '1,000': 1, 'minutes': 2, 'RAPTOR': 1})  Unlike a regular dictionary, when defaultdict tries to look up a key it doesn\u0026rsquo;t contain, it\u0026rsquo;ll automatically add a value for it using the argument we provided when we first created the defaultdict. If you see above, we entered an int as the argument, which allows it to automatically add an integer value.\nIf you want your defaultdict to have values be lists, you can pass a list as argument. Then, when you append a value, it is automatically contained in a list.\ndd_list = defaultdict(list) # defaultdict(list, {}) dd_list[2].append(1) # defaultdict(list, {2: [1]}) dd_list[4].append('string') # defaultdict(list, {2: [1], 4: ['string']})  You can also pass a dict into defaultdict, ensuring that all appended values are contained in a dict:\ndd_dict = defaultdict(dict) # defaultdict(dict, {}) # match key-with-value dd_dict['first_name'] = 'lebron' # defaultdict(dict, {'first_name': 'lebron'}) dd_dict['last_name'] = 'james' # match key with dictionary containing another key-value pair dd_dict['team']['city'] = 'Los Angeles' # defaultdict(dict, # {'first_name': 'lebron', # 'last_name': 'james', # 'team': {'city': 'Los Angeles'}})  Application: Grouping with defaultdict The follow example is from Real Python, a fantastic resource for all things Python.\nIt is common to use defaultdict to group items in a sequence or collection, setting the initial parameter (aka .default_factory) set to list.\ndep = [('Sales', 'John Doe'), ('Sales', 'Martin Smith'), ('Accounting', 'Jane Doe'), ('Marketing', 'Elizabeth Smith'), ('Marketing', 'Adam Doe')] from collections import defaultdict dep_dd = defaultdict(list) for department, employee in dep: dep_dd[department].append(employee) dep_dd #defaultdict(list, # {'Sales': ['John Doe', 'Martin Smith'], # 'Accounting': ['Jane Doe'], # 'Marketing': ['Elizabeth Smith', 'Adam Doe']})  What happens when you have duplicate entries? We\u0026rsquo;re jumping ahead slightly to use set handle duplicates and only group unique entries:\n# departments with duplicate entries dep = [('Sales', 'John Doe'), ('Sales', 'Martin Smith'), ('Accounting', 'Jane Doe'), ('Marketing', 'Elizabeth Smith'), ('Marketing', 'Elizabeth Smith'), ('Marketing', 'Adam Doe'), ('Marketing', 'Adam Doe'), ('Marketing', 'Adam Doe')] # use defaultdict with set dep_dd = defaultdict(set) # set object has no attribute 'append' # so use 'add' to achieve the same effect for department, employee in dep: dep_dd[department].add(employee) dep_dd #defaultdict(set, # {'Sales': {'John Doe', 'Martin Smith'}, # 'Accounting': {'Jane Doe'}, # 'Marketing': {'Adam Doe', 'Elizabeth Smith'}})  Application: Accumulating with defaultdict Finally, we\u0026rsquo;ll use defaultdict to accumulate values:\nincomes = [('Books', 1250.00), ('Books', 1300.00), ('Books', 1420.00), ('Tutorials', 560.00), ('Tutorials', 630.00), ('Tutorials', 750.00), ('Courses', 2500.00), ('Courses', 2430.00), ('Courses', 2750.00),] # enter float as argument dd = defaultdict(float) # collections.defaultdict # defaultdict(float, {'Books': 3970.0, 'Tutorials': 1940.0, 'Courses': 7680.0}) for product, income in incomes: dd[product] += income for product, income in dd.items(): print(f\u0026quot;Total income for {product}: ${income:,.2f}\u0026quot;) # Total income for Books: $3,970.00 # Total income for Tutorials: $1,940.00 # Total income for Courses: $7,680.00  I can see that defaultdict and dictionaries can be handy for grouping, counting and accumulating values in a column. We\u0026rsquo;ll come back to revisit these foundational concepts once the data science applications are clearer.\nIn summary, dictionaries and defaultdict can be used to group items, accumulate items and count items. Both can be used even when the key doesn\u0026rsquo;t (yet) exist, but its defaultdict handles this more succintly. For now, we\u0026rsquo;ll stop here and proceed to the next topic: counters.\nCounters Counter is a dict subclass for counting hashable objects (see doc). Back to our example in the previous section, we can use Counter instead of dict, specifically for counting:\nfrom collections import Counter # we can count the letters in this paragraph count_letters = Counter(\u0026quot;This table highlights 538's new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes minutes the table Wins NBA NBA RAPTOR more players\u0026quot;) # call count_letters count_letters # returns Counter({'T': 4, 'h': 19, 'i': 22, 's': 24, ' ': 61, 't': 29, 'a': 20, 'b': 5, 'l': 14, 'e': 35, 'g': 5, '5': 1, '3': 1, '8': 1, \u0026quot;'\u0026quot;: 1, 'n': 13, 'w': 3, 'N': 3, 'B': 3, 'A': 8, 'c': 3, ',': 6, 'R': 6, 'P': 4, 'O': 3, 'd': 7, 'o': 15, 'm': 8, 'r': 13, 'W': 4, 'v': 3, 'p': 8, '(': 2, ')': 2, '.': 2, 'x': 1, 'u': 3, 'y': 4, 'f': 3, '/': 1, '-': 2, 'k': 1, '1': 2, '0': 5})  Counter very easily did what defaultdict(int) did previously. We can even call the most_common method to get the most common letters:\n# get the thirteen most common letters for letter, count in count_letters.most_common(13): print(letter, count) # returns - 13 items 61 e 35 t 29 s 24 i 22 a 20 h 19 o 15 l 14 n 13 r 13 A 8 m 8  Sets We had a glimpse of set previously. There are two things the author emphasize with set. First, they\u0026rsquo;re faster than lists for checking membership:\nlines_list = [\u0026quot;This table highlights 538's new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes minutes the table Wins NBA NBA RAPTOR more players\u0026quot;] \u0026quot;zip\u0026quot; in lines_list # False, but have to check every element lines_set = set(lines_list) type(lines_set) # set \u0026quot;zip\u0026quot; in lines_set # Very fast to check  Because this was an arbitrary example, it\u0026rsquo;s not obvious that checking membership in set is faster than list so we\u0026rsquo;ll take the author\u0026rsquo;s word for it.\nThe second highlight for set is to find distinct items in a collection:\nnumber_list = [1,2,3,1,2,3] # list with six items item_set = set(number_list) # turn it into a set item_set # now has three items {1, 2, 3} turn_into_list = list(item_set) # turn into distinct item list  Controlflow I believe the main take away from this section is to briefly highlight the various control flows possible.\nHere\u0026rsquo;s a traditional if-else statement:\nx = 5 if x % 2 == 0: parity = \u0026quot;even\u0026quot; else: parity = \u0026quot;odd\u0026quot; parity # 'odd'  The author may, from time to time, opt to use a shorter ternary if-else one-liner, like so:\nparity = \u0026quot;even\u0026quot; if x % 2 == 0 else \u0026quot;odd\u0026quot;  The author points out that while while-loops exist:\nx = 0 while x \u0026lt; 10: print(f\u0026quot;{x} is less than 10\u0026quot;) x += 1  for and in will be used more often (the code below is both shorter and more readable):\nfor x in range(10): print(f\u0026quot;{x} is less than 10\u0026quot;)  We\u0026rsquo;ll also note that range(x) also goes up to x-1.\nFinally, more complex logic is possible, although we\u0026rsquo;ll have to revisit exactly when more complex logic is used in a data science context.\nfor x in range(10): if x == 3: continue if x == 5: break print(x)  Truthiness Booleans in Python, True and False, are only have the first letter capitalized. And Python uses None to indicate a nonexistent value. We\u0026rsquo;ll try to handle the exception below:\n1 \u0026lt; 2 # True (not TRUE) 1 \u0026gt; 2 # False (not FALSE) x = 1 try: assert x is None except AssertionError: print(\u0026quot;There was an AssertionError because x is not 'None'\u0026quot;)  A major takeaway for me is the concept of \u0026ldquo;truthy\u0026rdquo; and \u0026ldquo;falsy\u0026rdquo;. The first thing to note is that anything after if implies \u0026ldquo;is true\u0026rdquo; which is why if-statements can be used to check is a list, string or dictionary is empty:\nx = [1] y = [] # if x...is true # Truthy if x: print(\u0026quot;Truthy\u0026quot;) else: print(\u0026quot;Falsy\u0026quot;) # if y...is true # Falsy print(\u0026quot;Truthy\u0026quot;) if y else print(\u0026quot;Falsy\u0026quot;)  You\u0026rsquo;ll note the ternary version here is slightly less readable. Here are more examples to understand \u0026ldquo;truthiness\u0026rdquo;.\n## Truthy example # create a function that returns a string def some_func(): return \u0026quot;a string\u0026quot; # set s to some_func s = some_func() # use if-statement to check truthiness - returns 'a' if s: first_char = s[0] else: first_char = \u0026quot;\u0026quot; ## Falsy example # another function return empty string def another_func(): return \u0026quot;\u0026quot; # set another_func to y (falsy example) y = another_func() # when 'truthy' return second value, # when 'falsy' return first value first_character = y and y[0]  Finally, the author brings up all and any functions. The former returns True when every element is truthy; the latter returns True when at least one element is truthy:\nall([True, 1, {3}]) # True all([True, 1, {}]) # False any([True, 1, {}]) # True all([]) # True any([]) # False  You\u0026rsquo;ll note that the truthiness within the list is being evaluated. So all([]) suggests there are no \u0026lsquo;falsy\u0026rsquo; elements within the list, because it\u0026rsquo;s empty, so it evaluates to True.\nOn the other hand, any([]) suggests not even one (or at least one) element is \u0026lsquo;truthy\u0026rsquo;, because the list is empty, so it evaluates to False.\nSorting Sorting is generally straight forward with either sorted() or sort(). Here\u0026rsquo;s a more complex example:\n# create a list containing one paragraph lines = [\u0026quot;This table highlights 538's new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes minutes the table Wins NBA NBA RAPTOR more players\u0026quot;] # split paragraph into individual words lines = \u0026quot; \u0026quot;.join(lines_list).split() # import Counter from collections import Counter # count words in lines word_counts = Counter(lines) # sort words and count from largest to smallest wc = sorted(word_counts.items(), key=lambda x: x[1], # key line reverse=True)  Here\u0026rsquo;s another example involving coffee:\ncoffee_prices = { 'cappuccino': 54, 'latte': 56, 'espresso': 72, 'americano': 48, 'cortado': 41 } # .items() access dictionary key-value pairs # key is what the sorted() function will sort by # reverse indicates descending or ascending sorted(coffee_prices.items(), key=lambda x: x[1], reverse=False) # [('cortado', 41), # ('americano', 48), # ('cappuccino', 54), # ('latte', 56), # ('espresso', 72)]  list_comprehensions Previously, we saw if-statements expressed in one-line, for example:\ny = [] # Falsy print(\u0026quot;Truthy\u0026quot;) if y else print(\u0026quot;Falsy\u0026quot;)  We can also write for-loops in one-line. And thats a way to think about list comprehensions.\n# traditional for-loop num = [] for x in range(5): if x % 2 == 0: num.append(x) num # call num # list comprehension, provides the same thing [x for x in range(5) if x % 2 == 0]  Here are some examples from Data Science from Scratch:\n# [0, 2, 4] even_numbers = [x for x in range(5) if x % 2 == 0] # [0, 1, 4, 9, 16] squares = [x * x for x in range(5)] # [0, 4, 16] even_squares = [x * x for x in even_numbers]   Dan Bader provides a helpful way to conceptualizing list comprehensions:\n(values) = [ (expression) for (item) in (collections) ]  A good way to understand list comprehensions is to de-construct it back to a regular for-loop:\n# recreation of even_numbers even_bracket = [] for x in range(5): if x % 2 == 0: even_bracket.append(x) # recreation of squares square_bracket = [] for x in range(5): square_bracket.append(x * x) # recreate even_squares square_even_bracket = [] for x in even_bracket: square_even_bracket.append(x * x)  List comprehensions also allow for filtering with conditions.\n# traditional for-loop filtered_bracket = [] for x in range(10): if x \u0026gt; 5: filtered_bracket.append(x * x) # list comprehension filtered_comprehension = [x * x for x in range(10) if x \u0026gt; 5]  The key take-away here is that list comprehensions follow a pattern:\nvalues = [expression for item in collection if condition]  Python also supports dictionaries or sets comprehension, although we\u0026rsquo;ll have to revisit this post as to why we would want to do this in a data wrangling, transformation or analysis context.\n# {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} square_dict = {x: x * x for x in range(5)} # {1} square_set = {x * x for x in [1,-1]}  Finally, comprehensions can include nested for-loops:\npairs = [(x,y) for x in range(10) for y in range(10)]  We will expect to use list comprehensions often, so we\u0026rsquo;ll revisit this section as we see more applications in context.\nMap, Filter, Reduce, Partial In the first edition of this book the author introduced these functions, but has since reached enlightenment 🧘, he states:\n\u0026ldquo;On my journey toward enlightenment I have realized that these functions (i.e., map, filter, reduce, partial) are best avoided, and their uses in the book have been replaced with list comprehensions, for loops and other, more Pythonic constructs.\u0026rdquo; (p.36)\nHe\u0026rsquo;s being facetious, but I was intrigued anyways. So here\u0026rsquo;s an example replacing map with list comprehensions.\n# create list of names names = ['Russel', 'Kareem', 'Jordan', 'James'] # use map function to loop over names and apply an anonymous function greeted = map(lambda x: 'Hi ' + x, names) # map returns an iterator (see also lazy evaluation) print(greeted) # \u0026lt;map object at 0x7fc667c81f40\u0026gt; # because lazy evaluation, won't do anything unless iterate over it for name in greeted: print(name) #Hi Russel #Hi Kareem #Hi Jordan #Hi James ## List Comprehension way to do this operation greeted2 = ['Hi ' + name for name in names] # non-lazy evaluation (or eager) print(greeted2) # ['Hi Russel', 'Hi Kareem', 'Hi Jordan', 'Hi James']  Here\u0026rsquo;s another example replacing filter with list comprehensions:\n# create list of integers numbers = [13, 4, 18, 35] # filter creates an interator div_by_5 = filter(lambda num: num % 5 == 0, numbers) print(div_by_5) # \u0026lt;filter object at 0x7fc667c9ad30\u0026gt; print(list(div_by_5)) # must convert iterator into a list - [35] # using list comprehension to achieve the same thing another_div_by_5 = [num for num in numbers if num % 5 == 0] # lists do not use lazy evaluation, so it will print out immediately print(another_div_by_5) # [35]  Assert Automated Testing and Assert One of the many cool things about Data Science from Scratch (by Joel Grus) is his use of assertions as a way to \u0026ldquo;test\u0026rdquo; code. This is a software engineering practice (see test-driven development) that may not be as pervasive in data science, but I suspect, will see growth in usage and will soon become best practice, if we\u0026rsquo;re not already there.\nWhile there are testing frameworks that deserve their own chapters, throughout this book, fortunately the author has provided a simple way to test by way of the assert key word, here\u0026rsquo;s an example:\n# create function to return the largest value in a list def largest_item(x): return max(x) # assert that our function is working properly # we will see 'nothing' if things are working properly assert largest_item([10, 20, 5, 40, 99]) == 99 # an AssertionError will pop up if any other value is used assert largest_item([10, 20, 5, 40, 99]) == 40 --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) \u0026lt;ipython-input-21-12dc291d091e\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 assert largest_item([10, 20, 5, 40, 99]) == 40 # we can also create an assertion for input values def largest_item(x): assert x, \u0026quot;empty list has no largest value\u0026quot; return max(x)  Object-Oriented_Programming Object-oriented programming could be it\u0026rsquo;s own chapter, so we won\u0026rsquo;t try to shoot for comprehensiveness here. Instead, we\u0026rsquo;ll try to understand it\u0026rsquo;s basics and the assert function is going to help us understand it even better.\nObject-oriented programming could be it\u0026rsquo;s own chapter, so we\u0026rsquo;ll go over a toy example from the book and tie it to the previous section on assert.\nFirst, we\u0026rsquo;ll create a class CountingClicker that initializes at count 0, has several methods including a click method to increment the count, a read method to read the present number of count and a reset method to reset the count back to 0.\nThen we\u0026rsquo;ll write some assert statements to test that our class method is working as intended.\nYou\u0026rsquo;ll note that there are private methods and public methods. Private methods have the double underscore (aka dunder methods), they\u0026rsquo;re generally not called, but python won\u0026rsquo;t stop you. Then we have the more familiar public methods. Also, all the methods have to be written within the scope of the class CountingClicker.\nclass CountingClicker: \u0026quot;\u0026quot;\u0026quot;A class can/should have a docstring, just like a function\u0026quot;\u0026quot;\u0026quot; def __init__(self, count = 0): self.count = count def __repr__(self): return f\u0026quot;CountingClicker(count = {self.count})\u0026quot; def click(self, num_times = 1): \u0026quot;\u0026quot;\u0026quot;Click the clicker some number of times.\u0026quot;\u0026quot;\u0026quot; self.count += num_times def read(self): return self.count def reset(self): self.count = 0  After we\u0026rsquo;ve written the class and associated methods, we can write assert statements to test them. You\u0026rsquo;ll want to write the below statements in this order because we\u0026rsquo;re testing the behavior of our CountingClicker class.\nclicker = CountingClicker() assert clicker.read() == 0, \u0026quot;clicker should start with count 0\u0026quot; clicker.click() clicker.click() assert clicker.read() == 2, \u0026quot;after two clicks, clicker should have count of 2\u0026quot; clicker.reset() assert clicker.read() == 0, \u0026quot;after reset, clicker should be back to 0\u0026quot;  In summary, we created a class CountingClicker whose methods allow it to display in text (__repr__), click, read and reset.\nAll these methods belong to the class CountingClicker and will be passed along to new instances of classes - we have yet to see what this will look like as it relates to tasks in data science so we\u0026rsquo;ll revisit this post when we have updates on the applied end.\nIterables_and_Generators A Brief Forey into Lazy Evaluation A key concept that is introduced when discussing the creation of \u0026ldquo;generators\u0026rdquo; is using for and in to iterate over generators (like lists), but lazily on demand. This is formally called lazy evaluation or \u0026lsquo;call-by-need\u0026rsquo; which delays the evaluation of an expression until the value is needed. We can think of this as a form of optimization - avoiding repeating function calls when not needed.\nHere\u0026rsquo;s a graphic borrowed from Xiaoxu Gao, check out her post here:\nWe\u0026rsquo;ll create some generators (customized function/class), but bear in mind that it will be redundant with range(), both of which illustrate lazy evaluation.\n# Example 1: create natural_numbers() function that incrementally counts numbers def natural_numbers(): \u0026quot;\u0026quot;\u0026quot;returns 1, 2, 3, ...\u0026quot;\u0026quot;\u0026quot; n = 1 while True: yield n n += 1 # check it's type type(natural_numbers()) # generator # call it, you get: \u0026lt;generator object natural_numbers at 0x7fb4d787b2e0\u0026gt; natural_numbers() # the point of lazy evaluation is that it won't do anything # until you iterate over it (but avoid infinite loop with logic breaks) for i in natural_numbers(): print(i) if i == 37: break print(\u0026quot;exit loop\u0026quot;) # result 1...37 exit loop  Here\u0026rsquo;s another example using range, a built-in python function that also uses lazy evaluation. Even when you call this generator, it won\u0026rsquo;t do anything until you iterate over it.\nevens_below_30 = (i for i in range(30) if i % 2 == 0) # check its type - generator type(evens_below_30) # call it, you get: \u0026lt;generator object \u0026lt;genexpr\u0026gt; at 0x7fb4d70ef580\u0026gt; # calling it does nothing evens_below_30 # now iterate over it with for and in - now it does something # prints: 0, 2, 4, 6 ... 28 for i in evens_below_30: print(i)  Finally, this section brings up another important key word enumerate for when we want to iterate over a generator or list and get both values and indices:\n# create list of names names = ['Alice', 'Lebron', 'Kobe', 'Bob', 'Charles', 'Shaq', 'Kenny'] # Pythonic way for i, name in enumerate(names): print(f\u0026quot;index: {i}, name: {name}\u0026quot;) # NOT pythonic for i in range(len(names)): print(f\u0026quot;index: {i}, name: {names[i]}\u0026quot;) # Also NOT pythonic i = 0 for name in names: print(f\u0026quot;index {i} is {names[i]}\u0026quot;) i += 1  In my view, the pythonic way is much more readable here.\nPseudorandomness The random module is used extensively in data science. Particularly when random numbers need to be generated and we want reproducible results the next time we run our model (in Python its random.seed(x), in R its set.seed(x)), where x is any integer we decide (we just need to be consistent when we revisit our model).\nTechnically, the module produces deterministic results, hence it\u0026rsquo;s pseudorandom, here\u0026rsquo;s an example to highlight how the randomness is deterministic:\nimport random random.seed(10) # say we use 10 # this variable is from the book four_randoms = [random.random() for _ in range(4)] # call four_randoms - same result from Data Science from Scratch # because the book also uses random.seed(10) [0.5714025946899135, 0.4288890546751146, 0.5780913011344704, 0.20609823213950174] # if we use x instead of underscore # a different set of four \u0026quot;random\u0026quot; numbers is generated another_four_randoms = [random.random() for x in range(4)] [0.81332125135732, 0.8235888725334455, 0.6534725339011758, 0.16022955651881965]  Brief detour into _ Reading around from other sources suggests that the underscore \u0026ldquo;_\u0026rdquo; is used in a for loop when we don\u0026rsquo;t care about the variable (its a throwaway) and have no plans to use it, for example:\n# prints 'hello' five times for _ in range(5): print(\u0026quot;hello\u0026quot;) # we could use x as well for x in range(5): print(\u0026quot;hello\u0026quot;)  In the above example, either _ or x could have been used and there doesn\u0026rsquo;t seem to be much difference. We could technically call _, but its considered bad practice:\n# bad practice, but prints 0, 1, 2, 3, 4 for _ in range(5): print(_)  Nevertheless, _ matters in the context of pseudorandomness because it yields a different result:\nimport random random.seed(10) # these two yield different results, even with the same random.seed(10) four_randoms = [random.random() for _ in range(4)] another_four_randoms = [random.random() for x in range(4)]  But back to determinism, or pseudorandomness, we need to change the random.seed(11), then back to random.seed(10) to see this play out:\n# new random.seed() random.seed(11) # reset four_randoms four_randoms = [random.random() for _ in range(4)] [0.4523795535098186, 0.559772386080496, 0.9242105840237294, 0.4656500700997733] # change to previous random.seed() random.seed(10) # reset four_randoms (again) four_randoms = [random.random() for _ in range(4)] # get previous result (see above) [0.5714025946899135, 0.4288890546751146, 0.5780913011344704, 0.20609823213950174]  Other features of the random module include: random.randrange, random.shuffle, random.choice and random.sample:\nrandom.randrange(3,6) # choose randomly between [3,4,5] # random shuffle one_to_ten = [1,2,3,4,5,6,7,8,9,10] random.shuffle(one_to_ten) print(one_to_ten) # example: [8, 7, 9, 3, 5, 2, 10, 1, 6, 4] random.shuffle(one_to_ten) # again print(one_to_ten) # example: [3, 10, 8, 6, 9, 2, 7, 1, 4, 5] # random choice list_of_people = ([\u0026quot;Bush\u0026quot;, \u0026quot;Clinton\u0026quot;, \u0026quot;Obama\u0026quot;, \u0026quot;Biden\u0026quot;, \u0026quot;Trump\u0026quot;]) random.choice(list_of_people) # first time, 'Clinton' random.choice(list_of_people) # second time, 'Biden' # random sample lottery_numbers = range(60) # get a range of 60 numbers winning_numbers = random.sample(lottery_numbers, 6) # get a random sample of 6 numbers winning_numbers # example: [39, 24, 2, 37, 0, 15] # because its pseudorandom, if you want a different set of 6 numbers # reset the winning_numbers winning_numbers = random.sample(lottery_numbers, 6) winning_numbers # a different set of numbers [8, 12, 19, 34, 23, 49]  Regex Regular Expressions Whole books can be written about regular expressions so the author briefly highlights a couple features that may come in handy, re.match, re.search, re.split and re.sub:\nimport re re_examples = [ not re.match(\u0026quot;a\u0026quot;, \u0026quot;cat\u0026quot;), # re.match check the word cat 'starts' letter 'a' re.search(\u0026quot;a\u0026quot;, \u0026quot;cat\u0026quot;), # re.search check if word cat 'contains' letter 'a' not re.search(\u0026quot;c\u0026quot;, \u0026quot;dog\u0026quot;), # 'dog' does not contain 'c' 3 == len(re.split(\u0026quot;[ab]\u0026quot;, \u0026quot;carbs\u0026quot;)), # 3 equals length of \u0026quot;carbs\u0026quot; once you split out [ab] \u0026quot;R-D-\u0026quot; == re.sub(\u0026quot;[0-9]\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;R2D2\u0026quot;) # sub out numbers in 'R2D2' with hyphen \u0026quot;-\u0026quot; ] # test that all examples are true assert all(re_examples), \u0026quot;all the regex examples should be True\u0026quot;  The final line reviews our understanding of testing (assert) and truthiness (all) applied to our regular expression examples, pretty neat.\nFunctional_Programming see List Comprehensions\nzip and Argument Unpacking The author states, \u0026ldquo;it is rare that we\u0026rsquo;ll find this useful.\u0026rdquo; (p.37) So we\u0026rsquo;ll circle back if it comes up.\nargs and kwargs The authors states, \u0026ldquo;it is more correct and readable if you\u0026rsquo;re explicit about what sorts of arguments your functions require; accordingly, we will use args and kwargs only when we have no other option.\u0026rdquo; (p. 38) So we\u0026rsquo;ll circle back if it comes up.\nType Annotations How to Write Type Annotations ","date":1603411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603929600,"objectID":"fe52d559d8278a6cef401ac4e9c1f24f","permalink":"/post/dsfs_2/","publishdate":"2020-10-23T00:00:00Z","relpermalink":"/post/dsfs_2/","section":"post","summary":"Survey of python features relevant for Data Science","tags":["Python","Data Science"],"title":"Data Science from Scratch (ch2)","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Preparing API endpoints in Python with Flask In this post, we\u0026rsquo;ll create a minimal API endpoint that allows users to make request to calculate the area of a rectangle. The following code sets up an API endpoint locally. We\u0026rsquo;ll import Flask, a lightweight web application framework and CORS (cross-origin resource sharing) which allows for various HTTP requests.\nWe have two endpoints, one basic \u0026ldquo;hello world\u0026rdquo; and the other calculate the area (i.e., width x height).\nThis is saved in App.py. The command to run this file is $ python3 App.py. The last line ensures the API is running locally on localhost:5000.\nfrom flask import Flask, request from flask_cors import CORS, cross_origin import joblib import numpy as np app = Flask(__name__) CORS(app) @app.route('/') def helloworld(): return 'Helloworld' # Example request: http://localhost:5000/area?w=50\u0026amp;h=3 @app.route('/area', methods=['GET']) @cross_origin() def area(): w = float(request.values['w']) h = float(request.values['h']) return str(w * h) if __name__ == '__main__': app.run(host='0.0.0.0', port=5000, debug=True)  You can just run localhost:5000 and get Helloworld or make a request to get the area, for example: http://localhost:5000/area?w=20\u0026amp;h=33 (this yeilds 660)\nTraining a Logistic Regression classification model After setting up some API endpoints, it\u0026rsquo;s time to create a basic machine learning model. We\u0026rsquo;ll create a logistic regression model to classify flowers from the Iris dataset. This will be created in one jupyter notebook.\nWe\u0026rsquo;ll load all required libraries.\nfrom sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import joblib import numpy as np import pandas as pd  Then, we\u0026rsquo;ll load the Iris dataset that comes with scikit learn, sklearn.\niris = load_iris() # assign two variables at once X, y = iris['data'], iris['target']  We\u0026rsquo;ll reshape the data using numpy, then split the data into training and validation sets.\n# reshape data for logistic regression dataset = np.hstack((X, y.reshape(-1,1))) np.random.shuffle(dataset) # split data into training, validation sets X_train, X_test, y_train, y_test = train_test_split(dataset[:, :4], dataset[:, 4], test_size=0.2)  We\u0026rsquo;ll then fit a logistic regression model by fitting the training set to the validation set.\nmodel = LogisticRegression() model.fit(X_train, y_train)  Then, we\u0026rsquo;ll use the model to predict on the validation data (note: in a real project a distinction is made between validation and testing sets, but we\u0026rsquo;ll blur that distinction for this demo). You can also test the model to make a prediction on a single observation.\nIt\u0026rsquo;s also a good idea to get the accuracy_score(), although it may not be ideal for classification models.\n# make a prediction y_pred = model.predict(X_test) # get accuracy score accuracy_score(y_test, y_pred) # make prediction on single Iris obervation model.predict([[5.1, 3.5, 1.4, 0.2]])  Finally, we need to use joblib to save an iris.model to our directory, this will be used to connect to the API.\njoblib.dump(model, 'iris.model')  Creating an API endpoint for the Logistic Regression model Back in the App.py file, we\u0026rsquo;ll add this section to create an endpoint, the predict_species() function that loads the iris.model, then sends a Post request of the four parameter values from iris['data']. The predict_species() function will then return one of three flower species.\n@app.route('/iris', methods=['POST']) @cross_origin() def predict_species(): model = joblib.load('iris.model') #needs to be the correct path req = request.values['param'] inputs = np.array(req.split(','), dtype=np.float32).reshape(1,-1) predict_target = model.predict(inputs) if predict_target == 0: return 'Setosa' elif predict_target == 1: return 'Versicolor' else: return 'Virginica'  Testing the API endpoint on Postman Finally, we\u0026rsquo;ll use Postman, a platform for API development. We will post four parameters (i.e., sepal length, sepal width, petal length and petal width) to the API endpoint and expect to receive a name back, either Setosa, Versicolor or Virginica. In Postman, we\u0026rsquo;ll create a new collection and a new request:\nThe next step from here is to go beyond localhost and deploy the model. We\u0026rsquo;ll explore that in another post.\n","date":1602288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602288000,"objectID":"3e324bdd4365e43d8e215c6ff3eaacf1","permalink":"/post/mlaas/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/post/mlaas/","section":"post","summary":"Connecting models to an API","tags":["Machine Learning"],"title":"Machine learning as a service","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"Data Science Literature Review I saw an intriguing question posed on Twitter and some of the responses were illuminating.\nHere\u0026rsquo;s another variant of the question:\nAlthough Data Science has a long history, it\u0026rsquo;s considered a relatively young field.\nThis space will be used to document recommended reading for new entrants:\n  Downey, Allen (2016) There is only one test. source\n  Wickham, Hadley (2014) Tidy Data. The Journal of Statistical Software, vol 59. original, update\n  James, G., Witten, D., Hastie, T. \u0026amp; Tibshirani, R. (2014) An Introduction to Statistical Learning with Applications in R. source\n  Shmueli, G. (2010) To explain or to predict? Statistical Science, 25(3), 289-310. source\n  Hernan, M.A., Hsu, J. \u0026amp; Healy, B. (2019) A second chance to get causal inference right: A classification of Data Science tasks. Chance, vol 32(1). source\n  Gelman, A., Pasarica, C. \u0026amp; Dodhia, R. (2002) Let\u0026rsquo;s practice what we preach: Turning tables into graphs. The American Statistician, vol 56(2). source\n  Scott Formann-Roe (June, 2012) Understanding the Bias-Variance Tradeoff. source\n  Donoho, D (2017) 50 Years of Data Science. Journal of Computational and Graphical Statistics, vol 26(4). source\n  Wilson, G., Bryan, J., Cranston, K., Kitzes, J., Nederbragt, L. \u0026amp; Teal, T.K. (2017) Good enough practices in scientific computing. Plos Computational Biology. source\n  Kevin Markham (2019) 100 pandas tricks to save you time and energy. source\n  Chris Albon\u0026rsquo;s code snippets. source\n  Howard, J. \u0026amp; Gugger, S. (Aug 4, 2020) Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD 1st Ed. source\n  Brandon Rohrer (Jan, 2020) End-to-End Machine Learning: Complete Course Catalog. source; second source\n  John Rauser (Dec, 2016) How Humans See Data youtube\n  Broman, K.W. \u0026amp; Woo, K.H. (2018) Data Organization in Spreadsheets. The American Statistician, vol 72(1). source\n  Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaudhary, V., \u0026amp; Young, M. (2014) Machine Learning: The High Interest Credit Card of Technical Debt. source\n  3Blue1Brown for Linear Algebra youtube\n  Jenny Bryan. Stat 545: Data Wrangling, Exploration and Analysis with R. source\n  ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"dd5da896834a7a9098bf7fba0cee62e1","permalink":"/post/data_science_canon/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/post/data_science_canon/","section":"post","summary":"What is Data Science canon?","tags":["Data Science"],"title":"Essential Readings in Data Science","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"TidyTuesday 2020-08-18 (week 34) In the process of exploring dendrograms, I create jheri curls :)\nAnother plot with less hair:\nI call this Disco Fire:\nTidyTuesday 2020-10-13 (week 42) Dino-turn-Rorschach test\nTidyTuesday 2020-12-08 (week 50) Sunburst (BBC\u0026rsquo;s 100 Influential Women 2020)\nCircular Packing (experimental)\nOops, I did it again:\nThis one is inspired by Georgios Karamanis' #genuary submission using the {ggridges} package.\nThis is my daughter Milin, still lovely to me :)\n","date":1599091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610668800,"objectID":"6261ecf04eb95d1221396c4abdf09c2e","permalink":"/post/ggplot_art/","publishdate":"2020-09-03T00:00:00Z","relpermalink":"/post/ggplot_art/","section":"post","summary":"TidyTuesday Outtakes","tags":["Data Viz","RStats","Tidyverse"],"title":"A collection of weird pretty plots","type":"post"},{"authors":[],"categories":[],"content":" Context In web development, functions are everywhere and are written to get even the smallest tasks done like allowing users to click on a button or controlling where and how a pop-up modal appears. In data analysis, you can go without using functions as long as you’re working on small scale projects and do not need to share your code with others.\nMoreover, they can make your life a lot easier if you want to avoid copying and pasting your code in a bunch of different places (it also makes your code less error prone and easier to update).\nFunctions may require a slight perspective shift for those who aren’t familiar. In this post, I want to share how I snuck functions into my workflow for a specific project.\n Slipping Custom Functions into the Workflow The most intuitive way, in my opinion, to introduce functions is to take a certain data pre-processing sequence and turn it into a function. Below, I have a newly created dataframe called net_sales_year_month that is a dataframe with three columns (net_sales, Year, Month).\nSuppose my objective is to add a Day and month_year column, that combines Year, Month and Day (yyyy-mm-dd) into a date type. The pre-processing task would be to take net_sales_year_month and use the mutate function to create some new columns.\nThis is fine and well if you’re doing this one time, but what if you need to repeat this operation on multiple columns?\nThat’s where a custom function comes in.\nFor example, the function below called create_ymd_function simply replaces net_sales_year_month with a generic data, serving as the function parameter. Now any dataframe can be used as a parameter for the create_ymd_function.\nNote the BEFORE and AFTER sections below - they have the same output, but one is a more general function that can be used with other data frames.\n# Selecting columns to work with (net_sales) net_sales_year_month \u0026lt;- retail_sales2 %\u0026gt;% select(`Net Sales`, Year, Month) %\u0026gt;% rename(net_sales = `Net Sales`) # BEFORE net_sales_year_month %\u0026gt;% mutate( Day = 1, month_year = paste(Year, Month, Day), month_year = month_year %\u0026gt;% ymd(), month = month(month_year) ) ## # A tibble: 36 x 6 ## net_sales Year Month Day month_year month ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 8284. 2017 January 1 2017-01-01 1 ## 2 6388. 2017 February 1 2017-02-01 2 ## 3 4589. 2017 March 1 2017-03-01 3 ## 4 8533. 2017 April 1 2017-04-01 4 ## 5 6237. 2017 May 1 2017-05-01 5 ## 6 9370. 2017 June 1 2017-06-01 6 ## 7 5959. 2017 July 1 2017-07-01 7 ## 8 7740. 2017 August 1 2017-08-01 8 ## 9 6732. 2017 September 1 2017-09-01 9 ## 10 5327 2017 October 1 2017-10-01 10 ## # … with 26 more rows # AFTER # Function takes in dataframe to add columns for further analysis create_ymd_function \u0026lt;- function(data) { data %\u0026gt;% mutate( Day = 1, month_year = paste(Year, Month, Day), month_year = month_year %\u0026gt;% ymd(), month = month(month_year) ) } create_ymd_function(net_sales_year_month) ## # A tibble: 36 x 6 ## net_sales Year Month Day month_year month ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 8284. 2017 January 1 2017-01-01 1 ## 2 6388. 2017 February 1 2017-02-01 2 ## 3 4589. 2017 March 1 2017-03-01 3 ## 4 8533. 2017 April 1 2017-04-01 4 ## 5 6237. 2017 May 1 2017-05-01 5 ## 6 9370. 2017 June 1 2017-06-01 6 ## 7 5959. 2017 July 1 2017-07-01 7 ## 8 7740. 2017 August 1 2017-08-01 8 ## 9 6732. 2017 September 1 2017-09-01 9 ## 10 5327 2017 October 1 2017-10-01 10 ## # … with 26 more rows net_sales_year_month_2 \u0026lt;- create_ymd_function(net_sales_year_month)  Generalizing Functions Here’s another example of moving from specific to general functions.\nWith the create_line_chart function, i’m taking in a dataframe, piping into ggplot and visualizing a simple line graph with geom_line. You’ll note it is specific because it requires the dataframe to have a column named net_sales in order to work.\nBut what if I wanted to repeat this operation with total_orders or total_sales or some other metric?\nRight below, I create a more general function, create_line_chart_general that takes in any dataset and two columns as the function parameter.\nThis makes the function much more re-usable. However, it also introduces some R-specific commands like enquo() and !! to quote and unquote parameters for use in the function. We are entering lazy evaluation territory, which I’ll save for another post!\n# BEFORE: # This function only works for net_sales # It\u0026#39;s easy to just slip \u0026#39;data\u0026#39; as an argument # But the aesthetic mapping is done only one a specific column create_line_chart \u0026lt;- function(data){ data %\u0026gt;% ggplot(aes(x = month_year, y = net_sales)) + geom_line() } # AFTER: # This is a more generalizable function using enquo() and \u0026#39;!!\u0026#39; # note columns as function parameters create_line_chart_general \u0026lt;- function(dataset, col_name_1, col_name_2){ col_name_1 \u0026lt;- enquo(col_name_1) col_name_2 \u0026lt;- enquo(col_name_2) dataset %\u0026gt;% ggplot(aes(x = !!(col_name_1), y = !!(col_name_2))) + geom_line() } # Call the function with data and necessary parameters create_line_chart_general(net_sales_year_month_2, month_year, net_sales)  More Generalized Function This next function is slightly more complicated as it involves creating several more columns. But it can still be generalized using the tools discussed above.\ncreate_bpc_columns_general \u0026lt;- function(dataset, col_name){ col_name \u0026lt;- enquo(col_name) bpc_data \u0026lt;- dataset %\u0026gt;% mutate( avg_orders = mean(!!(col_name)), # calculate lagging difference moving_range = diff(as.zoo(!!(col_name)), na.pad=TRUE), # get absolute value moving_range = abs(moving_range), # change NA to 0 moving_range = ifelse(row_number()==1, 0, moving_range), avg_moving_range = mean(moving_range), lnpl = avg_orders - (2.66*avg_moving_range), lower_25 = avg_orders - (1.33*avg_moving_range), upper_25 = avg_orders + (1.33*avg_moving_range), unpl = avg_orders + (2.66*avg_moving_range) ) return(bpc_data) } create_bpc_columns_general(net_sales_year_month_2, net_sales) ## # A tibble: 36 x 13 ## net_sales Year Month Day month_year month avg_orders moving_range ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 8284. 2017 Janu… 1 2017-01-01 1 9058. 0 ## 2 6388. 2017 Febr… 1 2017-02-01 2 9058. 1896. ## 3 4589. 2017 March 1 2017-03-01 3 9058. 1798. ## 4 8533. 2017 April 1 2017-04-01 4 9058. 3944. ## 5 6237. 2017 May 1 2017-05-01 5 9058. 2295. ## 6 9370. 2017 June 1 2017-06-01 6 9058. 3132. ## 7 5959. 2017 July 1 2017-07-01 7 9058. 3410. ## 8 7740. 2017 Augu… 1 2017-08-01 8 9058. 1781. ## 9 6732. 2017 Sept… 1 2017-09-01 9 9058. 1008. ## 10 5327 2017 Octo… 1 2017-10-01 10 9058. 1405. ## # … with 26 more rows, and 5 more variables: avg_moving_range \u0026lt;dbl\u0026gt;, ## # lnpl \u0026lt;dbl\u0026gt;, lower_25 \u0026lt;dbl\u0026gt;, upper_25 \u0026lt;dbl\u0026gt;, unpl \u0026lt;dbl\u0026gt; net_sales_bpc_data \u0026lt;- create_bpc_columns_general(net_sales_year_month_2, net_sales)  Generalized Functions for Visualization This was the trickiest to convert into a general function and I’m still on the fence as to whether this is generalizable. In one sense, it is generalizable as I tested this create_bpc_visualization_general function on another column aside from net_sales, but it did require that I know that other columns in the dataset are: avg_orders, unpl, lnpl, upper_25 and lower_25.\nI have more exploring to do around quoting and unquoting enquo(), quos() for various ggplot geometries like geom_hline. Will report back with another post once I get those details down.\ncreate_bpc_visualization_general \u0026lt;- function(dataset, col_x, col_y, col_avg, col_unpl, col_lnpl, col_upper_25, col_lower_25){ col_x \u0026lt;- enquo(col_x) # month_year col_y \u0026lt;- enquo(col_y) # net_sales col_avg \u0026lt;- dataset$avg_orders col_unpl \u0026lt;- dataset$unpl col_lnpl \u0026lt;- dataset$lnpl col_upper_25 \u0026lt;- dataset$upper_25 col_lower_25 \u0026lt;- dataset$lower_25 dataset %\u0026gt;% ggplot(aes(x = !!(col_x), y = !!(col_y))) + geom_line() + geom_hline(yintercept = col_avg, color = \u0026#39;green\u0026#39;) + geom_hline(yintercept = col_unpl, color = \u0026#39;red\u0026#39;, linetype = \u0026#39;dashed\u0026#39;) + geom_hline(yintercept = col_lnpl, color = \u0026#39;red\u0026#39;, linetype = \u0026#39;dashed\u0026#39;) + geom_hline(yintercept = col_upper_25, color = \u0026#39;orange\u0026#39;) + geom_hline(yintercept = col_lower_25, color = \u0026#39;orange\u0026#39;) + # break x-axis into quarters scale_x_date(breaks = \u0026#39;3 month\u0026#39;) + # note: place before theme() theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs( title = glue(\u0026#39;{names(dataset[,1])}: Business Process Chart\u0026#39;), subtitle = \u0026quot;2017 - 2019\u0026quot;, x = \u0026quot;\u0026quot;, y = glue(\u0026#39;{names(dataset[,1])}\u0026#39;), caption = \u0026quot;----\u0026quot; ) + annotate(\u0026quot;text\u0026quot;, x = as.Date(\u0026quot;2017-02-01\u0026quot;), y = col_unpl, color = \u0026#39;red\u0026#39;, label = \u0026quot;UNLP\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = as.Date(\u0026quot;2017-02-01\u0026quot;), y = col_lnpl, color = \u0026#39;red\u0026#39;, label = \u0026quot;LNLP\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = as.Date(\u0026quot;2017-02-01\u0026quot;), y = col_upper_25, color = \u0026#39;orange\u0026#39;, label = \u0026quot;Upper 25%\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = as.Date(\u0026quot;2017-02-01\u0026quot;), y = col_avg, color = \u0026#39;green\u0026#39;, label = \u0026quot;Avg = 97\u0026quot;) } create_bpc_visualization_general(net_sales_bpc_data, month_year, net_sales, avg_orders, unpl, lnpl, upper_25, lower_25)  Summary It’s possible to do a fair amount of data analysis without using functions, but functions help you avoid endless copying and pasting and make your code less error prone.\nThere are many different types functions you could use. In this post, I share functions that take columns of data as arguments. These types of functions are well-suited for streamlining your data pre-processing and visualization tasks.\nShout out to Bruno Rodrigues for writing Modern R with the Tidyverse which has helped me get my head around writing custom functions.\n ","date":1596067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593091452,"objectID":"5e255caaa66f013d921626e5335a4768","permalink":"/post/custom-functions/","publishdate":"2020-07-30T00:00:00Z","relpermalink":"/post/custom-functions/","section":"post","summary":"Writing custom functions to automate data manipulation and visualization tasks","tags":[],"title":"Introducing Custom Functions into the Workflow","type":"post"},{"authors":[],"categories":[],"content":" Data Wrangling I’ve had experiencing using several of the functions in this portion of #DS4B 101, like select(), filter(), distinct(), group_by(), summarize() and so on, but this course is making me realize i’ve only skimmed the surface of dplyr.\nRecommended for anyone learning R who wants to level-up.\n Select \u0026amp; Arrange I’ve used the select function before, but I haven’t used it, really.\nI had previously selected by column name, never by numeric vector; and I had never used select_helpers (starts_with, contains, matches, num_range). I had never re-arranged columns using select(). Select_if() also comes in handy as well. Knowing data types allow for efficient selection (integer vs double).\nI’ve used arrange() and desc(), so I don’t go into too much detail here.\nlibrary(tidyverse) ## ── Attaching packages ──────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.1 ✓ purrr 0.3.3 ## ✓ tibble 3.0.1 ✓ dplyr 0.8.5 ## ✓ tidyr 1.0.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() # select first three columns using numeric vector USArrests %\u0026gt;% select(1:3) # re-arrange columns using select; place UrbanPop first USArrests %\u0026gt;% select(UrbanPop, everything()) # select if integer USArrests %\u0026gt;% select_if(is.integer) USArrests %\u0026gt;% select_if(~ !is.double(.)) # select if double USArrests %\u0026gt;% select_if(is.double) USArrests %\u0026gt;% select_if(~ !is.integer(.)) # swithing to IRIS dataframe # select helpers: starts_with, contains iris %\u0026gt;% select(starts_with(\u0026quot;petal\u0026quot;)) iris %\u0026gt;% select(contains(\u0026quot;length\u0026quot;)) %\u0026gt;% head() # using pull() instead of select() to pull out contents of a column iris %\u0026gt;% pull(Sepal.Width) %\u0026gt;% head() iris %\u0026gt;% select_if(is.factor) %\u0026gt;% head() # arrange Species in descending order by Petal.Length iris %\u0026gt;% select(Petal.Length, Species) %\u0026gt;% arrange(desc(Petal.Length))  Slice: Highlighting a Distinction between Base and Tidyverse Using slice() to grab portions of rows is another function I had not used prior. I might have used base R to grab specific rows. But looking back all my Base R operations were one-offs. Below is an example, I may use Base-R to grab the first five rows of a dataframe. If I wanted to sort by a particular column, then grab the first give rows, it turns into two operations. First sorting by column, decreasing and assigning to ‘x’, then grabing first five rows of ‘x’.\nWith dplyr, particularly patterns taught in the class, you can more seamlessly arrange, in descending order by a particular column (Solar.R), then slice the first five rows.\n# tidyverse to grab first five rows with slice(), but this allows pre-arranging it first. airquality %\u0026gt;% arrange(desc(Solar.R)) %\u0026gt;% slice(1:5) # base R to grab first five rows airquality[1:5,] # base R to first sort by Solar.R, then assign to variable \u0026#39;x\u0026#39;, then grab first five values x \u0026lt;- sort(airquality$Solar.R, decreasing = TRUE) x[1:5] # fastest way to grab Solar.R sort(airquality$Solar.R, decreasing = TRUE)[1:5]  Spread and Gather Here demonstrating gather() and spread() using the built-in iris dataset. The\n# gather (before spread) iris %\u0026gt;% # create unique ID for each row mutate(ID=row_number(Species)) %\u0026gt;% # put ID in first column select(ID, everything()) %\u0026gt;% # use values from columns 2-5 as value gather(key = Measure_Type, value = Values,2:5) %\u0026gt;% # can spread by either Measure_Type or Species spread(key = Species, value = Values)  Filter and Mutate with nested pipes Using both airquality and USArrests to demonstrate various data wrangling operations involving filter and mutate with nested pipes. I can’t believe I’ve only filtered one column at a time.\n# filter airquality for beginning and end of the month airquality %\u0026gt;% filter(Day \u0026lt; 5 | Day \u0026gt; 25) airquality %\u0026gt;% filter(Temp \u0026lt; 71 \u0026amp; Temp \u0026gt; 64) # Filter states that start with a certain alphabet USArrests # convert row name to column USArrest_rownames \u0026lt;- tibble::rownames_to_column(USArrests, \u0026quot;States\u0026quot;) # Descriptive statistics of Violence_Type by all States that start with \u0026quot;New\u0026quot; USArrest_rownames %\u0026gt;% select(States, UrbanPop, everything()) %\u0026gt;% filter(States %\u0026gt;% str_detect(\u0026quot;New\u0026quot;)) %\u0026gt;% # good to define which columns will be gather() in new column gather(`Murder`, `Assault`, `Rape`, key = \u0026quot;Violence_Type\u0026quot;, value = \u0026quot;cases\u0026quot; ) %\u0026gt;% group_by(Violence_Type) %\u0026gt;% summarize( avg = mean(cases), min = min(cases), max = max(cases) ) %\u0026gt;% ungroup() %\u0026gt;% mutate(avg = round(avg,1)) # Filter by states starting with \u0026quot;A\u0026quot; and murder higher than 10 USArrest_rownames %\u0026gt;% filter(Murder \u0026gt; 10, States %\u0026gt;% str_detect(\u0026quot;A\u0026quot;)) # filter for all states beginning with \u0026quot;New\u0026quot; USArrest_rownames %\u0026gt;% filter(States %\u0026gt;% str_detect(\u0026quot;New\u0026quot;)) %\u0026gt;% arrange(desc(Assault)) USArrest_rownames %\u0026gt;% filter(Murder \u0026gt; 5, States %\u0026gt;% str_detect(\u0026quot;M\u0026quot;)) %\u0026gt;% arrange(desc(Murder))  Handling Missing Values: Replace NA summarize_all() and replace_na() are a joy to use for handling missing values.\n# Quickly get number of \u0026quot;missing values\u0026quot; for all columns airquality %\u0026gt;% summarize_all(~sum(is.na(.))) # Get proportion of missing values for each column airquality %\u0026gt;% summarize_all(~ sum(is.na(.)) / length(.)) # Quickly replace missing values in columns Ozone and Solar.R with \u0026#39;0\u0026#39; airquality %\u0026gt;% replace_na(list(Ozone = 0, Solar.R = 0))  Chaining Multiple Pipes The coding patterns covered in this course is my biggest take-away thus far.\nPreviously, I had not used longer patterns of piping to explore data. Much of my code involved maybe 2-3 pipes, saving a new dataframe, then continuing to explore. Here i’m piping seven operations without creating a new dataframe and it allows for more efficient exploration, without having to save extra data frames.\nMaking me re-think my approach to data wrangling.\n# Chaining multiple pipes to more efficiently explore data iris %\u0026gt;% group_by(Species) %\u0026gt;% summarize( count = n(), mean = mean(Petal.Length), median = median(Petal.Length), sd = sd(Petal.Length), min = min(Petal.Length), max = max(Petal.Length) ) %\u0026gt;% ungroup() %\u0026gt;% mutate(Range = max - min) %\u0026gt;% rename( `Standard Deviation` = sd, `Average` = mean, `Mininum` = min, `Maximum` = max ) %\u0026gt;% arrange(desc(Average))  ","date":1593475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593523452,"objectID":"72fbb2a0cd22040eeddcf45c449bc240","permalink":"/post/ds4b-data-wrangling/","publishdate":"2020-06-30T00:00:00Z","relpermalink":"/post/ds4b-data-wrangling/","section":"post","summary":"Cover data wrangling foundations in dplyr","tags":[],"title":"Data Wrangling Foundations","type":"post"},{"authors":[],"categories":[],"content":" Packages and Libraries library(maps) library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.1 ✓ purrr 0.3.3 ## ✓ tibble 3.0.1 ✓ dplyr 0.8.5 ## ✓ tidyr 1.0.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ───────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x purrr::map() masks maps::map() library(sp) library(rmarkdown) library(knitr) opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)  World and Thai Maps First, we’ll use map_data function from ggplot2 to turn any map from the maps package into a data frame. This provides longitude and lattitude data. Then, we’ll filter for Thailand to get Thai longtitude and lattitude data.\nworld.map \u0026lt;- map_data(\u0026quot;world\u0026quot;) head(world.map) ## long lat group order region subregion ## 1 -69.89912 12.45200 1 1 Aruba \u0026lt;NA\u0026gt; ## 2 -69.89571 12.42300 1 2 Aruba \u0026lt;NA\u0026gt; ## 3 -69.94219 12.43853 1 3 Aruba \u0026lt;NA\u0026gt; ## 4 -70.00415 12.50049 1 4 Aruba \u0026lt;NA\u0026gt; ## 5 -70.06612 12.54697 1 5 Aruba \u0026lt;NA\u0026gt; ## 6 -70.05088 12.59707 1 6 Aruba \u0026lt;NA\u0026gt; THAI.map \u0026lt;- world.map %\u0026gt;% filter(region == \u0026quot;Thailand\u0026quot;) head(THAI.map) ## long lat group order region subregion ## 1 99.66309 6.521924 1404 87912 Thailand Ko Tarutao ## 2 99.64404 6.516113 1404 87913 Thailand Ko Tarutao ## 3 99.60664 6.596827 1404 87914 Thailand Ko Tarutao ## 4 99.65401 6.714111 1404 87915 Thailand Ko Tarutao ## 5 99.70136 6.570557 1404 87916 Thailand Ko Tarutao ## 6 99.66309 6.521924 1404 87917 Thailand Ko Tarutao  Longitude and Lattitude Value Ranges Before converting UTM to longitude/lattitude data, we should know the range of both Longitudes and Lattitudes for Thailand.\nsummary(THAI.map$long) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 97.37 99.08 100.26 100.71 102.27 105.64 summary(THAI.map$lat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.637 9.084 13.213 13.249 17.820 20.424  Jobpost Data Frame Our objective is to visualize utm_x and utm_y in the jobpost data frame by turning them into lattitude and longitude data first. The jobpost data frame is retrieved from PostgreSQL.\nPreparation includes writing it to CSV before loading into Rmarkdown.\njobpost \u0026lt;- read.csv(\u0026quot;jobpost.csv\u0026quot;) glimpse(jobpost) ## Rows: 50 ## Columns: 25 ## $ X \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … ## $ jobpost_id \u0026lt;int\u0026gt; 54, 66, 33, 34, 35, 36, 28, 32, 30, 55, 67, 68, 37,… ## $ job_name \u0026lt;fct\u0026gt; \u0026quot;Facebook Marketing\u0026quot;, \u0026quot;แอดมิน\u0026quot;, \u0026quot;Accountant\u0026quot;, \u0026quot;แคชเ… ## $ job_qty \u0026lt;int\u0026gt; 3, 1, 1, 2, 2, 5, 3, 1, 5, 1, 22, 10, 1, 1, 2, 2, 1… ## $ age_min \u0026lt;int\u0026gt; 22, 25, 29, 20, 20, 19, 28, 28, 20, 25, 30, 21, 18,… ## $ age_max \u0026lt;int\u0026gt; 26, 32, 35, 35, 35, 40, 120, 40, 40, 45, 45, 30, 50… ## $ study_field \u0026lt;fct\u0026gt; \u0026quot;-\u0026quot;, \u0026quot;แฟชั่น\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;จัดการผักผ… ## $ job_qualification \u0026lt;fct\u0026gt; \u0026quot;อ่าน เขียน ภาษาอังกฤษ ได้ดี\u0026quot;, \u0026quot;ตอบคำถาม ภาษาอังกฤษ… ## $ min_salary \u0026lt;int\u0026gt; 30000, 12000, 20000, 13000, 10000, 15000, 15000, 12… ## $ job_description \u0026lt;fct\u0026gt; \u0026quot;ทำการตลาดทางช่องทาง facebook\u0026quot;, \u0026quot;แอดมินดูแล เพจ เสื… ## $ manychat_id \u0026lt;dbl\u0026gt; 3.961592e+15, 2.984969e+15, 2.941175e+15, 3.416291e… ## $ job_sex \u0026lt;int\u0026gt; 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 3, … ## $ study_level \u0026lt;int\u0026gt; 5, 5, 5, 0, 2, 2, 3, 4, 4, 5, 5, 4, 0, 2, 2, 5, 5, … ## $ work_exp \u0026lt;int\u0026gt; 1, 0, 3, 1, 0, 0, 0, 3, 0, 3, 3, 0, 0, 1, 1, 3, 6, … ## $ created \u0026lt;fct\u0026gt; 2020-06-07 09:00:36, 2020-06-14 23:12:35, 2020-05-2… ## $ updated \u0026lt;fct\u0026gt; 2020-06-08 09:05:23, 2020-06-14 23:12:35, 2020-05-2… ## $ confirmed \u0026lt;fct\u0026gt; 2020-06-07 09:00:36, 2020-06-14 23:12:35, 2020-05-2… ## $ batch \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ location \u0026lt;fct\u0026gt; บางนา, รามอินทรา 65, พระรามเก้า ซอย 60, ห้าง ริเวอร… ## $ utm_x \u0026lt;dbl\u0026gt; 674486.5, 678167.2, 676504.5, 661251.7, 714943.7, 6… ## $ utm_y \u0026lt;dbl\u0026gt; 1511131, 1532008, 1519745, 1515611, 1477934, 152128… ## $ utm_zone_number \u0026lt;int\u0026gt; 47, 47, 47, 47, 47, 47, 48, 47, 47, 47, 35, 48, 47,… ## $ utm_zone_letter \u0026lt;fct\u0026gt; P, P, P, P, P, P, Q, P, P, P, L, P, P, P, P, P, P, … ## $ job_type \u0026lt;int\u0026gt; NA, NA, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, 0, 0, 0, 0… ## $ online \u0026lt;lgl\u0026gt; NA, NA, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…  Subset Data Frame called UTM We’ll select only utm_x and utm_y columns from jobpost because we’re interested in these two columns.\nutm \u0026lt;- data.frame(jobpost$utm_x, jobpost$utm_y) str(utm) ## \u0026#39;data.frame\u0026#39;: 50 obs. of 2 variables: ## $ jobpost.utm_x: num 674486 678167 676504 661252 714944 ... ## $ jobpost.utm_y: num 1511131 1532008 1519745 1515611 1477934 ...  Handle Missing Values and Outliers Row 50 in jobpost and also utm is missing so we’ll delete that. Then we’ll also delete row 11 because it’s location is Zambia, Africa and its longitude and lattitude numbers are very different from Thailand - which will distort the map.\nutm \u0026lt;- utm[-50, ] utm \u0026lt;- utm[-11, ] jobpost \u0026lt;- jobpost[-50, ] jobpost \u0026lt;- jobpost[-11, ] str(utm) ## \u0026#39;data.frame\u0026#39;: 48 obs. of 2 variables: ## $ jobpost.utm_x: num 674486 678167 676504 661252 714944 ... ## $ jobpost.utm_y: num 1511131 1532008 1519745 1515611 1477934 ... str(jobpost) ## \u0026#39;data.frame\u0026#39;: 48 obs. of 25 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ jobpost_id : int 54 66 33 34 35 36 28 32 30 55 ... ## $ job_name : Factor w/ 48 levels \u0026quot;.Net Developer\u0026quot;,..: 6 48 2 21 19 39 11 35 4 42 ... ## $ job_qty : int 3 1 1 2 2 5 3 1 5 1 ... ## $ age_min : int 22 25 29 20 20 19 28 28 20 25 ... ## $ age_max : int 26 32 35 35 35 40 120 40 40 45 ... ## $ study_field : Factor w/ 19 levels \u0026quot;-\u0026quot;,\u0026quot;Food science\u0026quot;,..: 1 12 1 1 1 1 1 6 1 5 ... ## $ job_qualification: Factor w/ 41 levels \u0026quot;-\u0026quot;,\u0026quot;- มีใบขับขี่รถยนต์\\n- ผ่านการเกณฑ์ทหาร\u0026quot;,..: 41 16 9 38 37 32 33 30 15 23 ... ## $ min_salary : int 30000 12000 20000 13000 10000 15000 15000 12000 11500 25000 ... ## $ job_description : Factor w/ 50 levels \u0026quot;- Develops, modifies application software according to specifications and requirements.\\n- Develops application\u0026quot;| __truncated__,..: 30 50 27 4 16 14 15 23 7 47 ... ## $ manychat_id : num 3.96e+15 2.98e+15 2.94e+15 3.42e+15 3.00e+15 ... ## $ job_sex : int 3 3 2 2 3 3 3 3 3 3 ... ## $ study_level : int 5 5 5 0 2 2 3 4 4 5 ... ## $ work_exp : int 1 0 3 1 0 0 0 3 0 3 ... ## $ created : Factor w/ 26 levels \u0026quot;2020-05-29 14:21:22\u0026quot;,..: 12 24 1 1 1 1 1 1 1 13 ... ## $ updated : Factor w/ 33 levels \u0026quot;2020-05-29 14:21:22\u0026quot;,..: 19 30 1 7 1 8 1 1 6 22 ... ## $ confirmed : Factor w/ 26 levels \u0026quot;2020-05-29 14:21:22\u0026quot;,..: 12 24 1 1 1 1 1 1 1 13 ... ## $ batch : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ location : Factor w/ 50 levels \u0026quot;112/3 หมู่ 7 ต.บางโฉลง อ.บางพลี จ.สมุทรปราการ 10540\u0026quot;,..: 25 35 29 47 30 9 38 11 49 39 ... ## $ utm_x : num 674486 678167 676504 661252 714944 ... ## $ utm_y : num 1511131 1532008 1519745 1515611 1477934 ... ## $ utm_zone_number : int 47 47 47 47 47 47 48 47 47 47 ... ## $ utm_zone_letter : Factor w/ 4 levels \u0026quot;L\u0026quot;,\u0026quot;N\u0026quot;,\u0026quot;P\u0026quot;,\u0026quot;Q\u0026quot;: 3 3 3 3 3 3 4 3 3 3 ... ## $ job_type : int NA NA 0 0 0 0 0 0 0 NA ... ## $ online : logi NA NA FALSE FALSE FALSE FALSE ...  Conversion of UTM into Lat/Long After some research, we find out that Thailand’s UTM zone is 47N. The stack overflow source I used to find the conversion code is here.\nWe’ll create two SpatialPoints object classes. Then transform them into a data frame containing lat and long data.\nRemember to load sp library for this operation.\nsputm \u0026lt;- SpatialPoints(utm, proj4string = CRS(\u0026quot;+proj=utm +zone=47N +datum=WGS84\u0026quot;)) spgeo \u0026lt;- spTransform(sputm, CRS(\u0026quot;+proj=longlat +datum=WGS84\u0026quot;)) thai.map2 \u0026lt;- data.frame(Location = jobpost$location, lat = spgeo$jobpost.utm_y, long = spgeo$jobpost.utm_x) head(thai.map2) ## Location lat long ## 1 บางนา 13.66385 100.6132 ## 2 รามอินทรา 65 13.85233 100.6486 ## 3 พระรามเก้า ซอย 60 13.74159 100.6324 ## 4 ห้าง ริเวอร์ไซด์ พลาซ่า เจริญนคร ชั้น 1 ใน้ บันไดเลื่อน 13.70512 100.4912 ## 5 เมืองชลบุรี 13.36114 100.9847 ## 6 กรุงเทพ 13.75633 100.5018  Visualize with GGPLOT2 Here we’ll visualize the THAI.map we created previously and overlay the new Lat/Long data points (from UTM).\nWe can see a concentration of utm data points from jobpost were made in Bangkok and the greater Bangkok areas with some jobs also posted outside Bangkok.\nTHAI.map %\u0026gt;% ggplot() + geom_map(map = THAI.map, aes(x = long, y = lat, map_id = region), fill = \u0026quot;white\u0026quot;, color = \u0026quot;black\u0026quot;) + geom_point(data = thai.map2, aes(x = long, y = lat, color = \u0026quot;red\u0026quot;, alpha = 0.9)) ## Warning: Ignoring unknown aesthetics: x, y  ","date":1593043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593091452,"objectID":"2a58547e9de6e1dd4ed674eed3755b5e","permalink":"/post/r-markdown-utm/","publishdate":"2020-06-25T00:00:00Z","relpermalink":"/post/r-markdown-utm/","section":"post","summary":"Convert UTM to lat/long","tags":[],"title":"Converting Universal Transverse Mercator (UTM) to lattitude/longitude data","type":"post"},{"authors":["Paul Apivat"],"categories":[],"content":"                                     ","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592956800,"objectID":"27bcfc28d6d93cfc757f920b67727409","permalink":"/slides/wpa/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/slides/wpa/","section":"slides","summary":"Wharton People Analytics Conf Data Visualizaton Competition 2020","tags":["People Analytics","Data Viz"],"title":"Data Visualization Competition 2020 - Wharton People Analytics Conf","type":"slides"},{"authors":null,"categories":null,"content":"This Data Visualization competition was hosted by the Wharton People Analytics conference. They partnered with Doctors Without Borders, a medical humanitarian organization that delivers emergency aid to people affected by conflict, epidemics and natural disasters. Our task was to analyze data dating back to 2 decades on their global workforce comprising over 45,000 people to understand the career paths of Medical Coordinators (a top-level position within the organization).\nVery excited to have won 2nd place in this competition! See details here.\n","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592956800,"objectID":"1e15b1f5b1330df00068aa5d60b8e2f6","permalink":"/project/wharton-people-analytics/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/project/wharton-people-analytics/","section":"project","summary":"An example of using the in-built project page.","tags":["Data Viz","People Analytics"],"title":"Wharton People Analytics Data Visualization Competition 2020","type":"project"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^) = \\begin{cases} p_0^ \u0026amp; \\text{if }k=1, \\\\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Paul Apivat"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Paul Apivat"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Paul Apivat","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic","开源"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Paul Apivat","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Paul Apivat","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]