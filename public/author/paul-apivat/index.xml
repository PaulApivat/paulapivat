<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paul Apivat</title>
    <link>/author/paul-apivat/</link>
      <atom:link href="/author/paul-apivat/index.xml" rel="self" type="application/rss+xml" />
    <description>Paul Apivat</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2022 Paul Apivat Hanvongse. All Rights Reserved.</copyright><lastBuildDate>Tue, 12 Apr 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Paul Apivat</title>
      <link>/author/paul-apivat/</link>
    </image>
    
    <item>
      <title>My journey to web3 data</title>
      <link>/post/before_crypto/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>/post/before_crypto/</guid>
      <description>&lt;h3 id=&#34;before-crypto&#34;&gt;Before Crypto&lt;/h3&gt;
&lt;p&gt;I entered crypto in the fall of 2017, upon buying my first BTC &amp;amp; ETH as incentive to keep up with the industry, but my journey &lt;em&gt;actually&lt;/em&gt; started in 2012 when I took a Ruby on Rails Backend programming course. Up till that point, my training had been in organizational psychology, but the seeds for data work were planted then as I took the 1 train from 116th and Broadway to the Flat Iron district for evening classes at General Assembly.&lt;/p&gt;
&lt;p&gt;After completing my 
&lt;a href=&#34;https://www.proquest.com/openview/c6b31a2a839ca8d292087c35e86ec137/1?pq-origsite=gscholar&amp;amp;cbl=18750&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dissertation&lt;/a&gt; in 2014, I worked in higher education, exploring how organizations balance social and financial goals. At the time, I was interested in how leaders helped their organizations manage the social-financial tension. When presented with an opportunity to see an 
&lt;a href=&#34;https://www.learneducation.co.th/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;education company&lt;/a&gt; pursue both social and financial missions, I jumped in to the operational side. My partner and I led an in-house edtech startup, while supporting the parent company&amp;rsquo;s mission to subsidize education for low income schools.&lt;/p&gt;
&lt;p&gt;I came away with two key thoughts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Social enterprises are harder to operate than traditional for-profit enterprises. Good in theory, I have yet to find one that sustainably works in practice.&lt;/li&gt;
&lt;li&gt;Social enterprises might be more of a bandaid to a systemic problem of underfunding public goods.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By the 2017, I was looking for a change and bought BTC &amp;amp; ETH at the market top. I spent the next year absorbing white papers and attending conferences (Deconomy in Korea, Ethereum Community Conference in Toronto). Despite the market crash, I knew permissionless, censorship resistant, public networks were here to stay.&lt;/p&gt;
&lt;p&gt;Having skin in the game got me down the rabbit hole. The interdisciplinary nature of the industry kept me staying. Crypto appeared to have implications for multiple strands in my life from economics, to software and data, to organizing and coordination, the intellectual stimulation of was endless.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Quick tangent.&lt;/p&gt;
&lt;p&gt;My country has vacillated between democratically elected governments and autocratic military regimes since 1932. In the 70&amp;rsquo;s, politically active college students (my parents&#39; generation) protested military regime, resulting in a bloody clash. Fast foward 50 years and we &lt;em&gt;still&lt;/em&gt; have people protesting. Several decades worth of empirical data suggest protesting to be headline grabbing, but ultimately unsustainable.&lt;/p&gt;
&lt;p&gt;The fact that crypto represents the chance at a new system is compelling for me. The alternative is to reform old decaying institutions from within or wait for old dinosaurs to die out. I&amp;rsquo;ve seen that story and don&amp;rsquo;t have another 50 years to wait around for evolution. Crypto represents a &lt;em&gt;chance&lt;/em&gt; at punctuated equilibrium and i&amp;rsquo;m here for it.&lt;/p&gt;
&lt;p&gt;Insert &amp;ldquo;remind me in 20 years&amp;rdquo; tweet.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I spent 2019 leveling up my technical skills, taking a web development bootcamp (Lambda School, now BloomTech), doing consulting (AWRL) and briefly working for a startup in the Cosmos ecosystem (TruStory).&lt;/p&gt;
&lt;p&gt;Things started coalescing in 2020 as I placed 2nd in a data visualization competition to be presented at the Wharton People Analytics conference before Covid19 shut the world down. I continued to hone my data skills.&lt;/p&gt;
&lt;p&gt;Another inflection point came with the Bankless membership airdrop in May 2021; I got in at the ground level and started contributing.&lt;/p&gt;
&lt;p&gt;I spent 2021 enabling capital and coordination through on- and off-chain data at 
&lt;a href=&#34;https://www.bankless.community/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bankless DAO&lt;/a&gt; as a core contributor ( 
&lt;a href=&#34;https://app.poap.xyz/scan/0xdfdf2d882d9ebce6c7eac3da9ab66cbfda263781&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;POAPs&lt;/a&gt;), from genesis to season 3.&lt;/p&gt;
&lt;p&gt;I was a founding member at the 
&lt;a href=&#34;https://www.notion.so/bankless/BanklessDAO-Wiki-82ba81e7da1c42adb7c4ab67a4f22e8f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Analytics Guild&lt;/a&gt; where I shared insights with the community through 
&lt;a href=&#34;https://forum.bankless.community/u/paulapivat/activity/topics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forum posts&lt;/a&gt;. My colleagues and I built 
&lt;a href=&#34;https://www.notion.so/bankless/DAO-Dash-41a151ce8ef74fcd893cba3b47223828&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DAO Dash&lt;/a&gt;, an in-house analytics platform to provide insights to guilds and projects at BanklessDAO. I also provided growth and usage metrics for the BanklessDAO 
&lt;a href=&#34;https://www.notion.so/bankless/Bounty-Board-318dc164cc5640cca17e0fb5f484fd90&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bounty Board&lt;/a&gt; project.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m currently focused on leveling up my &lt;strong&gt;smart contract sleuthing&lt;/strong&gt; skills to analyze DeFi protocols.&lt;/p&gt;
&lt;p&gt;I am working with the Bankless DeFi Innovation Index team to provide on-chain data overview of various projects that make up $GMI.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;d like help with on-chain analysis, please 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get in touch&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Inspect Element to Grab API Endpoints and Tokens</title>
      <link>/post/grab_api_inspect_element/</link>
      <pubDate>Fri, 28 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/post/grab_api_inspect_element/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;When maintaining data pipelines, sometimes API endpoints get changed or authorization tokens expire prompting a need to grab new &lt;strong&gt;API endpoints&lt;/strong&gt; and &lt;strong&gt;bearer tokens&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This post provide a brief walk-through on how to use &lt;strong&gt;Inspect Element&lt;/strong&gt; on a web browser to grab this data. Which represents a powerful way to access a project&amp;rsquo;s API &lt;em&gt;before&lt;/em&gt; reaching out to the team.&lt;/p&gt;
&lt;h3 id=&#34;scenario&#34;&gt;Scenario&lt;/h3&gt;
&lt;p&gt;We have a data pipeline to ingest 
&lt;a href=&#34;https://coordinape.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Coordinape&lt;/a&gt; data but after two months, the pipeline script returns a &lt;code&gt;Response 401&lt;/code&gt; error, suggesting a connection could not be made. We want a way to explore what&amp;rsquo;s going on.&lt;/p&gt;
&lt;h4 id=&#34;api-endpoint-with-inspect-element&#34;&gt;API Endpoint with Inspect Element&lt;/h4&gt;
&lt;p&gt;We go to the Coordinape application and sign-in with our Ethereum wallet. Then, I left click on go to &lt;code&gt;inspect element&lt;/code&gt;, navigating to the &lt;strong&gt;Network&lt;/strong&gt; tab as shown here:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./network_xhr.png&#34; alt=&#34;inspect_element&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once in the &lt;strong&gt;Network&lt;/strong&gt; tab, select &lt;strong&gt;Fetch/XHR&lt;/strong&gt; (XHR stands for XMLHttpRequest, but XML is phased out for JSON). Then we should see a list of events.&lt;/p&gt;
&lt;p&gt;The event highlighted in red indicates a &lt;strong&gt;Request URL&lt;/strong&gt; that shows the &lt;em&gt;current&lt;/em&gt; API endpoint (in Header)&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the endpoint: &lt;code&gt;https://api.coordinape.com/api/v2/manifest?circle_id=63&lt;/code&gt; (&lt;strong&gt;note&amp;amp;&lt;/strong&gt;: in the old endpoint, manifest was token-gifts. It looks like the team has consolidated it&amp;rsquo;s API endpoints)&lt;/p&gt;
&lt;h4 id=&#34;authorization&#34;&gt;Authorization&lt;/h4&gt;
&lt;p&gt;To grab the authorization, &lt;strong&gt;bearer token&lt;/strong&gt;, scroll down in &lt;strong&gt;Header&lt;/strong&gt; down to &lt;strong&gt;Request Headers&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./network_xhr_1.png&#34; alt=&#34;authorization&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is the information you&amp;rsquo;d store in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt;
&lt;h3 id=&#34;data-structure&#34;&gt;Data Structure&lt;/h3&gt;
&lt;p&gt;Finally, you can also get a sense for what kind of data will be returned with this new API endpoint by navigating to the &lt;strong&gt;Preview&lt;/strong&gt; tab next to &lt;strong&gt;Headers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./network_xhr_2.png&#34; alt=&#34;preview&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we see circle, circles, myUsers and profile. It looks like we have nested JSON that will need to be flattened.&lt;/p&gt;
&lt;p&gt;For more use of data to explore DAOs and web3 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learn Foundational Ethereum Topics with SQL</title>
      <link>/post/query_ethereum/</link>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/query_ethereum/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#transactions&#34;&gt;Transactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#breaking_down_transactions&#34;&gt;Breaking Down Transactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#blocks&#34;&gt;Blocks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#gas&#34;&gt;Gas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Many Ethereum tutorials target developers, but thereâs a lack of educational resources for data analyst or for people who wish to see on-chain data without running a client or node.&lt;/p&gt;
&lt;p&gt;This tutorial helps readers understand fundamental Ethereum concepts including transactions, blocks and gas by querying on-chain data with structured query language (SQL) through an interface provided by 
&lt;a href=&#34;https://duneanalytics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dune Analytics&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On-chain data can help us understand Ethereum, the network, and as an economy for computing power and should serve as a base for understanding challenges facing Ethereum today (i.e., rising gas prices) and, more importantly, discussions around scaling solutions.&lt;/p&gt;
&lt;h3 id=&#34;transactions&#34;&gt;Transactions&lt;/h3&gt;
&lt;p&gt;A userâs journey on Ethereum starts with initializing a user-controlled account or an entity with an ETH balance. There are two account types - user-controlled or a smart contract (see 
&lt;a href=&#34;https://ethereum.org/en/developers/docs/accounts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ethereum.org&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Any account can be viewed on a block explorer like 
&lt;a href=&#34;https://etherscan.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Etherscan&lt;/a&gt;. Block explorers are a portal to Ethereumâs data. They display, in real-time, data on blocks, transactions, miners, accounts and other on-chain activity (see 
&lt;a href=&#34;https://ethereum.org/en/developers/docs/data-and-analytics/block-explorers/#top&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;However, a user may wish to query the data directly to reconcile the information provided by external block explorers. 
&lt;a href=&#34;https://duneanalytics.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dune Analytics&lt;/a&gt; provides this capability to anyone with some knowledge of SQL.&lt;/p&gt;
&lt;p&gt;For reference, the smart contract account for the Ethereum Foundation (EF) can be viewed on 
&lt;a href=&#34;https://etherscan.io/address/0xde0b295669a9fd93d5f28d9ec85e40f4cb697bae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Etherscan&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One thing to note is that all accounts, including the EFâs, has a public address that can be used to send and receive transactions.&lt;/p&gt;
&lt;p&gt;The account balance on Etherscan comprises regular transactions and internal transactions. Internal transactions, despite the name, are not &lt;em&gt;actual&lt;/em&gt; transactions that change the state of the chain. They are value transfers initiated by executing a contract (
&lt;a href=&#34;https://ethereum.stackexchange.com/questions/3417/how-to-get-contract-internal-transactions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;). Since internal transactions have no signature, they are &lt;strong&gt;not&lt;/strong&gt; included on the blockchain and cannot be queried with Dune Analytics.&lt;/p&gt;
&lt;p&gt;Therefore, this tutorial will focus on regular transactions. This can be queried as such:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;WITH temp_table AS (
SELECT 
    hash,
    block_number,
    block_time,
    &amp;quot;from&amp;quot;,
    &amp;quot;to&amp;quot;,
    value / 1e18 AS ether,
    gas_used,
    gas_price / 1e9 AS gas_price_gwei
FROM ethereum.&amp;quot;transactions&amp;quot;
WHERE &amp;quot;to&amp;quot; = &#39;\xde0B295669a9FD93d5F28D9Ec85E40f4cb697BAe&#39;   
ORDER BY block_time DESC
)
SELECT
    hash,
    block_number,
    block_time,
    &amp;quot;from&amp;quot;,
    &amp;quot;to&amp;quot;,
    ether,
    (gas_used * gas_price_gwei) / 1e9 AS txn_fee
FROM temp_table

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will yield the same information as provided on Etherscan&amp;rsquo;s transaction page. For comparison, here are the two sources:&lt;/p&gt;
&lt;h4 id=&#34;etherscan&#34;&gt;Etherscan&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;./etherscan_view.png&#34; alt=&#34;etherscan_view&#34;&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://etherscan.io/address/0xde0B295669a9FD93d5F28D9Ec85E40f4cb697BAe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EF&amp;rsquo;s contract page on Etherscan.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;dune-analytics&#34;&gt;Dune Analytics&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;./dune_view.png&#34; alt=&#34;dune_view&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can find dashboard 
&lt;a href=&#34;https://duneanalytics.com/paulapivat/Learn-Ethereum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Click on the table to see the query (also see above).&lt;/p&gt;
&lt;h3 id=&#34;breaking_down_transactions&#34;&gt;Breaking_Down_Transactions&lt;/h3&gt;
&lt;p&gt;A submitted transaction includes several pieces of information including (
&lt;a href=&#34;https://ethereum.org/en/developers/docs/transactions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Recipient&lt;/strong&gt;: The receiving address (queried as &amp;ldquo;to&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Signature&lt;/strong&gt;: While a sender&amp;rsquo;s private keys signs a transaction, what we can query with SQL is a sender&amp;rsquo;s public address (&amp;ldquo;from&amp;rdquo;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Value&lt;/strong&gt;: This is the amount of ETH transferred (see &lt;code&gt;ether&lt;/code&gt; column).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: This is arbitrary data that&amp;rsquo;s been hashed (see &lt;code&gt;data&lt;/code&gt; column)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gasLimit&lt;/strong&gt;: The maximum amount of gas, or the cost of computation, that can be consumed by a transaction (see &lt;code&gt;gas_limit&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gasPrice&lt;/strong&gt;: The fee the sender pays to sign a transaction to the blockchain. Gas is denominated in Gwei which is 0.000000001 ETH (nine decimal places).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can query these specific pieces of information for transactions to the Ethereum Foundation public address:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT 
    &amp;quot;to&amp;quot;,
    &amp;quot;from&amp;quot;,
    value / 1e18 AS ether,
    data,
    gas_limit,
    gas_price / 1e9 AS gas_price_gwei,
    gas_used,
    ROUND(((gas_used / gas_limit) * 100),2) AS gas_used_pct
FROM ethereum.&amp;quot;transactions&amp;quot;
WHERE &amp;quot;to&amp;quot; = &#39;\xde0B295669a9FD93d5F28D9Ec85E40f4cb697BAe&#39;   
ORDER BY block_time DESC

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;blocks&#34;&gt;Blocks&lt;/h3&gt;
&lt;p&gt;Each transaction will change the state of the Ethereum virtual machine (
&lt;a href=&#34;https://ethereum.org/en/developers/docs/evm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EVM&lt;/a&gt;) (
&lt;a href=&#34;https://ethereum.org/en/developers/docs/transactions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;). Transactions are broadcasted to the network to be verified and included in a block. Each transaction is associated with a block number. To see the data, we could query a specific block number:  12396854 (the most recent block among Ethereum Foundation transactions as of this writing, 11/5/21).&lt;/p&gt;
&lt;p&gt;Moreover, when we query the next two blocks, we can see that each block contains the hash of the previous block (i.e., parent hash), illustrating how the blockchain is formed.&lt;/p&gt;
&lt;p&gt;Each block contains a reference to it parent block. This is shown below between the &lt;code&gt;hash&lt;/code&gt; and &lt;code&gt;parent_hash&lt;/code&gt; columns (
&lt;a href=&#34;https://ethereum.org/en/developers/docs/blocks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./parent_hash.png&#34; alt=&#34;parent_hash&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here is the 
&lt;a href=&#34;https://duneanalytics.com/queries/44856/88292&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;query&lt;/a&gt; on Dune Analytics:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT
   time,
   number,
   difficulty,
   hash,
   parent_hash,
   nonce
FROM ethereum.&amp;quot;blocks&amp;quot;
WHERE &amp;quot;number&amp;quot; = 12396854 OR &amp;quot;number&amp;quot; = 12396855 OR &amp;quot;number&amp;quot; = 12396856
LIMIT 10

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can  examine a block by querying time, block number, difficulty, hash, parent hash, and nonce.&lt;/p&gt;
&lt;p&gt;The only thing this query does not cover is &lt;em&gt;list of transaction&lt;/em&gt; which requires a separate query below and &lt;em&gt;state root&lt;/em&gt;. A full or archival node will store all transactions and state transitions, allowing for clients to query the state of the chain at any time. Because this requires large storage space, we can separate chain data from state data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chain data (list of blocks, transactions)&lt;/li&gt;
&lt;li&gt;State data (result of each transactionâs state transition)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;State root falls in the latter and is &lt;em&gt;implicit&lt;/em&gt; data (not stored on-chain), while chain data is explicit and stored on the chain itself (
&lt;a href=&#34;https://ethereum.stackexchange.com/questions/359/where-is-the-state-data-stored&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;For this tutorial, we&amp;rsquo;ll be focusing on on-chain data that &lt;em&gt;can&lt;/em&gt; be queried with SQL via Dune Analytics.&lt;/p&gt;
&lt;p&gt;As stated above, each block contains a list of transactions, we can query this by filtering for a specific block. We&amp;rsquo;ll try the most recent block, 12396854:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT * FROM ethereum.&amp;quot;transactions&amp;quot;
WHERE block_number = 12396854 
ORDER BY block_time DESC`

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the SQL output on Dune:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./list_of_txn.png&#34; alt=&#34;list_of_txn&#34;&gt;&lt;/p&gt;
&lt;p&gt;This single block being added to the chain changes the state of the Ethereum virtual machine (
&lt;a href=&#34;https://ethereum.org/en/developers/docs/evm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EVM&lt;/a&gt;). Dozens sometimes, hundreds of transactions are verified at once. In this specific case, 222 transactions were included.&lt;/p&gt;
&lt;p&gt;To see how many were actually successful, we would add another filter to count successful transactions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;WITH temp_table AS (
    SELECT * FROM ethereum.&amp;quot;transactions&amp;quot;
    WHERE block_number = 12396854 AND success = true
    ORDER BY block_time DESC
)
SELECT
    COUNT(success) AS num_successful_txn
FROM temp_table

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For block 12396854, out of 222 total transactions, 204 were successfully verified:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./successful_txn.png&#34; alt=&#34;successful_txn&#34;&gt;&lt;/p&gt;
&lt;p&gt;Transactions requests occur dozens of times per second, but blocks are committed approximately once every 15 seconds (
&lt;a href=&#34;https://ethereum.org/en/developers/docs/blocks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;To see that there is one block produced approximately every 15 seconds, we could take the number of seconds in a day (86400) divided by 15 to get an &lt;em&gt;estimate&lt;/em&gt; average number of blocks per day (~ 5760).&lt;/p&gt;
&lt;p&gt;The chart for Ethereum blocks produced per day (2016 - present) is:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./daily_blocks.png&#34; alt=&#34;daily_blocks&#34;&gt;&lt;/p&gt;
&lt;p&gt;The average number of blocks produced daily over this time period is ~5,874:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./avg_daily_blocks.png&#34; alt=&#34;avg_daily_blocks&#34;&gt;&lt;/p&gt;
&lt;p&gt;The queries are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# query to visualize number of blocks produced daily since 2016

SELECT 
    DATE_TRUNC(&#39;day&#39;, time) AS dt,
    COUNT(*) AS block_count
FROM ethereum.&amp;quot;blocks&amp;quot;
GROUP BY dt
OFFSET 1

# average number of blocks produced per day

WITH temp_table AS (
SELECT 
    DATE_TRUNC(&#39;day&#39;, time) AS dt,
    COUNT(*) AS block_count
FROM ethereum.&amp;quot;blocks&amp;quot;
GROUP BY dt
OFFSET 1
)
SELECT 
    AVG(block_count) AS avg_block_count
FROM temp_table
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average number of blocks produced per day since 2016 is slightly above that number at 5,874. Alternatively, dividing 86400 seconds by 5874 average blocks comes out to 14.7 seconds or approximately one block every 15 seconds.&lt;/p&gt;
&lt;h3 id=&#34;gas&#34;&gt;Gas&lt;/h3&gt;
&lt;p&gt;Blocks are bounded in size. Each block has a gas limit which is collectively set by miners and the network to prevent arbitrarily large block size to be less of a strain on full node in terms of disk space and speed requirements (
&lt;a href=&#34;https://ethereum.org/en/developers/docs/blocks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;One way to conceptualize block gas limit is to think of it as the &lt;strong&gt;supply&lt;/strong&gt; of available block space in which to batch transactions. The block gas limit can be queried and visualized from 2016 to present day:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./avg_gas_limit.png&#34; alt=&#34;avg_gas_limit&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT 
    DATE_TRUNC(&#39;day&#39;, time) AS dt,
    AVG(gas_limit) AS avg_block_gas_limit
FROM ethereum.&amp;quot;blocks&amp;quot;
GROUP BY dt
OFFSET 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then there is the actual gas used daily to pay for computing done on the Ethereum chain (i.e., sending transaction, calling a smart contract, minting an NFT). This is the &lt;strong&gt;demand&lt;/strong&gt; for available Ethereum block space:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./daily_gas_used.png&#34; alt=&#34;daily_gas_used&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT 
    DATE_TRUNC(&#39;day&#39;, time) AS dt,
    AVG(gas_used) AS avg_block_gas_used
FROM ethereum.&amp;quot;blocks&amp;quot;
GROUP BY dt
OFFSET 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also juxtapose these two charts together to see how &lt;strong&gt;demand and supply&lt;/strong&gt; line up:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./gas_demand_supply.png&#34; alt=&#34;gas_demand_supply&#34;&gt;&lt;/p&gt;
&lt;p&gt;Therefore we can understand gas prices as a function of demand for Ethereum block space, given available supply.&lt;/p&gt;
&lt;p&gt;Finally, we may want to query average daily gas prices for the Ethereum chain, however, doing so result in an especially long query time, so weâll filter our query to the average amount of gas paid per transaction by the Ethereum Foundation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./ef_daily_gas.png&#34; alt=&#34;ef_daily_gas&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can see gas prices paid in transaction to the Ethereum Foundation address over the years. Here is the query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT 
    block_time,
    gas_price / 1e9 AS gas_price_gwei,
    value / 1e18 AS eth_sent
FROM ethereum.&amp;quot;transactions&amp;quot;
WHERE &amp;quot;to&amp;quot; = &#39;\xde0B295669a9FD93d5F28D9Ec85E40f4cb697BAe&#39;   
ORDER BY block_time DESC
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;With this tutorial, we understand foundational Ethereum concepts and how the Ethereum blockchain works by querying and getting a feel for on-chain data.&lt;/p&gt;
&lt;p&gt;The dashboard that holds all code used in this tutorial can be found 
&lt;a href=&#34;https://duneanalytics.com/paulapivat/Learn-Ethereum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more use of data to explore web3 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pad Thai is a Terrible Choice</title>
      <link>/post/thai_dishes_project/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/post/thai_dishes_project/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#exploratory_questions&#34;&gt;Exploratory Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#web_scraping&#34;&gt;Web Scraping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#data_cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#data_visualization&#34;&gt;Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#text_mining&#34;&gt;Text Mining&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;Let&amp;rsquo;s order Thai.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Great, what&amp;rsquo;s your go-to dish?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Pad Thai.â&lt;/p&gt;
&lt;p&gt;This has bugged me for years and is the genesis for this project.&lt;/p&gt;
&lt;p&gt;People need to know they have other choices aside from Pad Thai. Pad Thai is one of 53 individual dishes and stopping there risks missing out on at least 201 shared Thai dishes (source: 
&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Thai_dishes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wikipedia&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This project is an opportunity to build a data set of Thai dishes by scraping tables off Wikipedia. We will use Python for web scraping and R for visualization. Web scraping is done in &lt;code&gt;Beautiful Soup&lt;/code&gt; (Python) and pre-processed further with &lt;code&gt;dplyr&lt;/code&gt; and visualized with &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Furthermore, we&amp;rsquo;ll use the &lt;code&gt;tidytext&lt;/code&gt; package in R to explore the names of Thai dishes (in English) to see if we can learn some interest things from text data.&lt;/p&gt;
&lt;p&gt;Finally, there is an opportunity to make an open source 
&lt;a href=&#34;https://github.com/holtzy/R-graph-gallery/pull/34&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;contribution&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The project repo is 
&lt;a href=&#34;https://github.com/PaulApivat/thai_dishes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;exploratory_questions&#34;&gt;Exploratory_Questions&lt;/h3&gt;
&lt;p&gt;The purpose of this analysis is to generate questions.&lt;/p&gt;
&lt;p&gt;Because &lt;strong&gt;exploratory analysis&lt;/strong&gt; is iterative, these questions were generated in the process of manipulating and visualizing data. We can use these questions to structure the rest of the post:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How might we organized Thai dishes?&lt;/li&gt;
&lt;li&gt;What is the best way to organized the different dishes?&lt;/li&gt;
&lt;li&gt;Which raw material(s) are most popular?&lt;/li&gt;
&lt;li&gt;Which raw materials are most important?&lt;/li&gt;
&lt;li&gt;Could you learn about Thai food just from the names of the dishes?&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;web_scraping&#34;&gt;Web_Scraping&lt;/h3&gt;
&lt;p&gt;We scraped over 
&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Thai_dishes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;300 Thai dishes&lt;/a&gt;. For each dish, we got:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thai name&lt;/li&gt;
&lt;li&gt;Thai script&lt;/li&gt;
&lt;li&gt;English name&lt;/li&gt;
&lt;li&gt;Region&lt;/li&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, we&amp;rsquo;ll use the following Python libraries/modules:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests
from bs4 import BeautifulSoup
import urllib.request
import urllib.parse
import urllib.error
import ssl
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll use &lt;code&gt;requests&lt;/code&gt; to send an HTTP requests to the wikipedia url we need. We&amp;rsquo;ll access network sockets using &amp;lsquo;secure sockets layer&amp;rsquo; (SSL). Then we&amp;rsquo;ll read in the html data to parse it with &lt;strong&gt;Beautiful Soup&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Before using &lt;strong&gt;Beautiful Soup&lt;/strong&gt;, we want to understand the structure of the page (and tables) we want to scrape under &lt;strong&gt;inspect element&lt;/strong&gt; on the browser (note: I used Chrome). We can see that we want the &lt;code&gt;table&lt;/code&gt; tag, along with &lt;code&gt;class&lt;/code&gt; of wikitable sortable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./web_scrap.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The main function we&amp;rsquo;ll use from &lt;strong&gt;Beautiful Soup&lt;/strong&gt; is &lt;code&gt;findAll()&lt;/code&gt; and the three parameters are &lt;code&gt;th&lt;/code&gt; (Header Cell in HTML table), &lt;code&gt;tr&lt;/code&gt; (Row in HTML table) and &lt;code&gt;td&lt;/code&gt; (Standard Data Cell).&lt;/p&gt;
&lt;p&gt;First, we&amp;rsquo;ll save the table headers in a list, which we&amp;rsquo;ll use when creating an empty &lt;code&gt;dictionary&lt;/code&gt; to store the data we need.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;header = [item.text.rstrip() for item in all_tables[0].findAll(&#39;th&#39;)]

table = dict([(x, 0) for x in header])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initially, we want to scrape one table, knowing that we&amp;rsquo;ll need to repeat the process for all 16 tables. Therefore we&amp;rsquo;ll use a &lt;em&gt;nested loop&lt;/em&gt;. Because all tables have 6 columns, we&amp;rsquo;ll want to create 6 empty lists.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll scrape through all table rows &lt;code&gt;tr&lt;/code&gt; and check for 6 cells (which we should have for 6 columns), then we&amp;rsquo;ll &lt;em&gt;append&lt;/em&gt; the data to each empty list we created.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# loop through all 16 tables
a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

# 6 empty list (for 6 columns) to store data
a1 = []
a2 = []
a3 = []
a4 = []
a5 = []
a6 = []

# nested loop for looping through all 16 tables, then all tables individually
for i in a:
    for row in all_tables[i].findAll(&#39;tr&#39;):
        cells = row.findAll(&#39;td&#39;)
        if len(cells) == 6:
            a1.append([string for string in cells[0].strings])
            a2.append(cells[1].find(text=True))
            a3.append(cells[2].find(text=True))
            a4.append(cells[3].find(text=True))
            a5.append(cells[4].find(text=True))
            a6.append([string for string in cells[5].strings])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You&amp;rsquo;ll note the code for &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a6&lt;/code&gt; are slightly different. In retrospect, I found that &lt;code&gt;cells[0].find(text=True)&lt;/code&gt; did &lt;strong&gt;not&lt;/strong&gt; yield certain texts, particularly if they were links, therefore a slight adjustment is made.&lt;/p&gt;
&lt;p&gt;The strings tag returns a &lt;code&gt;NavigableString&lt;/code&gt; type object while text returns a &lt;code&gt;unicode&lt;/code&gt; object (see 
&lt;a href=&#34;https://stackoverflow.com/questions/25327693/difference-between-string-and-text-beautifulsoup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stack overflow&lt;/a&gt; explanation).&lt;/p&gt;
&lt;p&gt;After we&amp;rsquo;ve scrapped the data, we&amp;rsquo;ll need to store the data in a &lt;code&gt;dictionary&lt;/code&gt; before converting to &lt;code&gt;data frame&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create dictionary
table = dict([(x, 0) for x in header])

# append dictionary with corresponding data list
table[&#39;Thai name&#39;] = a1
table[&#39;Thai script&#39;] = a2
table[&#39;English name&#39;] = a3
table[&#39;Image&#39;] = a4
table[&#39;Region&#39;] = a5
table[&#39;Description&#39;] = a6

# turn dict into dataframe
df_table = pd.DataFrame(table)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a6&lt;/code&gt;, we need to do an extra step of joining the strings together, so I&amp;rsquo;ve created two additional corresponding columns, &lt;code&gt;Thai name 2&lt;/code&gt; and &lt;code&gt;Description2&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Need to Flatten Two Columns: &#39;Thai name&#39; and &#39;Description&#39;
# Create two new columns
df_table[&#39;Thai name 2&#39;] = &amp;quot;&amp;quot;
df_table[&#39;Description2&#39;] = &amp;quot;&amp;quot;

# join all words in the list for each of 328 rows and set to thai_dishes[&#39;Description2&#39;] column
# automatically flatten the list
df_table[&#39;Description2&#39;] = [
    &#39; &#39;.join(cell) for cell in df_table[&#39;Description&#39;]]

df_table[&#39;Thai name 2&#39;] = [
    &#39; &#39;.join(cell) for cell in df_table[&#39;Thai name&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After we&amp;rsquo;ve scrapped all the data and converted from &lt;code&gt;dictionary&lt;/code&gt; to &lt;code&gt;data frame&lt;/code&gt;, we&amp;rsquo;ll write to CSV to prepare for data cleaning in R (&lt;strong&gt;note&lt;/strong&gt;: I saved the csv as thai_dishes.csv, but you can choose a different name).&lt;/p&gt;
&lt;h3 id=&#34;data_cleaning&#34;&gt;Data_Cleaning&lt;/h3&gt;
&lt;p&gt;Data cleaning is typically non-linear.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll manipulate the data to explore, learn &lt;em&gt;about&lt;/em&gt; the data and see that certain things need cleaning or, in some cases, going back to Python to re-scrape. The columns &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a6&lt;/code&gt; were scraped differently from other columns due to &lt;strong&gt;missing data&lt;/strong&gt; found during exploration and cleaning.&lt;/p&gt;
&lt;p&gt;For certain links, using &lt;code&gt;.find(text=True)&lt;/code&gt; did not work as intended, so a slight adjustment was made.&lt;/p&gt;
&lt;p&gt;For this post, &lt;code&gt;R&lt;/code&gt; is the tool of choice for cleaning the data.&lt;/p&gt;
&lt;p&gt;Here are other data cleaning tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Changing column names (snake case)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# read data
df &amp;lt;- read_csv(&amp;quot;thai_dishes.csv&amp;quot;)

# change column name
df &amp;lt;- df %&amp;gt;%
    rename(
        Thai_name = `Thai name`,
        Thai_name_2 = `Thai name 2`,
        Thai_script = `Thai script`,
        English_name = `English name`
    )

&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Remove newline escape sequence (\n)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# remove  \n from all columns ----
df$Thai_name &amp;lt;- gsub(&amp;quot;[\n]&amp;quot;, &amp;quot;&amp;quot;, df$Thai_name)
df$Thai_name_2 &amp;lt;- gsub(&amp;quot;[\n]&amp;quot;, &amp;quot;&amp;quot;, df$Thai_name_2)
df$Thai_script &amp;lt;- gsub(&amp;quot;[\n]&amp;quot;, &amp;quot;&amp;quot;, df$Thai_script)
df$English_name &amp;lt;- gsub(&amp;quot;[\n]&amp;quot;, &amp;quot;&amp;quot;, df$English_name)
df$Image &amp;lt;- gsub(&amp;quot;[\n]&amp;quot;, &amp;quot;&amp;quot;, df$Image)
df$Region &amp;lt;- gsub(&amp;quot;[\n]&amp;quot;, &amp;quot;&amp;quot;, df$Region)
df$Description &amp;lt;- gsub(&amp;quot;[\n]&amp;quot;, &amp;quot;&amp;quot;, df$Description)
df$Description2 &amp;lt;- gsub(&amp;quot;[\n]&amp;quot;, &amp;quot;&amp;quot;, df$Description2)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Add/Mutate new columns (major_groupings, minor_groupings):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add Major AND Minor Groupings ----
df &amp;lt;- df %&amp;gt;%
    mutate(
        major_grouping = as.character(NA),
        minor_grouping = as.character(NA)
        )
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Edit rows for missing data in Thai_name column: 26, 110, 157, 234-238, 240, 241, 246&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This was only necessary the first time round, after the changes are made to how I scraped &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a6&lt;/code&gt;, this step is &lt;strong&gt;no longer necessary&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# If necessary; may not need to do this after scraping a1 and a6 - see above
# Edit Rows for missing Thai_name
df[26,]$Thai_name &amp;lt;- &amp;quot;Khanom chin nam ngiao&amp;quot;
df[110,]$Thai_name &amp;lt;- &amp;quot;Lap Lanna&amp;quot;
df[157,]$Thai_name &amp;lt;- &amp;quot;Kai phat khing&amp;quot;
df[234,]$Thai_name &amp;lt;- &amp;quot;Nam chim chaeo&amp;quot;
df[235,]$Thai_name &amp;lt;- &amp;quot;Nam chim kai&amp;quot;
df[236,]$Thai_name &amp;lt;- &amp;quot;Nam chim paesa&amp;quot;
df[237,]$Thai_name &amp;lt;- &amp;quot;Nam chim sate&amp;quot;
df[238,]$Thai_name &amp;lt;- &amp;quot;Nam phrik i-ke&amp;quot;
df[240,]$Thai_name &amp;lt;- &amp;quot;Nam phrik kha&amp;quot;
df[241,]$Thai_name &amp;lt;- &amp;quot;Nam phrik khaep mu&amp;quot;
df[246,]$Thai_name &amp;lt;- &amp;quot;Nam phrik pla chi&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;save to &amp;ldquo;edit_thai_dishes.csv&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Write new csv to save edits made to data frame
write_csv(df, &amp;quot;edit_thai_dishes.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;data_visualization&#34;&gt;Data_Visualization&lt;/h3&gt;
&lt;p&gt;There are several ways to visualize the data. Because we want to communicate the diversity of Thai dishes, &lt;em&gt;aside&lt;/em&gt; from Pad Thai, we want a visualization that captures the many, many options.&lt;/p&gt;
&lt;p&gt;I opted for a &lt;strong&gt;dendrogram&lt;/strong&gt;. This graph assumes hierarchy within the data, which fits our project because we can organize the dishes in grouping and sub-grouping.&lt;/p&gt;
&lt;h4 id=&#34;how-might-we-organized-thai-dishes&#34;&gt;How might we organized Thai dishes?&lt;/h4&gt;
&lt;p&gt;We first make a distinction between &lt;strong&gt;individual&lt;/strong&gt; and &lt;strong&gt;shared&lt;/strong&gt; dishes to show that Pad Thai is not even close to being the best &lt;em&gt;individual&lt;/em&gt; dish. And, in fact, more dishes fall under the &lt;strong&gt;shared&lt;/strong&gt; grouping.&lt;/p&gt;
&lt;p&gt;To avoid cramming too much data into one visual, we&amp;rsquo;ll create two separate visualizations for individual vs. shared dishes.&lt;/p&gt;
&lt;p&gt;Here is the first &lt;strong&gt;dendrogram&lt;/strong&gt; representing 52 individual dish alternatives to Pad Thai.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./indiv_thai_dishes.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Creating a dendrogram requires using the &lt;code&gt;ggraph&lt;/code&gt; and &lt;code&gt;igraph&lt;/code&gt; libraries. First, we&amp;rsquo;ll load the libraries and sub-set our data frame by filtering for Individual Dishes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df &amp;lt;- read_csv(&amp;quot;edit_thai_dishes.csv&amp;quot;)

library(ggraph)
library(igraph)

df %&amp;gt;%
    select(major_grouping, minor_grouping, Thai_name, Thai_script) %&amp;gt;%
    filter(major_grouping == &#39;Individual dishes&#39;) %&amp;gt;%
    group_by(minor_grouping) %&amp;gt;%
    count() 

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create edges and nodes (i.e., from and to) to create the sub-groupings within Individual Dishes (i.e., Rice, Noodles and Misc):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Individual Dishes ----

# data: edge list
d1 &amp;lt;- data.frame(from=&amp;quot;Individual dishes&amp;quot;, to=c(&amp;quot;Misc Indiv&amp;quot;, &amp;quot;Noodle dishes&amp;quot;, &amp;quot;Rice dishes&amp;quot;))

d2 &amp;lt;- df %&amp;gt;%
    select(minor_grouping, Thai_name) %&amp;gt;%
    slice(1:53) %&amp;gt;%
    rename(
        from = minor_grouping,
        to = Thai_name
    ) 

edges &amp;lt;- rbind(d1, d2)

# plot dendrogram (idividual dishes)
indiv_dishes_graph &amp;lt;- graph_from_data_frame(edges)

ggraph(indiv_dishes_graph, layout = &amp;quot;dendrogram&amp;quot;, circular = FALSE) +
    geom_edge_diagonal(aes(edge_colour = edges$from), label_dodge = NULL) +
    geom_node_text(aes(label = name, filter = leaf, color = &#39;red&#39;), hjust = 1.1, size = 3) +
    geom_node_point(color = &amp;quot;whitesmoke&amp;quot;) +
    theme(
        plot.background = element_rect(fill = &#39;#343d46&#39;),
        panel.background = element_rect(fill = &#39;#343d46&#39;),
        legend.position = &#39;none&#39;,
        plot.title = element_text(colour = &#39;whitesmoke&#39;, face = &#39;bold&#39;, size = 25),
        plot.subtitle = element_text(colour = &#39;whitesmoke&#39;, face = &#39;bold&#39;),
        plot.caption = element_text(color = &#39;whitesmoke&#39;, face = &#39;italic&#39;)
    ) +
    labs(
        title = &#39;52 Alternatives to Pad Thai&#39;,
        subtitle = &#39;Individual Thai Dishes&#39;,
        caption = &#39;Data: Wikipedia | Graphic: @paulapivat&#39;
    ) +
    expand_limits(x = c(-1.5, 1.5), y = c(-0.8, 0.8)) +
    coord_flip() +
    annotate(&amp;quot;text&amp;quot;, x = 47, y = 1, label = &amp;quot;Miscellaneous (7)&amp;quot;, color = &amp;quot;#7CAE00&amp;quot;)+
    annotate(&amp;quot;text&amp;quot;, x = 31, y = 1, label = &amp;quot;Noodle Dishes (24)&amp;quot;, color = &amp;quot;#00C08B&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = 8, y = 1, label = &amp;quot;Rice Dishes (22)&amp;quot;, color = &amp;quot;#C77CFF&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = 26, y = 2, label = &amp;quot;Individual\nDishes&amp;quot;, color = &amp;quot;#F8766D&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;what-is-the-best-way-to-organized-the-different-dishes&#34;&gt;What is the best way to organized the different dishes?&lt;/h4&gt;
&lt;p&gt;There are approximately &lt;strong&gt;4X&lt;/strong&gt; as many &lt;em&gt;shared&lt;/em&gt; dishes as individual dishes, so the dendrogram should be &lt;strong&gt;circular&lt;/strong&gt; to fit the names of all dishes in one graphic.&lt;/p&gt;
&lt;p&gt;A wonderful resource I use regularly for these types of visuals is the 
&lt;a href=&#34;https://www.r-graph-gallery.com/339-circular-dendrogram-with-ggraph.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R Graph Gallery&lt;/a&gt;. There was a slight issue in how the &lt;strong&gt;text angles&lt;/strong&gt; were calculated so I submitted a 
&lt;a href=&#34;https://github.com/holtzy/R-graph-gallery/pull/34&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PR to fix&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Perhaps distinguishing between individual and shared dishes is too crude, within the dendrogram for 201 shared Thai dishes, we can see further sub-groupings including Curries, Sauces/Pastes, Steamed, Grilled, Deep-Fried, Fried &amp;amp; Stir-Fried, Salads, Soups and other Misc:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./shared_dishes_final.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Shared Dishes ----
df %&amp;gt;%
    select(major_grouping, minor_grouping, Thai_name, Thai_script) %&amp;gt;%
    filter(major_grouping == &#39;Shared dishes&#39;) %&amp;gt;%
    group_by(minor_grouping) %&amp;gt;%
    count() %&amp;gt;%
    arrange(desc(n))

d3 &amp;lt;- data.frame(from=&amp;quot;Shared dishes&amp;quot;, to=c(&amp;quot;Curries&amp;quot;, &amp;quot;Soups&amp;quot;, &amp;quot;Salads&amp;quot;,
                                            &amp;quot;Fried and stir-fried dishes&amp;quot;, &amp;quot;Deep-fried dishes&amp;quot;, &amp;quot;Grilled dishes&amp;quot;,
                                            &amp;quot;Steamed or blanched dishes&amp;quot;, &amp;quot;Stewed dishes&amp;quot;, &amp;quot;Dipping sauces and pastes&amp;quot;, &amp;quot;Misc Shared&amp;quot;))


d4 &amp;lt;- df %&amp;gt;%
    select(minor_grouping, Thai_name) %&amp;gt;%
    slice(54:254) %&amp;gt;%
    rename(
        from = minor_grouping,
        to = Thai_name
    )

edges2 &amp;lt;- rbind(d3, d4)

# create a vertices data.frame. One line per object of hierarchy
vertices = data.frame(
    name = unique(c(as.character(edges2$from), as.character(edges2$to)))
)

# add column with group of each name. Useful to later color points
vertices$group = edges2$from[ match(vertices$name, edges2$to)]

# Add information concerning the label we are going to add: angle, horizontal adjustment and potential flip
# calculate the ANGLE of the labels
vertices$id=NA
myleaves=which(is.na(match(vertices$name, edges2$from)))
nleaves=length(myleaves)
vertices$id[myleaves] = seq(1:nleaves)
vertices$angle = 360 / nleaves * vertices$id + 90    


# calculate the alignment of labels: right or left
vertices$hjust&amp;lt;-ifelse( vertices$angle &amp;lt; 275, 1, 0)



# flip angle BY to make them readable
vertices$angle&amp;lt;-ifelse(vertices$angle &amp;lt; 275, vertices$angle+180, vertices$angle)

# plot dendrogram (shared dishes)
shared_dishes_graph &amp;lt;- graph_from_data_frame(edges2)

ggraph(shared_dishes_graph, layout = &amp;quot;dendrogram&amp;quot;, circular = TRUE) +
    geom_edge_diagonal(aes(edge_colour = edges2$from), label_dodge = NULL) +
    geom_node_text(aes(x = x*1.15, y=y*1.15, filter = leaf, label=name, angle = vertices$angle, hjust= vertices$hjust, colour= vertices$group), size=2.7, alpha=1) +
    geom_node_point(color = &amp;quot;whitesmoke&amp;quot;) +
    theme(
        plot.background = element_rect(fill = &#39;#343d46&#39;),
        panel.background = element_rect(fill = &#39;#343d46&#39;),
        legend.position = &#39;none&#39;,
        plot.title = element_text(colour = &#39;whitesmoke&#39;, face = &#39;bold&#39;, size = 25),
        plot.subtitle = element_text(colour = &#39;whitesmoke&#39;, margin = margin(0,0,30,0), size = 20),
        plot.caption = element_text(color = &#39;whitesmoke&#39;, face = &#39;italic&#39;)
    ) +
    labs(
        title = &#39;Thai Food is Best Shared&#39;,
        subtitle = &#39;201 Ways to Make Friends&#39;,
        caption = &#39;Data: Wikipedia | Graphic: @paulapivat&#39;
    ) +
    #expand_limits(x = c(-1.5, 1.5), y = c(-0.8, 0.8)) +
    expand_limits(x = c(-1.5, 1.5), y = c(-1.5, 1.5)) +
    coord_flip() +
    annotate(&amp;quot;text&amp;quot;, x = 0.4, y = 0.45, label = &amp;quot;Steamed&amp;quot;, color = &amp;quot;#F564E3&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = 0.2, y = 0.5, label = &amp;quot;Grilled&amp;quot;, color = &amp;quot;#00BA38&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = -0.2, y = 0.5, label = &amp;quot;Deep-Fried&amp;quot;, color = &amp;quot;#DE8C00&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = -0.4, y = 0.1, label = &amp;quot;Fried &amp;amp;\n Stir-Fried&amp;quot;, color = &amp;quot;#7CAE00&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = -0.3, y = -0.4, label = &amp;quot;Salads&amp;quot;, color = &amp;quot;#00B4F0&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = -0.05, y = -0.5, label = &amp;quot;Soups&amp;quot;, color = &amp;quot;#C77CFF&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = 0.3, y = -0.5, label = &amp;quot;Curries&amp;quot;, color = &amp;quot;#F8766D&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = 0.5, y = -0.1, label = &amp;quot;Misc&amp;quot;, color = &amp;quot;#00BFC4&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = 0.5, y = 0.1, label = &amp;quot;Sauces\nPastes&amp;quot;, color = &amp;quot;#B79F00&amp;quot;)
    
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;text_mining&#34;&gt;Text_Mining&lt;/h3&gt;
&lt;h4 id=&#34;which-raw-materials-are-most-popular&#34;&gt;Which raw material(s) are most popular?&lt;/h4&gt;
&lt;p&gt;One way to answer this question is to use text mining to &lt;strong&gt;tokenize&lt;/strong&gt; by either word and count the words by frequency as one measure of popularity.&lt;/p&gt;
&lt;p&gt;In the below bar chart, we see frequency of words across all Thai Dishes. &lt;strong&gt;Mu&lt;/strong&gt; (à¸«à¸¡à¸¹) which means pork in Thai appears most frequently across all dish types and sub-grouping. Next we have &lt;strong&gt;kaeng&lt;/strong&gt; (à¹à¸à¸) which means curry. &lt;strong&gt;Phat&lt;/strong&gt; (à¸à¸±à¸) comings in third suggesting &amp;ldquo;stir-fry&amp;rdquo; is a popular cooking mode.&lt;/p&gt;
&lt;p&gt;As we can see &lt;strong&gt;not&lt;/strong&gt; all words refer to raw materials, so we may not be able to answer this question directly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./word_freq_barchart.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;library(tidytext)
library(scales)

# new csv file after data cleaning (see above)
df &amp;lt;- read_csv(&amp;quot;../web_scraping/edit_thai_dishes.csv&amp;quot;)

df %&amp;gt;%
    select(Thai_name, Thai_script) %&amp;gt;%
    # can substitute &#39;word&#39; for ngrams, sentences, lines
    unnest_tokens(ngrams, Thai_name) %&amp;gt;%  
    # to reference thai spelling: group_by(Thai_script)
    group_by(ngrams) %&amp;gt;%  
    tally(sort = TRUE) %&amp;gt;%  # alt: count(sort = TRUE)
    filter(n &amp;gt; 9) %&amp;gt;%
# visualize
# pipe directly into ggplot2, because using tidytools
    ggplot(aes(x = n, y = reorder(ngrams, n))) + 
    geom_col(aes(fill = ngrams)) +
    scale_fill_manual(values = c(
        &amp;quot;#c3d66b&amp;quot;,
        &amp;quot;#70290a&amp;quot;,
        &amp;quot;#2f1c0b&amp;quot;,
        &amp;quot;#ba9d8f&amp;quot;,
        &amp;quot;#dda37b&amp;quot;,
        &amp;quot;#8f5e23&amp;quot;,
        &amp;quot;#96b224&amp;quot;,
        &amp;quot;#dbcac9&amp;quot;,
        &amp;quot;#626817&amp;quot;,
        &amp;quot;#a67e5f&amp;quot;,
        &amp;quot;#be7825&amp;quot;,
        &amp;quot;#446206&amp;quot;,
        &amp;quot;#c8910b&amp;quot;,
        &amp;quot;#88821b&amp;quot;,
        &amp;quot;#313d5f&amp;quot;,
        &amp;quot;#73869a&amp;quot;,
        &amp;quot;#6f370f&amp;quot;,
        &amp;quot;#c0580d&amp;quot;,
        &amp;quot;#e0d639&amp;quot;,
        &amp;quot;#c9d0ce&amp;quot;,
        &amp;quot;#ebf1f0&amp;quot;,
        &amp;quot;#50607b&amp;quot;
    )) +
    theme_minimal() +
    theme(legend.position = &amp;quot;none&amp;quot;) +
    labs(
        x = &amp;quot;Frequency&amp;quot;,
        y = &amp;quot;Words&amp;quot;,
        title = &amp;quot;Frequency of Words in Thai Cuisine&amp;quot;,
        subtitle = &amp;quot;Words appearing at least 10 times in Individual or Shared Dishes&amp;quot;,
        caption = &amp;quot;Data: Wikipedia | Graphic: @paulapivat&amp;quot;
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also see words common to both Individual and Shared Dishes. We see other words like &lt;strong&gt;nuea&lt;/strong&gt; (beef), &lt;strong&gt;phrik&lt;/strong&gt; (chili) and &lt;strong&gt;kaphrao&lt;/strong&gt; (basil leaves).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./word_freq_indiv_shared_dishes.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# frequency for Thai_dishes (Major Grouping) ----

# comparing Individual and Shared Dishes (Major Grouping)
thai_name_freq &amp;lt;- df %&amp;gt;%
    select(Thai_name, Thai_script, major_grouping) %&amp;gt;%
    unnest_tokens(ngrams, Thai_name) %&amp;gt;% 
    count(ngrams, major_grouping) %&amp;gt;%
    group_by(major_grouping) %&amp;gt;%
    mutate(proportion = n / sum(n)) %&amp;gt;%
    select(major_grouping, ngrams, proportion) %&amp;gt;%
    spread(major_grouping, proportion) %&amp;gt;%
    gather(major_grouping, proportion, c(`Shared dishes`)) %&amp;gt;%
    select(ngrams, `Individual dishes`, major_grouping, proportion)


# Expect warming message about missing values
ggplot(thai_name_freq, aes(x = proportion, y = `Individual dishes`,
       color = abs(`Individual dishes` - proportion))) +
    geom_abline(color = &#39;gray40&#39;, lty = 2) +
    geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
    geom_text(aes(label = ngrams), check_overlap = TRUE, vjust = 1.5) +
    scale_x_log10(labels = percent_format()) +
    scale_y_log10(labels = percent_format()) +
    scale_color_gradient(limits = c(0, 0.01), 
                         low = &amp;quot;red&amp;quot;, high = &amp;quot;blue&amp;quot;) +    # low = &amp;quot;darkslategray4&amp;quot;, high = &amp;quot;gray75&amp;quot;
    theme_minimal() +
    theme(legend.position = &amp;quot;none&amp;quot;,
          legend.text = element_text(angle = 45, hjust = 1)) +
    labs(y = &amp;quot;Individual Dishes&amp;quot;,
         x = &amp;quot;Shared Dishes&amp;quot;,
         color = NULL,
         title = &amp;quot;Comparing Word Frequencies in the names Thai Dishes&amp;quot;,
         subtitle = &amp;quot;Individual and Shared Dishes&amp;quot;,
         caption = &amp;quot;Data: Wikipedia | Graphics: @paulapivat&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;which-raw-materials-are-most-important&#34;&gt;Which raw materials are most important?&lt;/h4&gt;
&lt;p&gt;We can only learn so much from frequency, so text mining practitioners have created &lt;strong&gt;term frequency - inverse document frequency&lt;/strong&gt; to better reflect how important a word is in a document or corpus (further details 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Again, the words don&amp;rsquo;t necessarily refer to raw materials, so this question can&amp;rsquo;t be fully answered directly here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./td_idf_thai_dishes.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;could-you-learn-about-thai-food-just-from-the-names-of-the-dishes&#34;&gt;Could you learn about Thai food just from the names of the dishes?&lt;/h4&gt;
&lt;p&gt;The short answer is &amp;ldquo;yes&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We learned just from frequency and &amp;ldquo;term frequency - inverse document frequency&amp;rdquo; not only the most frequent words, but the relative importance within the current set of words that we have tokenized with &lt;code&gt;tidytext&lt;/code&gt;. This informs us of not only popular raw materials (Pork), but also dish types (Curries) and other popular mode of preparation (Stir-Fry).&lt;/p&gt;
&lt;p&gt;We can even examine the &lt;strong&gt;network of relationships&lt;/strong&gt; between words. Darker arrows suggest a stronger relationship between pairs of words, for example &amp;ldquo;nam phrik&amp;rdquo; is a strong pairing. This means &amp;ldquo;chili sauce&amp;rdquo; in Thai and suggests the important role that it plays across many types of dishes.&lt;/p&gt;
&lt;p&gt;We learned above that &amp;ldquo;mu&amp;rdquo; (pork) appears frequently. Now we see that &amp;ldquo;mu&amp;rdquo; and &amp;ldquo;krop&amp;rdquo; are more related than other pairings (note: &amp;ldquo;mu krop&amp;rdquo; means &amp;ldquo;crispy pork&amp;rdquo;). We also saw above that &amp;ldquo;khao&amp;rdquo; appears frequently in Rice dishes. This alone is not surprising as &amp;ldquo;khao&amp;rdquo; means rice in Thai, but we see here &amp;ldquo;khao phat&amp;rdquo; is strongly related suggesting that fried rice (&amp;ldquo;khao phat&amp;rdquo;) is quite popular.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./network_thai_dishes.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Visualizing a network of Bi-grams with {ggraph} ----
library(igraph)
library(ggraph)
set.seed(2021)

thai_dish_bigram_counts &amp;lt;- df %&amp;gt;%
    select(Thai_name, minor_grouping) %&amp;gt;%
    unnest_tokens(bigram, Thai_name, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
    separate(bigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;%
    count(word1, word2, sort = TRUE)


# filter for relatively common combinations (n &amp;gt; 2)
thai_dish_bigram_graph &amp;lt;- thai_dish_bigram_counts %&amp;gt;%
    filter(n &amp;gt; 2) %&amp;gt;%
    graph_from_data_frame()


# polishing operations to make a better looking graph
a &amp;lt;- grid::arrow(type = &amp;quot;closed&amp;quot;, length = unit(.15, &amp;quot;inches&amp;quot;))

set.seed(2021)
ggraph(thai_dish_bigram_graph, layout = &amp;quot;fr&amp;quot;) +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                   arrow = a, end_cap = circle(.07, &#39;inches&#39;)) +
    geom_node_point(color = &amp;quot;dodgerblue&amp;quot;, size = 5, alpha = 0.7) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    labs(
        title = &amp;quot;Network of Relations between Word Pairs&amp;quot;,
        subtitle = &amp;quot;{ggraph}: common nodes in Thai food&amp;quot;,
        caption = &amp;quot;Data: Wikipedia | Graphics: @paulapivat&amp;quot;
    ) +
    theme_void()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we may be interested in word relationships &lt;em&gt;within&lt;/em&gt; individual dishes.&lt;/p&gt;
&lt;p&gt;The below graph shows a network of word pairs with moderate-to-high correlations. We can see certain words clustered near each other with relatively dark lines: kaeng (curry), pet (spicy), wan (sweet), khiao (green curry), phrik (chili) and mu (pork). These words represent a collection of ingredient, mode of cooking and description that are generally combined.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./indiv_dish_corr.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;set.seed(2021)

# Individual Dishes
individual_dish_words &amp;lt;- df %&amp;gt;%
    select(major_grouping, Thai_name) %&amp;gt;%
    filter(major_grouping == &#39;Individual dishes&#39;) %&amp;gt;%
    mutate(section = row_number() %/% 10) %&amp;gt;%
    filter(section &amp;gt; 0) %&amp;gt;%
    unnest_tokens(word, Thai_name)  # assume no stop words

individual_dish_cors &amp;lt;- individual_dish_words %&amp;gt;%
    group_by(word) %&amp;gt;% 
    filter(n() &amp;gt;= 2) %&amp;gt;%     # looking for co-occuring words, so must be 2 or greater
    pairwise_cor(word, section, sort = TRUE) 


individual_dish_cors %&amp;gt;%
    filter(correlation &amp;lt; -0.40) %&amp;gt;%
    graph_from_data_frame() %&amp;gt;%
    ggraph(layout = &amp;quot;fr&amp;quot;) +
    geom_edge_link(aes(edge_alpha = correlation, size = correlation), show.legend = TRUE) +
    geom_node_point(color = &amp;quot;green&amp;quot;, size = 5, alpha = 0.5) +
    geom_node_text(aes(label = name), repel = TRUE) +
    labs(
        title = &amp;quot;Word Pairs in Individual Dishes&amp;quot;,
        subtitle = &amp;quot;{ggraph}: Negatively correlated (r = -0.4)&amp;quot;,
        caption = &amp;quot;Data: Wikipedia | Graphics: @paulapivat&amp;quot;
    ) +
    theme_void()
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;p&gt;We have completed an exploratory data project where we scraped, clean, manipulated and visualized data using a combination of Python and R. We also used the &lt;code&gt;tidytext&lt;/code&gt; package for basic text mining task to see if we could gain some insights into Thai cuisine using  words from dish names scraped off Wikipedia.&lt;/p&gt;
&lt;p&gt;For more content on data science, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The More a Network is Worth, the Harder it is to Attack</title>
      <link>/post/crypto_price_attack/</link>
      <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/post/crypto_price_attack/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#implication&#34;&gt;Implication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, I use Python and R to access, parse, manipulate, then visualize data from 
&lt;a href=&#34;https://www.crypto51.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Crypto51.app&lt;/a&gt; to show the strong relationship between Market Capitalization and Cost to Attack among public crypto networks.&lt;/p&gt;
&lt;p&gt;The more a network is thought to be worth, the more expensive it is to attack. An important, but often overlooked reason to celebrate price gains.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;In this post, I query an API endpoint setup at 
&lt;a href=&#34;https://www.crypto51.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Crypto51.app&lt;/a&gt; to get &lt;code&gt;JSON&lt;/code&gt; data. Then, I use Python to parse and convert to &lt;code&gt;dataframe&lt;/code&gt;. Finally, I use R to wrangle and visualize.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get it!&lt;/p&gt;
&lt;p&gt;Here is the Python code to read in &lt;code&gt;JSON&lt;/code&gt; and convert to a &lt;code&gt;dataframe&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import json
import requests

r = requests.get(&#39;https://api.crypto51.app/coins.json&#39;)
dct = dict()
dct = r.json()

# loop through:
# last_updated
# coins
for x, y in dct.items():
    print(x)

type(dct[&#39;coins&#39;]) # list
len(dct[&#39;coins&#39;])  # 57 dictionaries in side this list

# convert list of 57 dictionaries into a pandas dataframe
df = pd.DataFrame.from_dict(dct[&#39;coins&#39;])
df.head()
df.to_csv(&#39;crypto51.csv&#39;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After creating a CSV, I&amp;rsquo;m transition to R, out of preference for &lt;code&gt;dataframe&lt;/code&gt; manipulation and visualization with this tool (you could do the following in &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;seaborn&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll load the &lt;code&gt;tidyverse&lt;/code&gt; and read in the CSV file we created. Then we&amp;rsquo;ll use a series of &lt;code&gt;magrittr&lt;/code&gt; 
&lt;a href=&#34;https://magrittr.tidyverse.org/reference/pipe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pipes&lt;/a&gt; to sequence our data manipulation in one flow. We&amp;rsquo;ll remove projects with &lt;strong&gt;no market_cap&lt;/strong&gt; data. We&amp;rsquo;ll remove the Handshake project because of missing data for &lt;code&gt;attack_hourly_cost&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll change &lt;code&gt;attack_hourly_cost&lt;/code&gt; data type into numeric. Then we&amp;rsquo;ll use &lt;code&gt;ggplot2&lt;/code&gt; to visualize a scatter plot with both X and Y axes transformed with &lt;code&gt;scale_*_log10()&lt;/code&gt; to make the scatter plot more interpretable.&lt;/p&gt;
&lt;p&gt;Bitcoin and Ethereum are annotated as the two leading projects (see chart below).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;library(tidyverse)

df &amp;lt;- read_csv(&amp;quot;crypto51.csv&amp;quot;)

df %&amp;gt;%
    # remove projects with no market_cap
    slice(1:38) %&amp;gt;% 
    filter(attack_hourly_cost != &amp;quot;?&amp;quot;) %&amp;gt;% 
    # change character to numeric
    mutate(
        attack_hourly_cost = as.numeric(attack_hourly_cost)
    ) %&amp;gt;% 
    ggplot(aes(x=market_cap, y=attack_hourly_cost)) +
    geom_point(aes(size = log10(market_cap)), color = &amp;quot;white&amp;quot;, alpha = 0.8) +
    # use log10 transformation to make chart more interpretable
    scale_y_log10(label= scales::dollar) +
    scale_x_log10(label= scales::dollar) +
    theme_minimal() +
    theme(
        legend.position = &#39;none&#39;,
        panel.background = element_rect(fill = &amp;quot;dodger blue&amp;quot;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_rect(fill = &amp;quot;dodger blue&amp;quot;),
        plot.title = element_text(colour = &amp;quot;white&amp;quot;, face = &amp;quot;bold&amp;quot;, size = 30, 
                                  margin = margin(10,0,30,0)),
        plot.caption = element_text(color = &amp;quot;white&amp;quot;),
        axis.title = element_text(colour = &amp;quot;white&amp;quot;, face = &amp;quot;bold&amp;quot;),
        axis.title.x = element_text(margin = margin(30,0,10,0)),
        axis.text = element_text(colour = &amp;quot;white&amp;quot;, face = &amp;quot;bold&amp;quot;),
        axis.title.y = element_text(margin = margin(0,20,0,30), angle = 0)
    ) +
    labs(
        x = &amp;quot;Market Capitalization&amp;quot;,
        y = &amp;quot;Attack\nHourly\nCost&amp;quot;,
        title = &amp;quot;The More a Crypto Network is Worth,\n the Harder it is to Attack.&amp;quot;,
        caption = &amp;quot;Data: www.crypto51.app | Graphics: @paulapivat&amp;quot;
    ) +
    # annotate instead of geom_text
    annotate(&amp;quot;text&amp;quot;, x = 205174310335, y = 800000, label = &amp;quot;Bitcoin&amp;quot;, color = &amp;quot;white&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = 30762751140, y = 418437, label = &amp;quot;Ethereum&amp;quot;, color = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;implication&#34;&gt;Implication&lt;/h2&gt;
&lt;h3 id=&#34;the-more-a-crypto-network-is-worth-the-harder-it-is-to-attack&#34;&gt;The More a Crypto Network is Worth, the Harder it is to Attack&lt;/h3&gt;
&lt;p&gt;All time high.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;d be hard pressed to find three more delicious words than these.&lt;/p&gt;
&lt;p&gt;When it comes to crypto, everyone keeps an eye on their portfolio value.&lt;/p&gt;
&lt;p&gt;Your bags aside, there is &lt;em&gt;another&lt;/em&gt; reason to celebrate price gains.&lt;/p&gt;
&lt;p&gt;Bitcoin&amp;rsquo;s big innovation was making &lt;strong&gt;digital transaction difficult to replicate&lt;/strong&gt; (unlike most digital files that are easily duplicated).&lt;/p&gt;
&lt;p&gt;Nodes follow the longest chain as the &amp;ldquo;correct&amp;rdquo; chain. However, this opens things up for any node(s) with more than 51% of the network hashing power to pull &lt;em&gt;shenanigans&lt;/em&gt;, such as &lt;strong&gt;double-spending&lt;/strong&gt;. Sending funds to one address on the main chain and the same funds to another address on a different chain.&lt;/p&gt;
&lt;p&gt;More hardware and hash power allow a node to secretly mine a side chain, which they can later âfoolâ the rest of the network into accepting.&lt;/p&gt;
&lt;p&gt;Since their inception, Bitcoin and Ethereum have gotten more difficult to mine over time. And when price increases, the capital costs of buying new equipment goes up.&lt;/p&gt;
&lt;p&gt;This makes it more difficult for any one entity to accumulate too much hash power and pull shenanigans. &lt;strong&gt;As a result, the entire network is more secure&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In fact, the data provided by crypto51.app suggests a near perfect correlation between Market Capitalization and Cost to Attack, at r = 0.94.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./price_nb3.png&#34; alt=&#34;Crypto Price Attack&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The more a crypto network is worth, the more expensive it is to attack.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another reason to celebrate price gains.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.crypto51.app&#34;&gt;www.crypto51.app&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Positive are Your Facebook Posts?</title>
      <link>/post/sentiment_analysis/</link>
      <pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/sentiment_analysis/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#getting_data&#34;&gt;Getting Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#tokenization&#34;&gt;Tokenization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#normalizing_sentences&#34;&gt;Normalizing Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#frequency&#34;&gt;Frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#sentiment_analysis&#34;&gt;Sentiment Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#data_transformation&#34;&gt;Data Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#visualization&#34;&gt;Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h4 id=&#34;why-sentiment-analysis&#34;&gt;Why Sentiment Analysis?&lt;/h4&gt;
&lt;p&gt;NLP is subfield of linguistic, computer science and artificial intelligence (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_language_processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wiki&lt;/a&gt;), and you could spend years studying it.&lt;/p&gt;
&lt;p&gt;However, I wanted a quick dive to a get an intuition for how NLP works, and we&amp;rsquo;ll do that via &lt;strong&gt;sentiment analysis&lt;/strong&gt;, categorizing text by their polarity.&lt;/p&gt;
&lt;p&gt;We can&amp;rsquo;t help but feel motivated to see insights about our &lt;em&gt;own&lt;/em&gt; social media post, so we&amp;rsquo;ll turn to a well known platform.&lt;/p&gt;
&lt;h4 id=&#34;how-well-does-facebook-know-us&#34;&gt;How well does Facebook know us?&lt;/h4&gt;
&lt;p&gt;To find out, I downloaded 14 years of posts to apply &lt;strong&gt;text&lt;/strong&gt; and &lt;strong&gt;sentiment&lt;/strong&gt; analysis. We&amp;rsquo;l use &lt;code&gt;Python&lt;/code&gt; to read and parse &lt;code&gt;json&lt;/code&gt; data from Facebook.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll perform tasks such as tokenization and normalization aided by Python&amp;rsquo;s &lt;strong&gt;Natural Language Toolkit&lt;/strong&gt;, &lt;code&gt;NLTK&lt;/code&gt;. Then, we&amp;rsquo;ll use the &lt;code&gt;Vader&lt;/code&gt; module (Hutto &amp;amp; Gilbert, 2014) for rule-based (lexicon) &lt;strong&gt;sentiment analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll transition our work flow to &lt;code&gt;R&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; for &lt;strong&gt;data manipulation&lt;/strong&gt; and &lt;strong&gt;visualization&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;getting_data&#34;&gt;Getting_Data&lt;/h2&gt;
&lt;p&gt;First, you&amp;rsquo;ll need to download your own Facebook data by following: Setting &amp;amp; Privacy &amp;gt; Setting &amp;gt; Your Facebook Information &amp;gt; Download Your Information &amp;gt; (select) Posts.&lt;/p&gt;
&lt;p&gt;Below, I named my file &lt;code&gt;your_posts_1.json&lt;/code&gt;, but you can change this.
We&amp;rsquo;ll use Python&amp;rsquo;s &lt;code&gt;json&lt;/code&gt; module read in data. We can get a feel for the data with &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;len&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json

# load json into python, assign to &#39;data&#39;
with open(&#39;your_posts_1.json&#39;) as file:
    data = json.load(file)

type(data)     # a list
type(data[0])  # first object in the list: a dictionary
len(data)      # my list contains 2166 dictionaries
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the Python libraries we use in this post:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.stem import LancasterStemmer, WordNetLemmatizer      # OPTIONAL (more relevant for individual words)
from nltk.corpus import stopwords
from nltk.probability import FreqDist
import re
import unicodedata
import nltk
import json
import inflect
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nltk.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Language Tookkit&lt;/a&gt; is a popular Python platform for working with human language data. While it has over 50 lexical resources, we&amp;rsquo;ll use the 
&lt;a href=&#34;https://github.com/cjhutto/vaderSentiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vader Sentiment Lexicon&lt;/a&gt;, that is &lt;em&gt;specifically&lt;/em&gt; attuned to sentiments expressed in social media.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.python.org/3/library/re.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regex&lt;/a&gt; (regular expressions) will be used to remove punctuation.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.python.org/3/library/unicodedata.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unicode Database&lt;/a&gt; will be used to remove non-ASCII characters.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.python.org/3/library/json.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JSON&lt;/a&gt; module helps us to read in json from Facebook.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://pypi.org/project/inflect/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inflect&lt;/a&gt; helps us to convert numbers to words.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pandas&lt;/a&gt; is a powerful data manipulation and data analysis tool for when we save our text data into a data frame and write to csv.&lt;/p&gt;
&lt;p&gt;After we have our data, we&amp;rsquo;ll 
&lt;a href=&#34;https://twitter.com/paulapivat/status/1352893979897909251?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dig through&lt;/a&gt; to get actual &lt;strong&gt;text data&lt;/strong&gt; (our posts).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll store this in a list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the &lt;code&gt;data&lt;/code&gt; key occasionally returns an empty array and we want to skip over those by checking &lt;code&gt;if len(v) &amp;gt; 0&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create empty list
empty_lst = []

# multiple nested loops to store all post in empty list
for dct in data:
    for k, v in dct.items():
        if k == &#39;data&#39;:
            if len(v) &amp;gt; 0:
                for k_i, v_i in vee[0].items():  
                    if k_i == &#39;post&#39;:
                        empty_lst.append(v_i)

print(&amp;quot;This is the empty list: &amp;quot;, empty_lst)
print(&amp;quot;\nLength of list: &amp;quot;, len(empty_lst))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a list of strings.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./list_of_strings.png&#34; alt=&#34;list_of_strings&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll loop through our list of strings (empty_lst) to tokenize each &lt;em&gt;sentence&lt;/em&gt; with &lt;code&gt;nltk.sent_tokenize()&lt;/code&gt;. We want to split the text into individual sentences.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./token_list_of_strings.png&#34; alt=&#34;token_list_of_strings&#34;&gt;&lt;/p&gt;
&lt;p&gt;This yields a list of list, which we&amp;rsquo;ll flatten:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# - list of list, len: 1762 (each list contain sentences)
nested_sent_token = [nltk.sent_tokenize(lst) for lst in empty_lst]

# flatten list, len: 3241
flat_sent_token = [item for sublist in nested_sent_token for item in sublist]
print(&amp;quot;Flatten sentence token: &amp;quot;, len(flat_sent_token))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;normalizing_sentences&#34;&gt;Normalizing_Sentences&lt;/h2&gt;
&lt;p&gt;For context on the functions used in this section, check out this article by Matthew Mayo on 
&lt;a href=&#34;https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Text Data Preprocessing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, we&amp;rsquo;ll remove non-ASCII characters (&lt;code&gt;remove_non_ascii(words)&lt;/code&gt;) including: &lt;code&gt;#&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;&#39;&lt;/code&gt; and &lt;code&gt;?&lt;/code&gt;, among many others. Then we&amp;rsquo;ll lowercase (&lt;code&gt;to_lowercase(words)&lt;/code&gt;), remove punctuation (&lt;code&gt;remove_punctuation(words)&lt;/code&gt;), replace numbers (&lt;code&gt;replace_numbers(words)&lt;/code&gt;), and remove stopwords (&lt;code&gt;remove_stopwords(words)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Example stopwords are: your, yours, yourself, yourselves, he, him, his, himself etc.&lt;/p&gt;
&lt;p&gt;This allows us to have each sentence be on equal playing field.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove Non-ASCII
def remove_non_ascii(words):
    &amp;quot;&amp;quot;&amp;quot;Remove non-ASCII character from List of tokenized words&amp;quot;&amp;quot;&amp;quot;
    new_words = []
    for word in words:
        new_word = unicodedata.normalize(&#39;NFKD&#39;, word).encode(
            &#39;ascii&#39;, &#39;ignore&#39;).decode(&#39;utf-8&#39;, &#39;ignore&#39;)
        new_words.append(new_word)
    return new_words


# To LowerCase
def to_lowercase(words):
    &amp;quot;&amp;quot;&amp;quot;Convert all characters to lowercase from List of tokenized words&amp;quot;&amp;quot;&amp;quot;
    new_words = []
    for word in words:
        new_word = word.lower()
        new_words.append(new_word)
    return new_words


# Remove Punctuation , then Re-Plot Frequency Graph
def remove_punctuation(words):
    &amp;quot;&amp;quot;&amp;quot;Remove punctuation from list of tokenized words&amp;quot;&amp;quot;&amp;quot;
    new_words = []
    for word in words:
        new_word = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, word)
        if new_word != &#39;&#39;:
            new_words.append(new_word)
    return new_words


# Replace Numbers with Textual Representations
def replace_numbers(words):
    &amp;quot;&amp;quot;&amp;quot;Replace all interger occurrences in list of tokenized words with textual representation&amp;quot;&amp;quot;&amp;quot;
    p = inflect.engine()
    new_words = []
    for word in words:
        if word.isdigit():
            new_word = p.number_to_words(word)
            new_words.append(new_word)
        else:
            new_words.append(word)
    return new_words

# Remove Stopwords
def remove_stopwords(words):
    &amp;quot;&amp;quot;&amp;quot;Remove stop words from list of tokenized words&amp;quot;&amp;quot;&amp;quot;
    new_words = []
    for word in words:
        if word not in stopwords.words(&#39;english&#39;):
            new_words.append(word)
    return new_words
    
# Combine all functions into Normalize() function
def normalize(words):
    words = remove_non_ascii(words)
    words = to_lowercase(words)
    words = remove_punctuation(words)
    words = replace_numbers(words)
    words = remove_stopwords(words)
    return words
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The below screen cap gives us an idea of the difference between sentence &lt;strong&gt;normalization&lt;/strong&gt; vs &lt;strong&gt;non-normalization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./normal_v_non.png&#34; alt=&#34;normal_v_non&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sents = normalize(flat_sent_token)
print(&amp;quot;Length of sentences list: &amp;quot;, len(sents))   # 3194
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The process of stemming and lemmatization makes more sense for individuals words (over sentences), so we won&amp;rsquo;t use them here.&lt;/p&gt;
&lt;h2 id=&#34;frequency&#34;&gt;Frequency&lt;/h2&gt;
&lt;p&gt;You can use the &lt;code&gt;FreqDist()&lt;/code&gt; function to get the most common sentences. Then, you could plot a line chart for a visual comparison of the most frequent sentences.&lt;/p&gt;
&lt;p&gt;Although simple, counting frequencies can yield some 
&lt;a href=&#34;https://twitter.com/paulapivat/status/1353704114467729408?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;insights&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from nltk.probability import FreqDist

# Find frequency of sentence
fdist_sent = FreqDist(sents)
fdist_sent.most_common(10)   

# Plot
fdist_sent.plot(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sentiment_analysis&#34;&gt;Sentiment_Analysis&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll use the &lt;code&gt;Vader&lt;/code&gt; module from &lt;code&gt;NLTK&lt;/code&gt;. Vader stands for:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Valence, Aware, Dictionary and sEntiment Reasoner.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are taking a &lt;strong&gt;Rule-based/Lexicon&lt;/strong&gt; approach to sentiment analysis because we have a fairly large dataset, but lack labeled data to build a robust training set. Thus, Machine Learning would &lt;strong&gt;not&lt;/strong&gt; be ideal for this task.&lt;/p&gt;
&lt;p&gt;To get an intuition for how the &lt;code&gt;Vader&lt;/code&gt; module works, we can visit the github repo to view &lt;code&gt;vader_lexicon.txt&lt;/code&gt; (
&lt;a href=&#34;https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;). This is a &lt;strong&gt;dictionary&lt;/strong&gt; that has been empirically validated. Sentiment ratings are provided by 10 independent human raters (pre-screened, trained and checked for inter-rater reliability).&lt;/p&gt;
&lt;p&gt;Scores range from (-4) Extremely Negative to (4) Extremely Positive, with (0) as Neutral. For example, &amp;ldquo;die&amp;rdquo; is rated -2.9, while &amp;ldquo;dignified&amp;rdquo; has a 2.2 rating. For more details visit their (
&lt;a href=&#34;https://github.com/cjhutto/vaderSentiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll create two empty lists to store the sentences and the polarity scores, separately.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sentiment&lt;/code&gt; captures each sentence and &lt;code&gt;sent_scores&lt;/code&gt;, which initializes the &lt;code&gt;nltk.sentiment.vader.SentimentIntensityAnalyzer&lt;/code&gt; to calculate &lt;strong&gt;polarity_score&lt;/strong&gt; of each sentence (i.e., negative, neutral, positive).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sentiment2&lt;/code&gt; captures each polarity and value in a list of tuples.&lt;/p&gt;
&lt;p&gt;The below screen cap should give you a sense of what we have:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./sentiment_2.png&#34; alt=&#34;sentiment_2&#34;&gt;&lt;/p&gt;
&lt;p&gt;After we have appended each sentence (&lt;code&gt;sentiment&lt;/code&gt;) and their polarity scores (&lt;code&gt;sentiment2&lt;/code&gt;, negative, neutral, positive), we&amp;rsquo;ll &lt;strong&gt;create data frames&lt;/strong&gt; to store these values.&lt;/p&gt;
&lt;p&gt;Then, we&amp;rsquo;ll write the data frames to &lt;strong&gt;CSV&lt;/strong&gt; to transition to &lt;code&gt;R&lt;/code&gt;. Note that we set index to false when saving for CSV. Python starts counting at 0, while &lt;code&gt;R&lt;/code&gt; starts at 1, so we&amp;rsquo;re better off re-creating the index as a separate column in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: There are more efficient ways for what I&amp;rsquo;m doing here. My solution is to save two CSV files and move the work flow over to &lt;code&gt;R&lt;/code&gt; for further data manipulation and visualization. This is primarily a personal preference for handling data frames and visualizations in &lt;code&gt;R&lt;/code&gt;, but I should point out this &lt;em&gt;can&lt;/em&gt; be done with &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# nltk.download(&#39;vader_lexicon&#39;)

sid = SentimentIntensityAnalyzer()

sentiment = []
sentiment2 = []

for sent in sents:
    sent1 = sent
    sent_scores = sid.polarity_scores(sent1)
    for x, y in sent_scores.items():
        sentiment2.append((x, y))
    sentiment.append((sent1, sent_scores))
    # print(sentiment)

# sentiment
cols = [&#39;sentence&#39;, &#39;numbers&#39;]
result = pd.DataFrame(sentiment, columns=cols)
print(&amp;quot;First five rows of results: &amp;quot;, result.head())

# sentiment2
cols2 = [&#39;label&#39;, &#39;values&#39;]
result2 = pd.DataFrame(sentiment2, columns=cols2)
print(&amp;quot;First five rows of results2: &amp;quot;, result2.head())

# save to CSV
result.to_csv(&#39;sent_sentiment.csv&#39;, index=False)
result2.to_csv(&#39;sent_sentiment_2.csv&#39;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data_transformation&#34;&gt;Data_Transformation&lt;/h2&gt;
&lt;p&gt;From this point forward, we&amp;rsquo;ll be using &lt;code&gt;R&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; for data manipulation and visualization. &lt;code&gt;RStudio&lt;/code&gt; is the IDE of choice here. We&amp;rsquo;ll create an &lt;code&gt;R Script&lt;/code&gt; to store all our data transformation and visualization process. We should be in the same directory in which the above CSV files were created with &lt;code&gt;pandas&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll load the two CSV files we saved and the &lt;code&gt;tidyverse&lt;/code&gt; library:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)

# load data
df &amp;lt;- read_csv(&amp;quot;sent_sentiment.csv&amp;quot;)       
df2 &amp;lt;- read_csv(&#39;sent_sentiment_2.csv&#39;)    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll create another column that matches the index for the first data frame (sent_sentiment.csv). I save it as &lt;code&gt;df1&lt;/code&gt;, but you could overwrite the original &lt;code&gt;df&lt;/code&gt; if you wanted.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create a unique identifier for each sentence
df1 &amp;lt;- df %&amp;gt;%
    mutate(row = row_number())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, for the second data frame (sent_sentiment_2.csv), we&amp;rsquo;ll create another column matching the index, but also use &lt;code&gt;pivot_wider&lt;/code&gt; from the &lt;code&gt;tidyr&lt;/code&gt; package. &lt;strong&gt;NOTE&lt;/strong&gt;: You&amp;rsquo;ll want to &lt;code&gt;group_by&lt;/code&gt; label first, then use &lt;code&gt;mutate&lt;/code&gt; to create a unique identifier.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll then use &lt;code&gt;pivot_wider&lt;/code&gt; to ensure that all polarity values (negative, neutral, positive) have their own columns.&lt;/p&gt;
&lt;p&gt;By creating a unique identifier using &lt;code&gt;mutate&lt;/code&gt; and &lt;code&gt;row_number()&lt;/code&gt;, we&amp;rsquo;ll be able to join (&lt;code&gt;left_join&lt;/code&gt;) by row.&lt;/p&gt;
&lt;p&gt;Finally, I save the operation to &lt;code&gt;df3&lt;/code&gt; which allows me to work off a fresh new data frame for visualization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# long-to-wide for df2
# note: first, group by label; then, create a unique identifier for each label then use pivot_wider

df3 &amp;lt;- df2 %&amp;gt;%
    group_by(label) %&amp;gt;%
    mutate(row = row_number()) %&amp;gt;%
    pivot_wider(names_from = label, values_from = values) %&amp;gt;%
    left_join(df1, by = &#39;row&#39;) %&amp;gt;%
    select(row, sentence, neg:compound, numbers) 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;visualization&#34;&gt;Visualization&lt;/h2&gt;
&lt;p&gt;First, we&amp;rsquo;ll visualize the positive and negative polarity scores separately, across all 3194 sentences (your numbers will vary).&lt;/p&gt;
&lt;p&gt;Here are positivity scores:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./positivity_line.png&#34; alt=&#34;positivity_line&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here are negativity scores:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./negativity_line.png&#34; alt=&#34;negativity_line&#34;&gt;&lt;/p&gt;
&lt;p&gt;When I sum positivity and negativity scores to get a ratio, it&amp;rsquo;s approximately 568:97 or  5.8x more positive than negative according to the &lt;code&gt;Vader&lt;/code&gt; (Valance Aware Dictionary and Sentiment Reasoner).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Vader&lt;/code&gt; module will take in every sentence and assign a valence score from -1 (most negative) to 1 (most positive). We can classify sentences as &lt;code&gt;pos&lt;/code&gt; (positive), &lt;code&gt;neu&lt;/code&gt; (neutral) and &lt;code&gt;neg&lt;/code&gt;(negative) or as a composite (&lt;code&gt;compound&lt;/code&gt;) score (i.e., normalized, weighted composite score). For more details, see 
&lt;a href=&#34;https://pypi.org/project/vader-sentiment/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vader-sentiment documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here is a chart to see &lt;em&gt;both&lt;/em&gt; positive and negative scores together (positive = blue, negative = red, neutral = black).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./sentiment2.png&#34; alt=&#34;sentiment2.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, we can also use &lt;code&gt;histograms&lt;/code&gt; to see the distribution of negative and positive sentiment among the sentences:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./patch_histo.png&#34; alt=&#34;patch_histo&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;non-normalized-data&#34;&gt;Non-Normalized Data&lt;/h4&gt;
&lt;p&gt;It turns out the &lt;code&gt;Vader&lt;/code&gt; module is fully capable of analyzing sentences with punctuation, word-shape (capitalization for emphasis), slang and even utf-8 encoded emojis.&lt;/p&gt;
&lt;p&gt;So to see if there would be any difference if we implemented sentiment analysis &lt;strong&gt;without normalization&lt;/strong&gt;, I re-ran all the analyses above.&lt;/p&gt;
&lt;p&gt;Here are the two version of data for comparison. Top for normalization and bottom for non-normalized.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./two_version.png&#34; alt=&#34;two_version&#34;&gt;&lt;/p&gt;
&lt;p&gt;While there are expected slight differences, they are only slight.&lt;/p&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;p&gt;I downloaded 14 years worth of Facebook posts to run a rule-based sentiment analysis and visualize the results, using a combination of &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I enjoyed using both for this project and sought to play to their strengths. I found parsing JSON straight-forward with Python, but once we transition to data frames, I was itching to get back to R.&lt;/p&gt;
&lt;p&gt;Because we lacked labeled data, using a rule-based/lexicon-approach to sentiment analysis made sense. Now that we have a label for valence scores, it may be possible to take a machine learning approach to predict the valence of future posts.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Hutto, C.J. &amp;amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Your Twitter Data</title>
      <link>/post/twitter_analytics/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/twitter_analytics/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#exploring_relationships&#34;&gt;Exploring Relationships&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview--setup&#34;&gt;Overview &amp;amp; Setup&lt;/h2&gt;
&lt;p&gt;This post uses various R libraries and functions to help you explore your Twitter Analytics Data. The first thing to do is download data from 
&lt;a href=&#34;https://analytics.twitter.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analytics.twitter.com&lt;/a&gt;. The assumption here is that you&amp;rsquo;re already a Twitter user and have been using for at least 6 months.&lt;/p&gt;
&lt;p&gt;Once there, you&amp;rsquo;ll click on the &lt;code&gt;Tweets&lt;/code&gt; tab, which should bring you to your Tweet activity with the option to &lt;strong&gt;Export data&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./twitter_analytics.png&#34; alt=&#34;twitter_analytics&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once you click on &lt;strong&gt;Export data&lt;/strong&gt;, you&amp;rsquo;ll choose &amp;ldquo;By day&amp;rdquo;, which provides your impressions and engagements metrics for everyday (you&amp;rsquo;ll also select the time period, in the drop down menu right next to Export data - the default is &amp;ldquo;Last 28 Days&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The other option is to choose &amp;ldquo;By Tweet&amp;rdquo; and that will download the text of each Tweet along with associated metrics. We could potentially do fun text analysis with this, but we&amp;rsquo;ll save that for another post.&lt;/p&gt;
&lt;p&gt;For this post, I downloaded all &lt;em&gt;available&lt;/em&gt; data, which goes five months back.&lt;/p&gt;
&lt;p&gt;After downloading, you&amp;rsquo;ll want to &lt;strong&gt;read&lt;/strong&gt; in the data and, in our case, &lt;strong&gt;combine&lt;/strong&gt; all five months into one data frame, we&amp;rsquo;ll use the &lt;code&gt;readr&lt;/code&gt; package and &lt;code&gt;read_csv()&lt;/code&gt; function contained in &lt;code&gt;tidyverse&lt;/code&gt;. Then we&amp;rsquo;ll use &lt;code&gt;rbind()&lt;/code&gt; to combine five data frames by rows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)

# load data from September to mid-January
df1 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20200901_20201001_en.csv&amp;quot;)
df2 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201001_20201101_en.csv&amp;quot;)
df3 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201101_20201201_en.csv&amp;quot;)
df4 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201201_20210101_en.csv&amp;quot;)
df5 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20210101_20210112_en.csv&amp;quot;)

# combining ALL five dataframes into ONE, by rows
df &amp;lt;- rbind(df1, df2, df3, df4, df5)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;exploring_relationships&#34;&gt;Exploring_Relationships&lt;/h2&gt;
&lt;p&gt;Twitter analytics tracks several metric that are broadly grouped under Engagements, including: retweets, replies, likes, user profile clicks, url clicks, hashtag clicks, detail expands, media views and media engagements.&lt;/p&gt;
&lt;p&gt;There are other metrics like &amp;ldquo;app opens&amp;rdquo; and &amp;ldquo;promoted engagements&amp;rdquo;, which are services I have not used and so do not have any data available.&lt;/p&gt;
&lt;h4 id=&#34;a-guiding-question&#34;&gt;A Guiding Question&lt;/h4&gt;
&lt;p&gt;It&amp;rsquo;s useful to have a guiding question as it helps focus your exploration. Let&amp;rsquo;s say, I was interested in whether one of my tweets prompted a reader to click on my profile. The metric for this is &lt;code&gt;user profile clicks&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;My initial guiding question for this post is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Which metrics are most strongly correlated with User Profile Clicks?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You could simply use the &lt;code&gt;cor.test()&lt;/code&gt; function, which comes with base R, to go one by one between &lt;em&gt;each&lt;/em&gt; metric and &lt;code&gt;User Profile Click&lt;/code&gt;. For example, below we calculate the correlation between three pairs of variables, &lt;code&gt;User Profile Clicks&lt;/code&gt; and &lt;code&gt;retweets&lt;/code&gt;, &lt;code&gt;replies&lt;/code&gt; and &lt;code&gt;likes&lt;/code&gt;, separately. After awhile, this can get tedious.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cor.test(x = df$`user profile clicks`, y = df$retweets)
cor.test(x = df$`user profile clicks`, y = df$replies)
cor.test(x = df$`user profile clicks`, y = df$likes)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quicker way to explore the relationship between pairs of metrics throughout a dataset is to use a &lt;strong&gt;correlelogram&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll start with base R. You&amp;rsquo;ll want to limit the number of variables you visualize so the correlelogram doesn&amp;rsquo;t become too cluttered. Here are four variables that correlate the highest with &lt;code&gt;User Profile Clicks&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# four columns are selected along with user profile clicks to plot
df %&amp;gt;%
    select(8, 12, 19:20, `user profile clicks`) %&amp;gt;%
    plot(pch = 20, cex = 1.5, col=&amp;quot;#69b3a2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s a visual:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./plot_strongest.png&#34; alt=&#34;plot_strongest&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here are another four metrics with &lt;em&gt;moderate&lt;/em&gt; relationships:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df %&amp;gt;%
    select(6:7, 10:11, `user profile clicks`) %&amp;gt;%
    plot(pch = 20, cex = 1.5, col=&amp;quot;#69b3a2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./plot_moderate.png&#34; alt=&#34;plot_moderate&#34;&gt;&lt;/p&gt;
&lt;p&gt;Visually, you can see the moderate relationship scatter plots are more dispersed, with a less identifiable direction.&lt;/p&gt;
&lt;p&gt;While base R is dependable, we can get more informative plots with the &lt;code&gt;GGally&lt;/code&gt; package. Here are the four highly correlated variables with &lt;code&gt;User Profile Clicks&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(GGally)

# GGally, Strongest Related
df %&amp;gt;%
    select(8, 12, 19:20, `user profile clicks`) %&amp;gt;%
    ggpairs(
        diag = NULL,
        title = &amp;quot;Strongest Relationships with User Profile Clicks: Sep 2020 - Jan 2021&amp;quot;,
        axisLabels = c(&amp;quot;internal&amp;quot;),
        xlab = &amp;quot;Value&amp;quot;
    )

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the correlelogram between the four most highly correlated variables with &lt;code&gt;user profile clicks&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./strongest.png&#34; alt=&#34;strongest&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here are the moderately correlated variables with &lt;code&gt;User Profile Clicks&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./moderate.png&#34; alt=&#34;moderate&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see, not only do these provide scatter plots, but they also show the numerical values of the correlation between each pair of variables, which is much more informative than base R.&lt;/p&gt;
&lt;p&gt;Now, its entirely possible that the pattern of correlation in your data is different as the initial patterns we&amp;rsquo;re seeing here are not meant to generalize to a different dataset.&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Descent -- Data Science from Scratch (ch8)</title>
      <link>/post/dsfs_8/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_8/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#gradient_descent&#34;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#from_scratch&#34;&gt;From Scratch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#take_away&#34;&gt;Take Away&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll explore Gradient Descent from the ground up starting conceptually, then using code to build up our intuition brick by brick.&lt;/p&gt;
&lt;p&gt;While this post is part of an ongoing series where I document my progress through 
&lt;a href=&#34;https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science from Scratch by Joel Grus&lt;/a&gt;, for this post I am drawing on external sources including AurÃ©lien Geron&amp;rsquo;s Hands-On Machine Learning to provide a context for why and when gradient descent is used.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll also be using external libraries such as &lt;code&gt;numpy&lt;/code&gt;, that are generally avoided in Data Science from Scratch, to help highlight concepts.&lt;/p&gt;
&lt;p&gt;While the book introduces gradient descent as a standalone topic, I find it more intuitive to reason about it within the context of a regression problem.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In any modeling task, there is error, and our objective is minimize the errors so that when we develop models from our training data, we&amp;rsquo;ll have some confidence that the predictions will work in testing and completely new data.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll train a &lt;em&gt;linear regression model&lt;/em&gt;. Our dataset will only have three data points. To create the model, we&amp;rsquo;ll setting up parameters (slope and intercept) that best &amp;ldquo;fits&amp;rdquo; the data (i.e., best-fitting line), for example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./best_fit_line2.png&#34; alt=&#34;best fit line2&#34;&gt;&lt;/p&gt;
&lt;p&gt;We know the values for both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, so we can calculate the slope and intercept directly through the &lt;strong&gt;normal equation&lt;/strong&gt;, which is the 
&lt;a href=&#34;http://mlwiki.org/index.php/Normal_Equation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analytical approach&lt;/a&gt; to finding regression coefficients (slope and intercept):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Normal Equation

import numpy as np
import matplotlib.pyplot as plt

x = np.array([2, 4, 5])
y = np.array([45, 85, 105])

# computing Normal Equation
x_b = np.c_[np.ones((3, 1)), x]       # add x0 = 1 to each of three instances
theta = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)

# array([ 5., 20.])
theta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key line is &lt;code&gt;np.linalg.inv()&lt;/code&gt; which computes the multiplicative inverse of a matrix.&lt;/p&gt;
&lt;p&gt;Our slope is 20 and intercept is 5 (i.e., &lt;code&gt;theta&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We could also have used the more familiar &amp;ldquo;rise over run&amp;rdquo; ((85 - 45) / (4 - 2)) or (40/2) or 20, but we want to illustrate the &lt;strong&gt;normal equation&lt;/strong&gt; which should come in handy when we go beyond the simplistic three data point example.&lt;/p&gt;
&lt;p&gt;We could also use the &lt;code&gt;LinearRegression&lt;/code&gt; class from &lt;code&gt;sklearn&lt;/code&gt; to call the least squares (&lt;code&gt;np.linalg.lstsq()&lt;/code&gt;) function directly:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Linear Regression

from sklearn.linear_model import LinearRegression
import numpy as np

x = np.array([2, 4, 5])
y = np.array([45, 85, 105])

x = x.reshape(-1, 1)              # reshape because sklearn expect 2D array

x_b = np.c_[np.ones((3, 1)), x]   # add x0 = 1 to each of three instances

theta, residuals, rank, s = np.linalg.lstsq(x_b, y, rcond=1e-6)

# array([ 5., 20.])
print(&amp;quot;theta:&amp;quot;, theta)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This appraoch also yields the slope (20) and intercept (5) directly.&lt;/p&gt;
&lt;p&gt;We know the parameters of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; in our example, but we want to see how &lt;strong&gt;learning from data&lt;/strong&gt; would work. Here&amp;rsquo;s the equation we&amp;rsquo;re working with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;y = 20 * x + 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here&amp;rsquo;s what it looks like (intercept = 5, slope = 20)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./known_x_y.png&#34; alt=&#34;known x y&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;gradient_descent&#34;&gt;Gradient_Descent&lt;/h2&gt;
&lt;h4 id=&#34;why&#34;&gt;Why?&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;normal equation&lt;/strong&gt; and the &lt;strong&gt;least squares&lt;/strong&gt; approach can handle large training sets efficiently, but when your model has a large number of features or too many training instances to fit into memory, &lt;strong&gt;gradient descent&lt;/strong&gt; is an often used alternative.&lt;/p&gt;
&lt;p&gt;Moreover, linear least squares assume the errors have a normal distribution and the relationship in the data is linear (this is where closed-form solutions like the normal equation excel). When the data is non-linear, an iterative solution (gradient descent) can be used.&lt;/p&gt;
&lt;p&gt;With linear regression we seek to minimize the sum-of-squares differences between the observed data and the predicted values (aka the error), in a &lt;strong&gt;non-iterative&lt;/strong&gt; fashion.&lt;/p&gt;
&lt;p&gt;Alternatively, we use gradient descent to find the slope and intercept that minimizes the average squared error, however, in an &lt;strong&gt;iterative fashion&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;using-gradient-descent-to-fit-a-model&#34;&gt;Using Gradient Descent to Fit a Model&lt;/h4&gt;
&lt;p&gt;The process for gradient descent is to start with a &lt;strong&gt;random&lt;/strong&gt; slope and intercept, then compute the gradient of the mean squared error, while adjusting the slope/intercept (&lt;code&gt;theta&lt;/code&gt;) in the direction that continues to minimize the error. This is repeated iteratively until we find a point where errors are &lt;em&gt;most&lt;/em&gt; minimized.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This section builds heavily on a previous post on linear algebra. You&amp;rsquo;ll want to 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;read this post&lt;/a&gt; to get a feel for the functions used to construct the functions we see in this post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import TypeVar, List, Iterator
import math
import random
import matplotlib.pyplot as plt
from typing import Callable
from typing import List
import numpy as np

x = np.array([2, 4, 5])

# instead of putting y directly, we&#39;ll use the equation: 20 * x + 5, which is a direct representation of its relationship to x

# y = np.array([45, 85, 105])   

# both x and y are represented in inputs
inputs = [(x, 20 * x + 5) for x in range(2, 6)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we&amp;rsquo;ll start with random values for the slope and intercept; we&amp;rsquo;ll also establish a learning rate, which controls how much a change in the model is warranted in response to the estimated error each time the model parameters (slope and intercept) are updated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 1. start with a random value for slope and intercept
theta = [random.uniform(-1, 1), random.uniform(-1, 1)]

learning_rate = 0.001
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we&amp;rsquo;ll compute the mean of the gradients, then adjust the slope/intercept in the direction of minimizing the gradient, which is based on the error.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll note that this for-loop has 100 iterations. The more iterations we go through, the more that errors are minimized and the more we approach a slope/intercept where the model &amp;ldquo;fits&amp;rdquo; the data better.&lt;/p&gt;
&lt;p&gt;You can see in this list, &lt;code&gt;[linear_gradient(x, y, theta) for x, y in inputs]&lt;/code&gt;, that our &lt;code&gt;linear_gradient&lt;/code&gt; function is applied to the known &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values in the list of tuples, &lt;code&gt;inputs&lt;/code&gt;, along with random values for slope/intercept (&lt;code&gt;theta&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We multiply each &lt;code&gt;x&lt;/code&gt; value with a random value for slope, then add a random value for intercept. This yields the initial prediction. Error is the gap between the initial prediction and &lt;em&gt;actual&lt;/em&gt; &lt;code&gt;y&lt;/code&gt; values. We minimize the squared error by using its gradient.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# start with a function that determines the gradient based on the error from a single data point
def linear_gradient(x: float, y: float, theta: Vector) -&amp;gt; Vector:
    slope, intercept = theta
    predicted = slope * x + intercept   # model prediction
    error = (predicted - y)             # error is (predicted - actual)
    squared_error = error ** 2          # minimize squared error
    grad = [2 * error * x, 2 * error]   # using its gradient
    return grad
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;linear_gradient&lt;/code&gt; function along with initial parameters are then passed to &lt;code&gt;vector_mean&lt;/code&gt;, which utilize &lt;code&gt;scalar_multiply&lt;/code&gt; and &lt;code&gt;vector_sum&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def vector_mean(vectors: List[Vector]) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Computes the element-wise average&amp;quot;&amp;quot;&amp;quot;
    n = len(vectors)
    return scalar_multiply(1/n, vector_sum(vectors))

def scalar_multiply(c: float, v: Vector) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Multiplies every element by c&amp;quot;&amp;quot;&amp;quot;
    return [c * v_i for v_i in v]
    
def vector_sum(vectors: List[Vector]) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Sum all corresponding elements (componentwise sum)&amp;quot;&amp;quot;&amp;quot;
    # Check that vectors is not empty
    assert vectors, &amp;quot;no vectors provided!&amp;quot;
    # Check the vectorss are all the same size
    num_elements = len(vectors[0])
    assert all(len(v) == num_elements for v in vectors), &amp;quot;different sizes!&amp;quot;
    # the i-th element of the result is the sum of every vector[i]
    return [sum(vector[i] for vector in vectors)
            for i in range(num_elements)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This yields the gradient. Then, each &lt;code&gt;gradient_step&lt;/code&gt; is deteremined as our function adjusts the initial random &lt;code&gt;theta&lt;/code&gt; values (slope/intercept) in the direction that minimizes the error.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gradient_step(v: Vector, gradient: Vector, step_size: float) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Moves `step_size` in the `gradient` direction from `v`&amp;quot;&amp;quot;&amp;quot;
    assert len(v) == len(gradient)
    step = scalar_multiply(step_size, gradient)
    return add(v, step)
    
def add(v: Vector, w: Vector) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Adds corresponding elements&amp;quot;&amp;quot;&amp;quot;
    assert len(v) == len(w), &amp;quot;vectors must be the same length&amp;quot;
    return [v_i + w_i for v_i, w_i in zip(v, w)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All this comes together in this &lt;strong&gt;for-loop&lt;/strong&gt; to print out how the slope and intercept change with each iteration (we start with 100):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for epoch in range(100):     # start with 100 &amp;lt;--- change this figure to try different iterations
    # compute the mean of the gradients
    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])
    # take a step in that direction
    theta = gradient_step(theta, grad, -learning_rate)
    print(epoch, grad, theta)

slope, intercept = theta

#assert 19.9 &amp;lt; slope &amp;lt; 20.1,  &amp;quot;slope should be about 20&amp;quot;
#assert 4.9 &amp;lt; intercept &amp;lt; 5.1, &amp;quot;intercept should be about 5&amp;quot;
print(&amp;quot;slope&amp;quot;, slope)
print(&amp;quot;intercept&amp;quot;, intercept)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;iterative-descent&#34;&gt;Iterative Descent&lt;/h4&gt;
&lt;p&gt;At 100 iterations, the slope is 18.87 and intercept is 4.87 and the gradient is -32.48 (error for the slope) and -8.45 (error for the intercept). These numbers suggest that we need to decrease the slope and intercept from our random starting point, but our emphasis needs to be on decreasing the slope.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./100_iterations.png&#34; alt=&#34;100 iterations&#34;&gt;&lt;/p&gt;
&lt;p&gt;At 200 iterations, the slope is 19.97 and intercept is 4.86 and the gradient is -1.76 (error for the slope) and -0.48 (error for the intercept). Our errors have been reduced significantly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./200_iterations.png&#34; alt=&#34;200 iterations&#34;&gt;&lt;/p&gt;
&lt;p&gt;At 1000 iterations, the slope is 19.97 (not much difference from 200 iterations) and intercept is 5.09 and the gradients are markedly lower at -0.004 (error for the slope) and 0.02 (error for the intercept). Here the errors may not be much different from zero and we are near our optimal point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./1000_iterations.png&#34; alt=&#34;1000 iterations&#34;&gt;&lt;/p&gt;
&lt;p&gt;In summary, the &lt;strong&gt;normal equation&lt;/strong&gt; and &lt;strong&gt;regression&lt;/strong&gt; approaches gave us a slope of 20 and intercept of 5. With gradient descent, we approached these values with each successive iterations, 1000 iterations yielding &lt;strong&gt;less error&lt;/strong&gt; than 100 or 200 iterations.&lt;/p&gt;
&lt;h2 id=&#34;from_scratch&#34;&gt;From_Scratch&lt;/h2&gt;
&lt;p&gt;As mentioned above, the functions used to compute the gradients and adjust the slope/intercept build on functions we explored in 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt;. Here&amp;rsquo;s a visual showing how the functions we used to iteratively arrive at the slope and intercept through gradient descent was built:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./ch8_funct.png&#34; alt=&#34;ch8_funct&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;take_away&#34;&gt;Take_Away&lt;/h2&gt;
&lt;p&gt;Gradient descent is an optimization technique often used in machine learning and in this post, we built some intuition around how it works by applying it to a simple linear regression problem, favoring code over math (which we&amp;rsquo;ll return to in a later post). Gradient Descent is useful if you are expecting computational complexity due to the number of features or training instances.&lt;/p&gt;
&lt;p&gt;We placed gradient descent in context, in comparison to a more analytical approach, normal equation and the least squares method, both of which are non-iterative.&lt;/p&gt;
&lt;p&gt;Furthermore, we saw how the functions used in this post can be traced back to a previous post on 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linear algebra&lt;/a&gt;, thus giving us a big picture view of how the building blocks of data science and an intuition for areas we&amp;rsquo;ll need to explore at a deeper, perhaps at a more mathematical, level.&lt;/p&gt;
&lt;p&gt;This post is part of an ongoing series where I document my progress through 
&lt;a href=&#34;https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science from Scratch by Joel Grus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./book_disclaim_ch8.png&#34; alt=&#34;book disclaim ch8&#34;&gt;&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science from Scratch (ch7) - Hypothesis and Inference</title>
      <link>/post/dsfs_7/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_7/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#central_limit_theorem&#34;&gt;Central Limit Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#hypothesis_testing&#34;&gt;Hypothesis Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#p_values&#34;&gt;p-Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#confidence_intervals&#34;&gt;Confidence Intervals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#connecting_dots&#34;&gt;Connecting dots with Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This is a continuation of my progress through Data Science from Scratch by Joel Grus. We&amp;rsquo;ll use a classic coin-flipping example in this post because it is simple to illustrate with both &lt;strong&gt;concept&lt;/strong&gt; and &lt;strong&gt;code&lt;/strong&gt;. The goal of this post is to connect the dots between several concepts including the Central Limit Theorem, Hypothesis Testing, p-Values and confidence intervals, using python to build our intuition.&lt;/p&gt;
&lt;h2 id=&#34;central_limit_theorem&#34;&gt;Central_Limit_Theorem&lt;/h2&gt;
&lt;p&gt;Terms like &amp;ldquo;null&amp;rdquo; and &amp;ldquo;alternative&amp;rdquo; hypothesis are used quite frequently, so let&amp;rsquo;s set some context. The &amp;ldquo;null&amp;rdquo; is the &lt;strong&gt;default&lt;/strong&gt; position. The &amp;ldquo;alternative&amp;rdquo;, alt for short, is something we&amp;rsquo;re &lt;em&gt;comparing to&lt;/em&gt; the default (null).&lt;/p&gt;
&lt;p&gt;The classic coin-flipping exercise is to test the &lt;em&gt;fairness&lt;/em&gt; off a coin. If a coin is fair, it&amp;rsquo;ll land on heads 50% of the time (and tails 50% of the time). Let&amp;rsquo;s translate into hypothesis testing language:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis&lt;/strong&gt;: Probability of landing on Heads = 0.5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alt Hypothesis&lt;/strong&gt;: Probability of landing on Heads != 0.5.&lt;/p&gt;
&lt;p&gt;Each coin flip is a &lt;strong&gt;Bernoulli trial&lt;/strong&gt;, which is an experiment with two outcomes - outcome 1, &amp;ldquo;success&amp;rdquo;, (probability &lt;em&gt;p&lt;/em&gt;) and outcome 0, &amp;ldquo;fail&amp;rdquo; (probability &lt;em&gt;p - 1&lt;/em&gt;). The reason it&amp;rsquo;s a Bernoulli trial is because there are only two outcome with a coin flip (heads or tails). Read more about 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bernoulli here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code for a single Bernoulli Trial:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bernoulli_trial(p: float) -&amp;gt; int:
    &amp;quot;&amp;quot;&amp;quot;Returns 1 with probability p and 0 with probability 1-p&amp;quot;&amp;quot;&amp;quot;
    return 1 if random.random() &amp;lt; p else 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you &lt;strong&gt;sum the independent Bernoulli trials&lt;/strong&gt;, you get a &lt;strong&gt;Binomial(n,p)&lt;/strong&gt; random variable, a variable whose &lt;em&gt;possible&lt;/em&gt; values have a probability distribution. The &lt;strong&gt;central limit theorem&lt;/strong&gt; says as &lt;strong&gt;n&lt;/strong&gt; or the &lt;em&gt;number&lt;/em&gt; of independent Bernoulli trials get large, the Binomial distribution approaches a normal distribution.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code for when you sum all the Bernoulli Trials to get a Binomial random variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def binomial(n: int, p: float) -&amp;gt; int:
    &amp;quot;&amp;quot;&amp;quot;Returns the sum of n bernoulli(p) trials&amp;quot;&amp;quot;&amp;quot;
    return sum(bernoulli_trial(p) for _ in range(n))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: A single &amp;lsquo;success&amp;rsquo; in a Bernoulli trial is &amp;lsquo;x&amp;rsquo;. Summing up all those x&amp;rsquo;s into X, is a Binomial random variable. Success doesn&amp;rsquo;t imply desirability, nor does &amp;ldquo;failure&amp;rdquo; imply undesirability. They&amp;rsquo;re just terms to count the cases we&amp;rsquo;re looking for (i.e., number of heads in multiple coin flips to assess a coin&amp;rsquo;s fairness).&lt;/p&gt;
&lt;p&gt;Given that our &lt;strong&gt;null&lt;/strong&gt; is (p = 0.5) and &lt;strong&gt;alt&lt;/strong&gt; is (p != 0.5), we can run some independent bernoulli trials, then sum them up to get a binomial random variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./independent_coin_flips.png&#34; alt=&#34;independent_coin_flips&#34;&gt;&lt;/p&gt;
&lt;p&gt;Each &lt;code&gt;bernoulli_trial&lt;/code&gt; is an experiment with either 0 or 1 as outcomes. The &lt;code&gt;binomial&lt;/code&gt; function sums up &lt;strong&gt;n&lt;/strong&gt; bernoulli(0.5) trails. We ran both twice and got different results. Each bernoulli experiment can be a success(1) or faill(0); summing up into a binomial random variable means we&amp;rsquo;re taking the probability p(0.5) &lt;em&gt;that a coin flips head&lt;/em&gt; and we ran the experiment 1,000 times to get a random binomial variable.&lt;/p&gt;
&lt;p&gt;The first 1,000 flips we got 510. The second 1,000 flips we got 495. We can repeat this process many times to get a &lt;em&gt;distribution&lt;/em&gt;. We can plot this distribution to reinforce our understanding. To this we&amp;rsquo;ll use &lt;code&gt;binomial_histogram&lt;/code&gt; function. This function picks points from a Binomial(n,p) random variable and plots their histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter
import matplotlib.pyplot as plt

def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2
    

def binomial_histogram(p: float, n: int, num_points: int) -&amp;gt; None:
    &amp;quot;&amp;quot;&amp;quot;Picks points from a Binomial(n, p) and plots their histogram&amp;quot;&amp;quot;&amp;quot;
    data = [binomial(n, p) for _ in range(num_points)]
    # use a bar chart to show the actual binomial samples
    histogram = Counter(data)
    plt.bar([x - 0.4 for x in histogram.keys()],
            [v / num_points for v in histogram.values()],
            0.8,
            color=&#39;0.75&#39;)
    mu = p * n
    sigma = math.sqrt(n * p * (1 - p))
    # use a line chart to show the normal approximation
    xs = range(min(data), max(data) + 1)
    ys = [normal_cdf(i + 0.5, mu, sigma) -
          normal_cdf(i - 0.5, mu, sigma) for i in xs]
    plt.plot(xs, ys)
    plt.title(&amp;quot;Binomial Distribution vs. Normal Approximation&amp;quot;)
    plt.show()

# call function   
binomial_histogram(0.5, 1000, 10000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This plot is then rendered:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./binomial_coin_fairness.png&#34; alt=&#34;binomial_coin_fairness&#34;&gt;&lt;/p&gt;
&lt;p&gt;What we did was sum up independent &lt;code&gt;bernoulli_trial&lt;/code&gt;(s) of 1,000 coin flips, where the probability of head is p = 0.5, to create a &lt;code&gt;binomial&lt;/code&gt; random variable. We then repeated this a large number of times (N = 10,000), then plotted a histogram of the distribution of all binomial random variables. And because we did it so many times, it approximates the standard normal distribution (smooth bell shape curve).&lt;/p&gt;
&lt;p&gt;Just to demonstrate how this works, we can generate several &lt;code&gt;binomial&lt;/code&gt; random variables:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./several_binomial.png&#34; alt=&#34;several_binomial&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we do this 10,000 times, we&amp;rsquo;ll generate the above histogram. You&amp;rsquo;ll notice that because we are testing whether the coin is fair, the probability of heads (success) &lt;em&gt;should&lt;/em&gt; be at 0.5 and, from 1,000 coin flips, the &lt;strong&gt;mean&lt;/strong&gt;(&lt;code&gt;mu&lt;/code&gt;) should be a 500.&lt;/p&gt;
&lt;p&gt;We have another function that can help us calculate &lt;code&gt;normal_approximation_to_binomial&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random
from typing import Tuple
import math


def normal_approximation_to_binomial(n: int, p: float) -&amp;gt; Tuple[float, float]:
    &amp;quot;&amp;quot;&amp;quot;Return mu and sigma corresponding to a Binomial(n, p)&amp;quot;&amp;quot;&amp;quot;
    mu = p * n
    sigma = math.sqrt(p * (1 - p) * n)
    return mu, sigma
    
# call function
# (500.0, 15.811388300841896)
normal_approximation_to_binomial(1000, 0.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When calling the function with our parameters, we get a mean &lt;code&gt;mu&lt;/code&gt; of 500 (from 1,000 coin flips) and a standard deviation &lt;code&gt;sigma&lt;/code&gt; of 15.8114. Which means that 68% of the time, the binomial random variable will be 500 +/- 15.8114 and 95% of the time it&amp;rsquo;ll be 500 +/- 31.6228 (see 
&lt;a href=&#34;https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;68-95-99.7 rule&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;hypothesis_testing&#34;&gt;Hypothesis_Testing&lt;/h2&gt;
&lt;p&gt;Now that we have seen the results of our &amp;ldquo;coin fairness&amp;rdquo; experiment plotted on a binomial distribution (approximately normal), we will be, for the purpose of testing our hypothesis, be interested in the probability of its realized value (binomial random variable) lies &lt;strong&gt;within or outside a particular interval&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This means we&amp;rsquo;ll be interested in questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What&amp;rsquo;s the probability that the binomial(n,p) is below a threshold?&lt;/li&gt;
&lt;li&gt;Above a threshold?&lt;/li&gt;
&lt;li&gt;Between an interval?&lt;/li&gt;
&lt;li&gt;Outside an interval?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, the &lt;code&gt;normal_cdf&lt;/code&gt; (normal cummulative distribution function), which we learned in a 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_6/#distributions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;, &lt;em&gt;is&lt;/em&gt; the probability of a variable being &lt;em&gt;below&lt;/em&gt; a certain threshold.&lt;/p&gt;
&lt;p&gt;Here, the probability of X (success or heads for a &amp;lsquo;fair coin&amp;rsquo;) is at 0.5 (&lt;code&gt;mu&lt;/code&gt; = 500, &lt;code&gt;sigma&lt;/code&gt; = 15.8113), and we want to find the probability that X falls below 490, which comes out to roughly 26%&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;normal_probability_below = normal_cdf

# probability that binomal random variable, mu = 500, sigma = 15.8113, is below 490

# 0.26354347477247553
normal_probability_below(490, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the other hand, the &lt;code&gt;normal_probability_above&lt;/code&gt;, probability that X falls &lt;em&gt;above&lt;/em&gt; 490 would be
1 - 0.2635 = 0.7365 or roughly 74%.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normal_probability_above(lo: float,
                             mu: float = 0,
                             sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;The probability that an N(mu, sigma) is greater than lo.&amp;quot;&amp;quot;&amp;quot;
    return 1 - normal_cdf(lo, mu, sigma)
    
# 0.7364565252275245
normal_probability_above(490, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make sense of this we need to recall the binomal distribution, that approximates the normal distribution, but we&amp;rsquo;ll draw a vertical line at 490.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./binomial_vline.png&#34; alt=&#34;binomial_vline&#34;&gt;&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re asking, given the binomal distribution with &lt;code&gt;mu&lt;/code&gt; 500 and &lt;code&gt;sigma&lt;/code&gt; at 15.8113, what is the probability that a binomal random variable falls below the threshold (left of the line); the answer is approximately 26% and correspondingly falling above the threshold (right of the line), is approximately 74%.&lt;/p&gt;
&lt;h3 id=&#34;between-interval&#34;&gt;Between interval&lt;/h3&gt;
&lt;p&gt;We may also wonder what the probability of a binomial random variable &lt;strong&gt;falling between 490 and 520&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./binomial_2_vline.png&#34; alt=&#34;binomial_2_vline&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here is the function to calculate this probability and it comes out to approximately 63%. &lt;em&gt;note&lt;/em&gt;: Bear in mind the full area under the curve is 1.0 or 100%.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normal_probability_between(lo: float,
                               hi: float,
                               mu: float = 0,
                               sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;The probability that an N(mu, sigma) is between lo and hi.&amp;quot;&amp;quot;&amp;quot;
    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)

# 0.6335061861416337
normal_probability_between(490, 520, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the area outside of the interval should be 1 - 0.6335 = 0.3665:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normal_probability_outside(lo: float,
                               hi: float,
                               mu: float = 0,
                               sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;The probability that an N(mu, sigma) is not between lo and hi.&amp;quot;&amp;quot;&amp;quot;
    return 1 - normal_probability_between(lo, hi, mu, sigma)
    
# 0.3664938138583663
normal_probability_outside(490, 520, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to the above, we may also be interested in finding (symmetric) intervals around the mean that account for a &lt;em&gt;certain level of likelihood&lt;/em&gt;, for example, 60% probability centered around the mean.&lt;/p&gt;
&lt;p&gt;For this operation we would use the &lt;code&gt;inverse_normal_cdf&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def inverse_normal_cdf(p: float,
                       mu: float = 0,
                       sigma: float = 1,
                       tolerance: float = 0.00001) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Find approximate inverse using binary search&amp;quot;&amp;quot;&amp;quot;
    # if not standard, compute standard and rescale
    if mu != 0 or sigma != 1:
        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)
    low_z = -10.0     # normal_cdf(-10) is (very close to) 0
    hi_z = 10.0       # normal_cdf(10) is (very close to) 1
    while hi_z - low_z &amp;gt; tolerance:
        mid_z = (low_z + hi_z) / 2      # Consider the midpoint
        mid_p = normal_cdf(mid_z)       # and the CDF&#39;s value there
        if mid_p &amp;lt; p:
            low_z = mid_z               # Midpoint too low, search above it
        else:
            hi_z = mid_z                # Midpoint too high, search below it
    return mid_z
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we&amp;rsquo;d have to find the cutoffs where the upper and lower tails each contain 20% of the probability. We calculate &lt;code&gt;normal_upper_bound&lt;/code&gt; and &lt;code&gt;normal_lower_bound&lt;/code&gt; and use those to calculate the &lt;code&gt;normal_two_sided_bounds&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normal_upper_bound(probability: float,
                       mu: float = 0,
                       sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Returns the z for which P(Z &amp;lt;= z) = probability&amp;quot;&amp;quot;&amp;quot;
    return inverse_normal_cdf(probability, mu, sigma)


def normal_lower_bound(probability: float,
                       mu: float = 0,
                       sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Returns the z for which P(Z &amp;gt;= z) = probability&amp;quot;&amp;quot;&amp;quot;
    return inverse_normal_cdf(1 - probability, mu, sigma)


def normal_two_sided_bounds(probability: float,
                            mu: float = 0,
                            sigma: float = 1) -&amp;gt; Tuple[float, float]:
    &amp;quot;&amp;quot;&amp;quot;
    Returns the symmetric (about the mean) bounds
    that contain the specified probability
    &amp;quot;&amp;quot;&amp;quot;
    tail_probability = (1 - probability) / 2
    # upper bound should have tail_probability above it
    upper_bound = normal_lower_bound(tail_probability, mu, sigma)
    # lower bound should have tail_probability below it
    lower_bound = normal_upper_bound(tail_probability, mu, sigma)
    return lower_bound, upper_bound
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So if we wanted to know what the cutoff points were for a 60% probability around the mean and standard deviation (&lt;code&gt;mu&lt;/code&gt; = 500, &lt;code&gt;sigma&lt;/code&gt; = 15.8113), it would be between &lt;strong&gt;486.69 and 513.31&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Said differently, this means roughly 60% of the time, we can expect the binomial random variable to fall between 486 and 513.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# (486.6927811021805, 513.3072188978196)
normal_two_sided_bounds(0.60, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;significance-and-power&#34;&gt;Significance and Power&lt;/h3&gt;
&lt;p&gt;Now that we have a handle on the binomial normal distribution, thresholds (left and right of the mean), and cut-off points, we want to make a &lt;strong&gt;decision about significance&lt;/strong&gt;. Probably the most important part of &lt;em&gt;statistical significance&lt;/em&gt; is that it is a decision to be made, not a standard that is externally set.&lt;/p&gt;
&lt;p&gt;Significance is a decision about how willing we are to make a &lt;em&gt;type 1&lt;/em&gt; error (false positive), which we explored in a 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_6/#applying_bayes_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;. The convention is to set it to a 5% or 1% willingness to make a type 1 error. Suppose we say 5%.&lt;/p&gt;
&lt;p&gt;We would say that out of 1,000 coin flips, 95% of the time, we&amp;rsquo;d get between 469 and 531 heads on a &amp;ldquo;fair coin&amp;rdquo; and 5% of the time, outside of this 469-531 range.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# (469.0104394712448, 530.9895605287552)
normal_two_sided_bounds(0.95, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we recall our hypotheses:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis&lt;/strong&gt;: Probability of landing on Heads = 0.5 (fair coin)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alt Hypothesis&lt;/strong&gt;: Probability of landing on Heads != 0.5 (biased coin)&lt;/p&gt;
&lt;p&gt;Each binomial distribution (test) that consist of 1,000 bernoulli trials, each &lt;em&gt;test&lt;/em&gt; where the number of heads falls outside the range of 469-531, we&amp;rsquo;ll &lt;strong&gt;reject the null&lt;/strong&gt; that the coin is fair. And we&amp;rsquo;ll be wrong (false positive), 5% of the time. It&amp;rsquo;s a false positive when we &lt;strong&gt;incorrectly reject&lt;/strong&gt; the null hypothesis, when it&amp;rsquo;s actually true.&lt;/p&gt;
&lt;p&gt;We also want to avoid making a type-2 error (false negative), where we &lt;strong&gt;fail to reject&lt;/strong&gt; the null hypothesis, when it&amp;rsquo;s actually false.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Its important to keep in mind that terms like &lt;em&gt;significance&lt;/em&gt; and &lt;em&gt;power&lt;/em&gt; are used to describe &lt;strong&gt;tests&lt;/strong&gt;, in our case, the test of whether a coin is fair or not. Each test is the sum of 1,000 independent bernoulli trials.&lt;/p&gt;
&lt;p&gt;For a &amp;ldquo;test&amp;rdquo; that has a 95% significance, we&amp;rsquo;ll assume that out of a 1,000 coin flips, it&amp;rsquo;ll land on heads between 469-531 times and we&amp;rsquo;ll determine the coin is fair. For the 5% of the time it lands outside of this range, we&amp;rsquo;ll determine the coin to be &amp;ldquo;unfair&amp;rdquo;, but we&amp;rsquo;ll be wrong because it actually is fair.&lt;/p&gt;
&lt;p&gt;To calculate the power of the test, we&amp;rsquo;ll take the assumed &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt; with a 95% bounds (based on the assumption that the probability of the coin landing on heads is 0.5 or 50% - a fair coin). We&amp;rsquo;ll determine the lower and upper bounds:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)
lo # 469.01026640487555
hi # 530.9897335951244
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if the coin was &lt;em&gt;actually biased&lt;/em&gt;, we should reject the null, but we fail to. Let&amp;rsquo;s suppose the actual probability that the coin lands on heads is 55% ( &lt;strong&gt;biased&lt;/strong&gt; towards head):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)
mu_1    # 550.0
sigma_1 # 15.732132722552274
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the same range 469 - 531, where the coin is assumed &amp;lsquo;fair&amp;rsquo; with &lt;code&gt;mu&lt;/code&gt; at 500 and &lt;code&gt;sigma&lt;/code&gt; at 15.8113:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./95sig_binomial.png&#34; alt=&#34;95sig_binomial&#34;&gt;&lt;/p&gt;
&lt;p&gt;If the coin, in fact, had a bias towards head (p = 0.55), the distribution would shift right, but if our 95% significance test remains the same, we get:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./type2_error.png&#34; alt=&#34;type2_error&#34;&gt;&lt;/p&gt;
&lt;p&gt;The probability of making a type-2 error is 11.345%. This is the probability that we&amp;rsquo;re see that the coin&amp;rsquo;s distribution is within the previous interval 469-531, thinking we should accept the null hypothesis (that the coin is fair), but in actuality, failing to see that the distribution has shifted to the coin having a &lt;em&gt;bias&lt;/em&gt; towards heads.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.11345199870463285
type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other way to arrive at this is to find the probability, under the &lt;em&gt;new&lt;/em&gt; &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt; (new distribution), that X (number of successes) will fall &lt;em&gt;below&lt;/em&gt; 531.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.11357762975476304
normal_probability_below(531, mu_1, sigma_1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the probability of making a type-2 error or the probability that the &lt;em&gt;new&lt;/em&gt; distribution falls below 531 is approximately 11.3%.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;power to detect&lt;/strong&gt; a type-2 error is 1.00 minus the probability of a type-2 error (1 - 0.113 = 0.887), or 88.7%.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;power = 1 - type_2_probability # 0.8865480012953671
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we may be interested in &lt;strong&gt;increasing power&lt;/strong&gt; to detect a type-2 error. Instead of using a &lt;code&gt;normal_two_sided_bounds&lt;/code&gt; function to find the cut-off points (i.e., 469 and 531), we could use a &lt;em&gt;one-sided test&lt;/em&gt; that rejects the null hypothesis (&amp;lsquo;fair coin&amp;rsquo;) when X (number of heads on a coin-flip) is much larger than 500.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code, using &lt;code&gt;normal_upper_bound&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 526.0073585242053
hi = normal_upper_bound(0.95, mu_0, sigma_0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means shifting the upper bounds from 531 to 526, providing more probability in the upper tail. This means the probability of a type-2 error goes down from 11.3 to 6.3.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./increase_power.png&#34; alt=&#34;increase_power&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# previous probability of type-2 error
# 0.11357762975476304
normal_probability_below(531, mu_1, sigma_1)


# new probability of type-2 error
# 0.06356221447122662
normal_probability_below(526, mu_1, sigma_1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the new (stronger) &lt;strong&gt;power to detect&lt;/strong&gt; type-2 error is 1.0 - 0.064 = 0.936 or 93.6% (up from 88.7% above).&lt;/p&gt;
&lt;h2 id=&#34;p_values&#34;&gt;p_values&lt;/h2&gt;
&lt;p&gt;p-Values represent &lt;em&gt;another way&lt;/em&gt; of deciding whether to accept or reject the Null Hypothesis. Instead of choosing bounds, thresholds or cut-off points, we could compute the probability, assuming the Null Hypothesis is true, that we would see a value &lt;em&gt;as extreme as&lt;/em&gt; the one we just observed.&lt;/p&gt;
&lt;p&gt;Here is the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def two_sided_p_values(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;
    How likely are we to see a value at least as extreme as x (in either
    direction) if our values are from an N(mu, sigma)?
    &amp;quot;&amp;quot;&amp;quot;
    if x &amp;gt;= mu:
        # x is greater than the mean, so the tail is everything greater than x
        return 2 * normal_probability_above(x, mu, sigma)
    else:
        # x is less than the mean, so the tail is everything less than x
        return 2 * normal_probability_below(x, mu, sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to compute, assuming we have a &amp;ldquo;fair coin&amp;rdquo; (&lt;code&gt;mu&lt;/code&gt; = 500, &lt;code&gt;sigma&lt;/code&gt; = 15.8113), what is the probability of seeing a value like 530? (&lt;strong&gt;note&lt;/strong&gt;: We use 529.5 instead of 530 below due to 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Continuity_correction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continuity correction&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Answer: approximately 6.2%&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.06207721579598835
two_sided_p_values(529.5, mu_0, sigma_0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value, 6.2% is higher than our (hypothetical) 5% significance, so we don&amp;rsquo;t reject the null. On the other hand, if X was slightly more extreme, 532, the probability of seeing that value would be approximately 4.3%, which is less than 5% significance, so we would reject the null.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.04298479507085862
two_sided_p_values(532, mu_0, sigma_0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For one-sided tests, we would use the &lt;code&gt;normal_probability_above&lt;/code&gt; and &lt;code&gt;normal_probability_below&lt;/code&gt; functions created above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;upper_p_value = normal_probability_above
lower_p_value = normal_probability_below
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under the &lt;code&gt;two_sided_p_values&lt;/code&gt; test, the extreme value of 529.5 had a probability of 6.2% of showing up, but not low enough to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;However, with a one-sided test, &lt;code&gt;upper_p_value&lt;/code&gt; for the same threshold is now 3.1% and we would reject the null hypothesis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.031038607897994175
upper_p_value(529.5, mu_0, sigma_0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;confidence_intervals&#34;&gt;Confidence_Intervals&lt;/h2&gt;
&lt;p&gt;A &lt;em&gt;third&lt;/em&gt; approach to deciding whether to accept or reject the null is to use confidence intervals. We&amp;rsquo;ll use the 530 as we did in the p-Values example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p_hat = 530/1000
mu = p_hat
sigma = math.sqrt(p_hat * (1 - p_hat) / 1000) # 0.015782902141241326

# (0.4990660982192851, 0.560933901780715)
normal_two_sided_bounds(0.95, mu, sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The confidence interval for a coin flipping heads 530 (out 1,000) times is (0.4991, 0.5609). Since this interval &lt;strong&gt;contains&lt;/strong&gt; the p = 0.5 (probability of heads 50% of the time, assuming a fair coin), we do not reject the null.&lt;/p&gt;
&lt;p&gt;If the extreme value were &lt;em&gt;more&lt;/em&gt; extreme at 540, we would arrive at a different conclusion:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p_hat = 540/1000
mu = p_hat
sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)

(0.5091095927295919, 0.5708904072704082)
normal_two_sided_bounds(0.95, mu, sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we would be 95% confident that the mean of this distribution is contained between 0.5091 and 0.5709 and this &lt;strong&gt;does not&lt;/strong&gt; contain 0.500 (albiet by a slim margin), so we reject the null hypothesis that this is a fair coin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: Confidence intervals are about the &lt;em&gt;interval&lt;/em&gt; not probability p. We interpret the confidence interval as, if you were to repeat the experiment many times, 95% of the time, the &amp;ldquo;true&amp;rdquo; parameter, in our example p = 0.5, would lie within the observed confidence interval.&lt;/p&gt;
&lt;h2 id=&#34;connecting_dots&#34;&gt;Connecting_Dots&lt;/h2&gt;
&lt;p&gt;We used several python functions to build intuition around statistical hypothesis testing. To higlight this &amp;ldquo;from scratch&amp;rdquo; aspect of the book here is a diagram tying together the various python function used in this post:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./connecting_dots.png&#34; alt=&#34;connecting_dots&#34;&gt;&lt;/p&gt;
&lt;p&gt;This post is part of an ongoing series where I document my progress through 
&lt;a href=&#34;https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science from Scratch by Joel Grus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./book_disclaimer.png&#34; alt=&#34;book_disclaimer&#34;&gt;&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics &amp; Probability in Code</title>
      <link>/post/statistics_probability/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/statistics_probability/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#permutations&#34;&gt;Permutations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Itertools&lt;/code&gt; are a core set of fast, memory efficient tools for creating iterators for efficient looping (read the 
&lt;a href=&#34;https://docs.python.org/3/library/itertools.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; here).&lt;/p&gt;
&lt;h2 id=&#34;itertools-permutations&#34;&gt;Itertools Permutations&lt;/h2&gt;
&lt;p&gt;One (of many) uses for &lt;code&gt;itertools&lt;/code&gt; is to create a &lt;code&gt;permutations()&lt;/code&gt; function that will return all possible combinations of items in a list.&lt;/p&gt;
&lt;p&gt;I was working on a project that involved user funnels with different stages and we were wondering how many different &amp;ldquo;paths&amp;rdquo; a user &lt;em&gt;could&lt;/em&gt; take, so this was naturally a good fit for using &lt;strong&gt;permutations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./sample_funnel.png&#34; alt=&#34;sample_funnel&#34;&gt;
&lt;em&gt;Sample Funnel&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our hypothetical example, we&amp;rsquo;re looking at a funnel with three stages for a total of 6 permutations. Here&amp;rsquo;s the formula:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./permutation_formula.png&#34; alt=&#34;permutation_formula&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re using a sales/marketing funnel, you&amp;rsquo;ll have in mind what your funnel would look like so you may &lt;strong&gt;not&lt;/strong&gt; want all possible paths, but if you&amp;rsquo;re interested in exploring potentially &lt;em&gt;overlooked&lt;/em&gt; paths, read on.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the python 
&lt;a href=&#34;https://docs.python.org/3.6/library/itertools.html#itertools.permutations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; for &lt;code&gt;itertools&lt;/code&gt;, and &lt;code&gt;permutations&lt;/code&gt; specifically. We&amp;rsquo;ll break down the code to better understand what&amp;rsquo;s going on in this function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;note:&lt;/strong&gt; I found a clearer alternative after the fact. Feel free to skip to the final section below, although there is value in comparing the two versions.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll start off with the &lt;code&gt;iterable&lt;/code&gt; which is a &lt;code&gt;list&lt;/code&gt; with three strings. The &lt;code&gt;permutations&lt;/code&gt; function takes in two parameters, the &lt;code&gt;iterable&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt; which is the number of items from the list that we&amp;rsquo;re interested in finding the combination of. If we have three items in the list, we generally want to find &lt;em&gt;all possible&lt;/em&gt; combinations of those three items.&lt;/p&gt;
&lt;p&gt;Here is the code, and subsequent breakdown:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# list of length 3
list1 = [&#39;stage 1&#39;, &#39;stage 2&#39;, &#39;stage 3&#39;]

# iterable is the list
# r = number of items from the list to find combinations of


def permutations(iterable, r=None):
    &amp;quot;&amp;quot;&amp;quot;Find all possible order of a list of elements&amp;quot;&amp;quot;&amp;quot;
    # permutations(&#39;ABCD&#39;,2)--&amp;gt; AB AC AD BA BC BD CA CB CD DA DB DC
    # permutations(range(3))--&amp;gt; 012 021 102 120 201 210
    # permutations(list1, 6)--&amp;gt; ...720 permutations
    pool = tuple(iterable)
    n = len(pool)
    r = n if r is None else r
    if r &amp;gt; n:
        return
    indices = list(range(n))                     # [0, 1, 2]
    cycles = list(range(n, n-r, -1))             # [3, 2, 1]
    yield tuple(pool[i] for i in indices[:r])
    print(&amp;quot;Now entering while-loop \n&amp;quot;)
    while n:
        for i in reversed(range(r)):
            cycles[i] -= 1
            if cycles[i] == 0:
                indices[i:] = indices[i+1:] + indices[i:i+1]
                cycles[i] = n - i
            else:
                j = cycles[i]
                indices[i], indices[-j] = indices[-j], indices[i]
                yield tuple(pool[i] for i in indices[:r])
                print(&amp;quot;indices[:r]&amp;quot;, indices[:r])
                print(&amp;quot;pool[i]:&amp;quot;, tuple(pool[i] for i in indices[:r]))
                print(&amp;quot;n:&amp;quot;, n)
                break
        else:
            print(&amp;quot;return:&amp;quot;)
            return


#permutations(list1, 6)

perm = permutations(list1, 3)
count = 0

for p in perm:
    count += 1
    print(p)
print(&amp;quot;there are:&amp;quot;, count, &amp;quot;permutations.&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing we do is take the &lt;code&gt;iterable&lt;/code&gt; input parameter is turn it from a &lt;code&gt;list&lt;/code&gt; into a &lt;code&gt;tuple&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pool = tuple(iterable)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are several reasons to do this. First, &lt;code&gt;tuples&lt;/code&gt; are &lt;em&gt;faster&lt;/em&gt; than &lt;code&gt;lists&lt;/code&gt;; the &lt;code&gt;permutations()&lt;/code&gt; function will do several operations to the input so changing it to a &lt;code&gt;tuple&lt;/code&gt; allows faster operations and because &lt;code&gt;tuples&lt;/code&gt; are &lt;em&gt;immutable&lt;/em&gt;, we can do a bunch of different operations without fear that we might &lt;em&gt;inadvertently&lt;/em&gt; change the list.&lt;/p&gt;
&lt;p&gt;We then create &lt;code&gt;n&lt;/code&gt; from the length of &lt;code&gt;pool&lt;/code&gt; (in our case it&amp;rsquo;s 3) and the additional &lt;code&gt;r&lt;/code&gt; parameter, which defaults to &lt;code&gt;None&lt;/code&gt; is also 3 as we&amp;rsquo;re interested in seeing &lt;strong&gt;all combinations&lt;/strong&gt; of a list of three elements.&lt;/p&gt;
&lt;p&gt;We also have a line that ensures that &lt;code&gt;r&lt;/code&gt; can never be greater than the number of elements in the &lt;code&gt;iterable&lt;/code&gt; (list).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if r &amp;gt; n:
    return
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create &lt;code&gt;indices&lt;/code&gt; and &lt;code&gt;cycles&lt;/code&gt;. Indices are basically the index of each item, starting with 0 to 2, for three items. Cycles uses &lt;code&gt;range(n, n-r, -1)&lt;/code&gt;, which in our case is &lt;code&gt;range(3, 3-3, -1)&lt;/code&gt;; this means &lt;strong&gt;start&lt;/strong&gt; at three and &lt;strong&gt;end&lt;/strong&gt; at zero, in -1 &lt;strong&gt;steps&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The next chunk of code is a &lt;code&gt;while-loop&lt;/code&gt; that will continue for the length of the list, &lt;code&gt;n&lt;/code&gt; (note the &lt;code&gt;break&lt;/code&gt; at the bottom to exit out of this loop).&lt;/p&gt;
&lt;p&gt;After each &lt;code&gt;if-else&lt;/code&gt; cycle, a new set of &lt;code&gt;indices&lt;/code&gt; are created, which then gets looped through with &lt;code&gt;pool&lt;/code&gt;, the interable parameter input, which changes the order of the elements in the list.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll note in the commented code above, &lt;code&gt;cycles&lt;/code&gt; start off at [3,2,1] and &lt;code&gt;indices&lt;/code&gt; start off at [0,1,2]. Each loop through the code changes the &lt;code&gt;indices&lt;/code&gt; where &lt;code&gt;indices[i:]&lt;/code&gt; successively gets longer [2], then [1,2], then [1,2,3]. While &lt;code&gt;cycles&lt;/code&gt; changes as it trends toward [1,1,1], which point the code breaks out of the loop.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;while n:
        for i in reversed(range(r)):
            cycles[i] -= 1
            if cycles[i] == 0:
                indices[i:] = indices[i+1:] + indices[i:i+1]
                cycles[i] = n - i
            else:
                j = cycles[i]
                indices[i], indices[-j] = indices[-j], indices[i]
                yield tuple(pool[i] for i in indices[:r])
                print(&amp;quot;indices[:r]&amp;quot;, indices[:r])
                print(&amp;quot;pool[i]:&amp;quot;, tuple(pool[i] for i in indices[:r]))
                print(&amp;quot;n:&amp;quot;, n)
                break
        else:
            print(&amp;quot;return:&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;permutations(iterable, r)&lt;/code&gt; function actually creates a &lt;code&gt;generator&lt;/code&gt; so we need to loop through it again to print out all the permutations of the list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;lt;generator object permutations at 0x7fe19400fdd0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We add another for-loop at the bottom to print out all the permutations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;perm = permutations(list1, 3)
count = 0

for p in perm:
    count += 1
    print(p)
print(&amp;quot;there are:&amp;quot;, count, &amp;quot;permutations.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is our result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./permutations.png&#34; alt=&#34;permutations&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;a-clearer-alternative-permutation-using-recursion&#34;&gt;A Clearer Alternative: Permutation Using Recursion&lt;/h3&gt;
&lt;p&gt;As is often the case, there is a better way I found in retrospect from 
&lt;a href=&#34;https://stackoverflow.com/questions/104420/how-to-generate-all-permutations-of-a-list&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this stack overflow&lt;/a&gt; (h/t to 
&lt;a href=&#34;https://twitter.com/lebigot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eric O Lebigot&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def all_perms(elements):
    if len(elements) &amp;lt;= 1:
        yield elements  # Only permutation possible = no permutation
    else:
        # Iteration over the first element in the result permutation:
        for (index, first_elmt) in enumerate(elements):
            other_elmts = elements[:index] + elements[index+1:]
            for permutation in all_perms(other_elmts):
                yield [first_elmt] + permutation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;enumerate&lt;/code&gt; built-in function obviates the need to separately create &lt;code&gt;cycles&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt;. The local variable &lt;code&gt;other_elmts&lt;/code&gt; separates the other elements in the list from the &lt;code&gt;first_elmt&lt;/code&gt;, then the second for-loop recursively finds the permutation of the other elements before adding with the &lt;code&gt;first_elmt&lt;/code&gt; on the final line, yielding all possible permutations of a list. As with the previous case, the result of this function is a &lt;code&gt;generator&lt;/code&gt; which requires looping through and printing the permutations.&lt;/p&gt;
&lt;p&gt;I found this much easier to digest than the documentation version.&lt;/p&gt;
&lt;p&gt;Permutations can be useful when you have varied user journeys through your product and you want to figure out all the possible paths. With this short python script, you can easily print out all options for consideration.&lt;/p&gt;
&lt;h3 id=&#34;take-aways&#34;&gt;Take Aways&lt;/h3&gt;
&lt;p&gt;From the perspective of a user funnel, &lt;strong&gt;permutations&lt;/strong&gt; allow us to explore all possible &lt;em&gt;paths&lt;/em&gt; a user might take. For our hypothetical example, a three-step funnel yields six possible paths a user could navigate from start to finish.&lt;/p&gt;
&lt;p&gt;Knowing permutations should also &lt;strong&gt;give us pause&lt;/strong&gt; when deciding whether to add another &amp;ldquo;step&amp;rdquo; to a funnel. Going from a three-step funnel to a four-step funnel increases the number of possible paths from six to 24 - a quadruple increase.&lt;/p&gt;
&lt;p&gt;Not only does this increase &lt;strong&gt;friction&lt;/strong&gt; between your user and the &amp;lsquo;end goal&amp;rsquo; (conversion), whatever that may be for your product, but it also increases complexity (and potentially confusion) in the user experience.&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science from Scratch (ch6) - Probability</title>
      <link>/post/dsfs_6/</link>
      <pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_6/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#challenge&#34;&gt;Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#marginal_and_joint_probabilities&#34;&gt;Marginal and Joint Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#conditional_probability&#34;&gt;Conditional Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#bayes_theorem&#34;&gt;Bayes&#39; Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#applying_bayes_theorem&#34;&gt;Applying Bayes&#39; Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#distributions&#34;&gt;Distributions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h2 id=&#34;challenge&#34;&gt;Challenge&lt;/h2&gt;
&lt;p&gt;The first challenge in this section is distinguishing between &lt;strong&gt;two&lt;/strong&gt; conditional probability statements.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the setup. We have a family with two (unknown) children with two assumptions. First, each child is equally likely to be a boy or a girl. Second, the gender of the second child is &lt;em&gt;independent&lt;/em&gt; of the gender of the first child.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for statement one is roughly 50% or (1/2).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for statement two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;But at first glance, they look similar.&lt;/p&gt;
&lt;h2 id=&#34;marginal_and_joint_probabilities&#34;&gt;Marginal_and_Joint_Probabilities&lt;/h2&gt;
&lt;p&gt;The book jumps straight to conditional probabilities, but first, we&amp;rsquo;ll have to look at &lt;strong&gt;marginal&lt;/strong&gt; and &lt;strong&gt;joint&lt;/strong&gt; probabilities. Then we&amp;rsquo;ll create a &lt;strong&gt;joint probabilities table&lt;/strong&gt; and &lt;strong&gt;sum&lt;/strong&gt; probabilities to help us figure out the differences. We&amp;rsquo;ll then &lt;em&gt;resume&lt;/em&gt; with &lt;strong&gt;conditional probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Before anything, we need to realize the situation we have is one of &lt;strong&gt;independence&lt;/strong&gt;. The gender of one child is &lt;strong&gt;independent&lt;/strong&gt; of a second child.&lt;/p&gt;
&lt;p&gt;The intuition for this scenario will be different from a &lt;strong&gt;dependent&lt;/strong&gt; situation. For example, if we draw two cards from a deck (without replacement), the probabilities are different. The probability of drawing one King â ï¸ is (4/52) and the probability of drawing a second King â£ï¸ is now (3/51); the probability of the second event (a second King) is &lt;em&gt;dependent&lt;/em&gt; on the result of the first draw.&lt;/p&gt;
&lt;p&gt;Ok back to the two unknown children.&lt;/p&gt;
&lt;p&gt;We can say the probability of the first child being either a boy or a girl is 50/50. Moreover, the probability of the second child, which is &lt;strong&gt;independent&lt;/strong&gt; of the first, is &lt;em&gt;also&lt;/em&gt; 50/50. Remember, our first assumption is that &lt;em&gt;each child is equally likely to be a boy or a girl&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s put these numbers in a table. The (1/2) probabilities shown here are called &lt;strong&gt;marginal&lt;/strong&gt; probabilities (note how they&amp;rsquo;re at the margins of the table).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./marginal.png&#34; alt=&#34;marginal&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since we have two gender (much like two sides of a flipped coin), we can intuitively figure out &lt;em&gt;all&lt;/em&gt; possible outcomes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;first child (Boy), second child (Boy)&lt;/li&gt;
&lt;li&gt;first child (Boy), second child (Girl)&lt;/li&gt;
&lt;li&gt;first child (Girl), second child (Boy)&lt;/li&gt;
&lt;li&gt;first child (Girl), second child (Girl)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are &lt;em&gt;4 possible outcomes&lt;/em&gt; so the probability of getting any one of the four outcomes is (1/4). We can actually write these probabilities in the middle of the table, the &lt;strong&gt;joint probabilities&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./joint.png&#34; alt=&#34;joint&#34;&gt;&lt;/p&gt;
&lt;p&gt;To recap, the probability of the first child being either boy or girl is 50/50, simple enough. The probability of the second child being either boy or girl is also 50/50. When put in a table, this yielded the &lt;strong&gt;marginal probability&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now we want to know the probability of say, &amp;lsquo;first child being a boy and second child being a girl&amp;rsquo;. This is a &lt;strong&gt;joint probability&lt;/strong&gt; because is is the probability that the first child take a specific gender (boy) &lt;strong&gt;AND&lt;/strong&gt; the second child take a specific gender (girl).&lt;/p&gt;
&lt;p&gt;If two event are &lt;strong&gt;independent&lt;/strong&gt;, and in this case they are, their &lt;strong&gt;joint probabilities&lt;/strong&gt; are the &lt;em&gt;product&lt;/em&gt; of the probabilities of &lt;strong&gt;each one happening&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The probability of the first child being a Boy (1/2) &lt;strong&gt;and&lt;/strong&gt; second child being a Girl (1/2); The product of each marginal probability is the joint probability (1/2 * 1/2 = 1/4).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./product_marginal.png&#34; alt=&#34;product_marginal&#34;&gt;&lt;/p&gt;
&lt;p&gt;This can be repeated for the other three joint probabilities.&lt;/p&gt;
&lt;h2 id=&#34;conditional_probability&#34;&gt;Conditional_Probability&lt;/h2&gt;
&lt;p&gt;Now we get into &lt;strong&gt;conditional probability&lt;/strong&gt; which is the probability of one event happening (i.e., second child being a Boy or Girl) &lt;strong&gt;given that&lt;/strong&gt; or &lt;strong&gt;on conditional that&lt;/strong&gt; another event happened (i.e., first child being a Boy).&lt;/p&gt;
&lt;p&gt;At this point, it might be a good idea to get familiar with notation.&lt;/p&gt;
&lt;p&gt;A joint probability is the product of each individual event happening (assuming they are independent events). For example we might have two individual events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy): 1/2&lt;/li&gt;
&lt;li&gt;P(2nd Child = Boy): 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is their &lt;strong&gt;joint probability&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy, 2nd Child = Boy) =&amp;gt;&lt;/li&gt;
&lt;li&gt;P(1st Child = Boy) * P(2nd Child = Boy) =&amp;gt;&lt;/li&gt;
&lt;li&gt;(1/2 * 1/2 = 1/4)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is a relationship between &lt;strong&gt;conditional&lt;/strong&gt; probabilities and &lt;strong&gt;joint&lt;/strong&gt; probabilities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy | 2nd Child = Boy) = P(1st Child = Boy, 2nd Child = Boy) / P(2nd Child = Boy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Namely, the &lt;strong&gt;conditional&lt;/strong&gt; probability is equal to the &lt;strong&gt;joint&lt;/strong&gt; probability divided by the conditional.&lt;/p&gt;
&lt;p&gt;Thie works out to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy | 2nd Child = Boy) = (1/4) / (1/2)
or&lt;/li&gt;
&lt;li&gt;(1/4) * (2/1)
= 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, the probability that the second child is a boy, given that the first child is a boy is &lt;em&gt;still&lt;/em&gt; 50% (this implies that with respect to &lt;strong&gt;conditional&lt;/strong&gt; probability, if the events are &lt;strong&gt;independent&lt;/strong&gt; it is not different from a single event).&lt;/p&gt;
&lt;p&gt;Now we&amp;rsquo;re ready to tackle the two challenges posed at the beginning of this post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Challenge 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s break it down. First we want the probability of the event that &amp;ldquo;both children are girls&amp;rdquo;. We&amp;rsquo;ll take the product of two events; the probability that the first child is a girl (1/2) and the probability that the second child is a girl (1/2). So the  &lt;strong&gt;joint probability of both&lt;/strong&gt; child being girls is 1/2 * 1/2 = 1/4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl, 2nd Child = Girl) = 1/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Second, we want that to be &lt;strong&gt;given that&lt;/strong&gt; the &amp;ldquo;older child is a girl&amp;rdquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl) = 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conditional probability&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(Both Child = Girls | 1st Child = Girl) = P(1st Child = Girl, 2nd Child = Girl) / P(1st Child = Girl)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(Both Child = Girls | 1st Child = Girl) = (1/4) / (1/2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(1/4) * (2/1) = &lt;strong&gt;1/2&lt;/strong&gt; or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let&amp;rsquo;s break down the second challenge:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Challenge 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, we start with &amp;ldquo;both children are girls&amp;rdquo;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl, 2nd Child = Girl) = 1/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, we have &amp;ldquo;on condition that at least one of the children is a girl&amp;rdquo;. We&amp;rsquo;ll reference a &lt;strong&gt;joint probability table&lt;/strong&gt;. We see that when trying to figure out the probability that &amp;ldquo;at least one of the children is a girl&amp;rdquo;, we rule out the scenario where &lt;strong&gt;both&lt;/strong&gt; children are boys. The remaining 3 out of 4 probabilities, fit the condition.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./at_least.png&#34; alt=&#34;at least&#34;&gt;&lt;/p&gt;
&lt;p&gt;The probability of at least one children being a girl is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1/4) + (1/4) + (1/4) = 3/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So (introducing notation):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(B) = &amp;ldquo;probability of both child being girls&amp;rdquo; (i.e., 1st Child = Girl, 2nd Child = Girl)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(L) = &amp;ldquo;probability of at least one child being a girl&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|L) = P(B,L) / P(L)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|L) = (1/4) / (3/4) = (1/4) * (4/3) = &lt;strong&gt;1/3&lt;/strong&gt; or roughly &lt;strong&gt;33%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-take-away&#34;&gt;Key Take-away&lt;/h4&gt;
&lt;p&gt;When two events are &lt;strong&gt;independent&lt;/strong&gt;, their &lt;strong&gt;joint probability&lt;/strong&gt; is the product of each event:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(E,F) = P(E) * P(F)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Their &lt;strong&gt;conditional&lt;/strong&gt; probability is the &lt;strong&gt;joint probability&lt;/strong&gt; divided by the conditional (i.e., P(F)).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(E|F) = P(E,F) / P(F)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And so for our two challenge scenarios, we have:&lt;/p&gt;
&lt;p&gt;Challenge 1:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;B = probability that both children are girls&lt;/li&gt;
&lt;li&gt;G = probability that the &lt;em&gt;older&lt;/em&gt; children is a girl&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This can be stated as: P(B|G) = P(B,G) / P(G)&lt;/p&gt;
&lt;p&gt;Challenge 2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;B = probability that both children are girls&lt;/li&gt;
&lt;li&gt;L = probability that &lt;em&gt;at least one&lt;/em&gt; children is a girl&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This can be stated as: P(B|L) = P(B,L) / P(L)&lt;/p&gt;
&lt;h4 id=&#34;python-code&#34;&gt;Python Code&lt;/h4&gt;
&lt;p&gt;Now that we have an intuition and have worked out the problem on paper, we can use code to express conditional probability:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import enum, random
class Kid(enum.Enum):
    BOY = 0
    GIRL = 1
    
def random_kid() -&amp;gt; Kid:
    return random.choice([Kid.BOY, Kid.GIRL])
    
both_girls = 0
older_girl = 0
either_girl = 0

random.seed(0)
for _ in range(10000):
    younger = random_kid()
    older = random_kid()
    if older == Kid.GIRL:
        older_girl += 1
    if older == Kid.GIRL and younger == Kid.GIRL:
        both_girls += 1
    if older == Kid.GIRL or younger == Kid.GIRL:
        either_girl += 1
        
print(&amp;quot;P(both | older):&amp;quot;, both_girls / older_girl)   # 0.5007089325501317
print(&amp;quot;P(both | either):&amp;quot;, both_girls / either_girl) # 0.3311897106109325
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that code confirms our intuition.&lt;/p&gt;
&lt;p&gt;We use a &lt;code&gt;for-loop&lt;/code&gt; and &lt;code&gt;range(10000)&lt;/code&gt; to randomly simulate 10,000 scenarios. The &lt;code&gt;random_kid&lt;/code&gt; function randomly picks either a boy or girl (assumption #1). We set the following variables to start a 0, &lt;code&gt;both_girls&lt;/code&gt; (both children are girls); &lt;code&gt;older_girl&lt;/code&gt; (first child is a girl); and &lt;code&gt;either_girl&lt;/code&gt; (at least one child is a girl).&lt;/p&gt;
&lt;p&gt;Then, each of these variables are incremented by 1 through each of the 10,000 loops if it meets certain conditions. After we finish looping, we can call on each of the three variables to see if they match our calculations above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;either_girl #7,464 / 10,000 ~ roughly 75% or 3/4 probability that there is at least one girl
both_girls  #2,472 / 10,000 ~ roughly 25% or 1/4 probability that both children are girls
older_girl  #4,937 / 10,000 ~ roughly 50% or 1/2 probability that the first child is a girl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will look at Bayes Theorem next.&lt;/p&gt;
&lt;h2 id=&#34;bayes_theorem&#34;&gt;Bayes_Theorem&lt;/h2&gt;
&lt;p&gt;Previously, we established an understanding of &lt;strong&gt;conditional&lt;/strong&gt; probability, but building up with &lt;strong&gt;marginal&lt;/strong&gt; and &lt;strong&gt;joint&lt;/strong&gt; probabilities. We explored the conditional probabilities of two outcomes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome one is roughly 50% or (1/2).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayes&#39; Theorem&lt;/strong&gt; is simply &lt;em&gt;an alternate&lt;/em&gt; way of calculating conditional probability.&lt;/p&gt;
&lt;p&gt;Previously, we used the &lt;strong&gt;joint&lt;/strong&gt; probability to calculate the &lt;strong&gt;conditional&lt;/strong&gt; probability.&lt;/p&gt;
&lt;h3 id=&#34;outcome-1&#34;&gt;Outcome 1&lt;/h3&gt;
&lt;p&gt;Here&amp;rsquo;s the conditional probability for outcome 1, using a joint probability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(G) = &amp;lsquo;Probability that first child is a girl&amp;rsquo; (1/2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B) = &amp;lsquo;Probability that both children are girls&amp;rsquo; (1/4)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|G) = P(B,G) / P(G)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|G) =  (1/4) / (1/2) = &lt;strong&gt;1/2&lt;/strong&gt; or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Technically, we &lt;em&gt;can&amp;rsquo;t&lt;/em&gt; use joint probability because the two events are &lt;em&gt;not independent&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To clarify, the probability of the older child being a certain gender and the probability of the younger child being a certain gender &lt;em&gt;is&lt;/em&gt; independent, but &lt;code&gt;P(B|G)&lt;/code&gt; the &amp;lsquo;probability of &lt;em&gt;both&lt;/em&gt; child being a girl&amp;rsquo; and &amp;lsquo;the probability of the older child being a girl&amp;rsquo; are &lt;em&gt;not independent&lt;/em&gt;; and hence we express it as a &lt;em&gt;conditional&lt;/em&gt; probability.&lt;/p&gt;
&lt;p&gt;So, the joint probability of &lt;code&gt;P(B,G)&lt;/code&gt; is just event B,&lt;code&gt;P(B)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an alternate way to calculate the conditional probability (&lt;strong&gt;without&lt;/strong&gt; joint probability):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(B|G) = P(G|B) * P(B) / P(G)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;P(B|G) = 1 * (1/4) / (1/2)&lt;/li&gt;
&lt;li&gt;P(B|G) = (1/4) * (2/1)&lt;/li&gt;
&lt;li&gt;P(B|G) = 1/2 = &lt;strong&gt;50%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: P(G|B) is &amp;lsquo;the probability that the first child is a girl, given that &lt;strong&gt;both&lt;/strong&gt; children are girls is a certainty (1.0)&amp;rsquo;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reverse&lt;/strong&gt; conditional probability, can also be calculated, without joint probability:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability of the older child being a girl, given that both children are girls?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(G|B) = P(B|G) * P(G) / P(B)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem (reverse case)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;P(G|B) = (1/2) * (1/2) / (1/4)&lt;/li&gt;
&lt;li&gt;P(G|B) = (1/4) / (1/4)&lt;/li&gt;
&lt;li&gt;P(G|B) = 1 = &lt;strong&gt;100%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is consistent with what we already derived above, namely that P(G|B) is a &lt;strong&gt;certainty&lt;/strong&gt; (probability = 1.0), that the older child is a girl, &lt;strong&gt;given that&lt;/strong&gt; both children are girls.&lt;/p&gt;
&lt;p&gt;We can point out two additional observations / rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;While, joint probabilities are &lt;strong&gt;symmetrical&lt;/strong&gt;: P(B,G) == P(G,B),&lt;/li&gt;
&lt;li&gt;Conditional probabilities are &lt;strong&gt;not symmetrical&lt;/strong&gt;: P(B|G) != P(G|B)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bayes-theorem-alternative-expression&#34;&gt;Bayes&#39; Theorem: Alternative Expression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt; is a way of calculating conditional probability &lt;em&gt;without&lt;/em&gt; the joint probability, summarized here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(B|G) = P(G|B) * P(B) / P(G)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(G|B) = P(B|G) * P(G) / P(B)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem (reverse case)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You&amp;rsquo;ll note that &lt;code&gt;P(G)&lt;/code&gt; is the denominator in the former, and &lt;code&gt;P(B)&lt;/code&gt; is the denominator in the latter.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What if, for some reasons, we don&amp;rsquo;t have access to the denominator?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We could derive both &lt;code&gt;P(G)&lt;/code&gt; and &lt;code&gt;P(B)&lt;/code&gt; in another way using the &lt;code&gt;NOT&lt;/code&gt; operator:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(G) = P(G,B) + P(G,not B) = P(G|B) * P(B) + P(G|not B) * P(not B)&lt;/li&gt;
&lt;li&gt;P(B) = P(B,G) + P(B,not G) = P(B|G) * P(G) + P(B|not G) * P(not G)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the alternative expression of Bayes Theorem for the probability of &lt;em&gt;both&lt;/em&gt; children being girls, given that the first child is a girl ( P(B|G) ) is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|G) = P(G|B) * P(B) / ( P(G|B) * P(B) + P(G|not B) * P(not B) )&lt;/li&gt;
&lt;li&gt;P(B|G) =     1 * 1/4 / (1 * 1/4 + 1/3 * 3/4)&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/4  /  (1/4 + 3/12)&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/4  /  2/4  =  1/4 * 4/2&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/2 or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can check the result in code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bayes_theorem(p_b, p_g_given_b, p_g_given_not_b):
   # calculate P(not B)
   not_b = 1 - p_b
   # calculate P(G)
   p_g = p_g_given_b * p_b + p_g_given_not_b * not_b
   # calculate P(B|G)
   p_b_given_g = (p_g_given_b * p_b) / p_g
   return p_b_given_g
   
#P(B)
p_b = 1/4

# P(G|B)
p_g_given_b = 1

# P(G|notB)
p_g_given_not_b = 1/3

# calculate P(B|G)
result = bayes_theorem(p_b, p_g_given_b, p_g_given_not_b)

# print result
print(&#39;P(B|G) = %.2f%%&#39; % (result * 100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the probability that the first child is a girl, given that &lt;em&gt;both&lt;/em&gt; children are girls ( P(G|B) ) is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(G|B) = P(B|G) * P(G) / ( P(G|B) * P(G) + P(B|not G) * P(not G) )&lt;/li&gt;
&lt;li&gt;P(G|B) =   1/2 * 1/2  / ((1/2 * 1/2) + (0 * 1/2))&lt;/li&gt;
&lt;li&gt;P(G|B) =  1/4  /  1/4&lt;/li&gt;
&lt;li&gt;P(G|B) = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s unpack Outcome 2.&lt;/p&gt;
&lt;h3 id=&#34;outcome-2&#34;&gt;Outcome 2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll go through the same process as above.&lt;/p&gt;
&lt;p&gt;We could use &lt;strong&gt;joint&lt;/strong&gt; probability to calculate the &lt;strong&gt;conditional&lt;/strong&gt; probability. As with the previous outcome, the joint probability of &lt;code&gt;P(B,G)&lt;/code&gt; is just event B,&lt;code&gt;P(B)&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(B,L) / P(L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Or, we could use Bayes&#39; Theorem to figure out the &lt;strong&gt;conditional&lt;/strong&gt; probability &lt;strong&gt;without joint&lt;/strong&gt; probability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(L|B) * P(B) / P(L)&lt;/li&gt;
&lt;li&gt;P(B|L) =  (1 * 1/4) / (3/4)&lt;/li&gt;
&lt;li&gt;P(B|L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And, if there&amp;rsquo;s no &lt;code&gt;P(L)&lt;/code&gt;, we can calculate that indirectly, also using Bayes&#39; Theorem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(L) = P(L|B) * P(B) + P(L|not B) * P(not B)&lt;/li&gt;
&lt;li&gt;P(L) =  1 * (1/4) + (2/3) * (3/4)&lt;/li&gt;
&lt;li&gt;P(L) =  (1/4) + (2/4)&lt;/li&gt;
&lt;li&gt;P(L) = 3/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, we can use &lt;code&gt;P(L)&lt;/code&gt; in the way Bayes&#39; Theorem is commonly expressed, when we don&amp;rsquo;t have the denominator:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(L|B) * P(B) / ( P(L|B) * P(B) + P(L|not B) * P(not B) )&lt;/li&gt;
&lt;li&gt;P(B|L) =  1 * (1/4) / (3/4)&lt;/li&gt;
&lt;li&gt;P(B|L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we&amp;rsquo;ve gone through the calculation for two conditional probabilities, &lt;code&gt;P(B|G)&lt;/code&gt; and &lt;code&gt;P(B|L)&lt;/code&gt;, using Bayes Theorem, and implemented code for one of the scenarios, let&amp;rsquo;s take a step back and assess what this &lt;em&gt;means&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bayesian-terminology&#34;&gt;Bayesian Terminology&lt;/h3&gt;
&lt;p&gt;I think its useful to understand that probability in general shines when we want to describe uncertainty and that Bayes&#39; Theorem allows us to quantify how much the data we observe, should change our beliefs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes_table.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have two &lt;strong&gt;posteriors&lt;/strong&gt;, &lt;code&gt;P(B|G)&lt;/code&gt; and &lt;code&gt;P(B|L)&lt;/code&gt;, both with equal &lt;strong&gt;priors&lt;/strong&gt; and &lt;strong&gt;likelihood&lt;/strong&gt;, but with &lt;em&gt;different&lt;/em&gt; &lt;strong&gt;evidence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Said differently, we want to know the &amp;lsquo;probability that both children are girls`, given &lt;em&gt;different&lt;/em&gt; conditions.&lt;/p&gt;
&lt;p&gt;In the first case, our condition is &amp;lsquo;the first child is a girl&amp;rsquo; and in the second case, our condition is &amp;lsquo;&lt;em&gt;at least one&lt;/em&gt; of the child is a girl&amp;rsquo;. The question is which condition will increase the probability that &lt;strong&gt;both&lt;/strong&gt; children are girls?&lt;/p&gt;
&lt;p&gt;Bayes&amp;rsquo; Theorem allows us to update our belief about the probability in these two cases, as we incorporate varied data into our framework.&lt;/p&gt;
&lt;p&gt;What the calculations tell us is that the &lt;strong&gt;evidence&lt;/strong&gt; that &amp;lsquo;one child is a girl&amp;rsquo; increases the probability that &lt;strong&gt;both&lt;/strong&gt; children are girls &lt;em&gt;more than&lt;/em&gt; the other piece of &lt;strong&gt;evidence&lt;/strong&gt; that &amp;lsquo;at least one child is a girl&amp;rsquo; increases that probability.&lt;/p&gt;
&lt;p&gt;And our beliefs should be updated accordingly.&lt;/p&gt;
&lt;p&gt;At the end of the day, understanding conditional probability (and Bayes Theorem) comes down to &lt;strong&gt;counting&lt;/strong&gt;. For our hypothetical scenarios, we only need one hand:&lt;/p&gt;
&lt;p&gt;When we look at the probability table for outcome one, &lt;code&gt;P(B|G)&lt;/code&gt;, we can see how the posterior probability comes out to 1/2:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./outcome_one.png&#34; alt=&#34;outcome_one&#34;&gt;&lt;/p&gt;
&lt;p&gt;When we look at the probability table for outcome two, &lt;code&gt;P(B|L)&lt;/code&gt;, we can see how the posterior probability comes out to 1/3:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./outcome_two.png&#34; alt=&#34;outcome_two&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is part of an ongoing series documenting my progress through Data Science from Scratch by Joel Grus:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./conditional_prob_ch6.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;applying_bayes_theorem&#34;&gt;Applying_Bayes_Theorem&lt;/h2&gt;
&lt;p&gt;Now that we have a basic understanding of Bayes Theorem, let&amp;rsquo;s extend the application to a slightly more complex example. This section was inspired by this 
&lt;a href=&#34;https://twitter.com/3blue1brown/status/1333121058824613889?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweet&lt;/a&gt; from Grant Sanderson (of 
&lt;a href=&#34;https://www.youtube.com/watch?v=HZGCoVF3YvM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown fame&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./grant_tweet.png&#34; alt=&#34;grant_tweet&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is a classic application of Bayes Theorem - the &lt;strong&gt;medical diagnostic scenario&lt;/strong&gt;. The above tweet can be re-stated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability of you &lt;em&gt;actually having the disease&lt;/em&gt;, given that you tested positive?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This happens to be even more relevant as we&amp;rsquo;re living through a generational pandemic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start off with a conceptual understanding, using the tools we learned previously. First, we have to keep in mind &lt;strong&gt;testing&lt;/strong&gt; and &lt;strong&gt;actually having the disease&lt;/strong&gt; are &lt;strong&gt;not independent&lt;/strong&gt; events. Therefore, we will use &lt;strong&gt;conditional probability&lt;/strong&gt; to express their joint outcomes.&lt;/p&gt;
&lt;p&gt;The intuitive visual to illustrate this is the &lt;strong&gt;tree diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./initial_tree.png&#34; alt=&#34;initial_tree&#34;&gt;&lt;/p&gt;
&lt;p&gt;The initial given information contains the information in the tree.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(D): Probability of having the disease (covid-19)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(P): Probability of testing positive&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;*P(D|P): Our objective is to find the probability of having the disease, given a positive test&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1 in 1,000 actively have covid-19, P(D), this implies&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;999 in 1,000 do &lt;strong&gt;not&lt;/strong&gt; actively have covid-19, P(not D)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1% or 0.01 false positive (given)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;10% or 0.1 false negative (given)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;false positive&lt;/strong&gt; is when you &lt;em&gt;don&amp;rsquo;t&lt;/em&gt; have the disease, but your test (in error) shows up positive. &lt;strong&gt;False negative&lt;/strong&gt; is when you &lt;em&gt;have&lt;/em&gt; the disease, but your test (in error) shows up negative. We are provided this information and have to calculate other values to fill in the tree.&lt;/p&gt;
&lt;p&gt;We know that all possible events have to add up to 1, so if 1 in 1,000 actively have the disease, we know that 999 in 1,000 do not have it. If the false negative is 10%, then the &lt;strong&gt;true positive&lt;/strong&gt; is 90%. If the false positive is 1%, then the &lt;strong&gt;true negative&lt;/strong&gt; is 99%. From our calculations, the tree can be updated:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./second_tree.png&#34; alt=&#34;second_tree&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we&amp;rsquo;ve filled out the tree, we can use &lt;strong&gt;Bayes&#39; Theorem&lt;/strong&gt; to find &lt;code&gt;P(D|P)&lt;/code&gt;. Here&amp;rsquo;s Bayes&#39; Theorem that we discussed in the previous section. We have Bayes&#39; Theorem, the denominator, probability of testing positive &lt;code&gt;P(P)&lt;/code&gt; and the &lt;em&gt;second&lt;/em&gt; version of Bayes Theorem in cases were we &lt;em&gt;do not know&lt;/em&gt; the probability of testing positive (as in the present case):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes1.png&#34; alt=&#34;bayes1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then we can plug-in the denominator to get the alternative version of Bayes&#39; Theorem:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes2.png&#34; alt=&#34;bayes2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how the numbers add up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(D|P) = P(P|D) * P(D) / P(P|D) * P(D) + P(P|not D) * P(not D)&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.9 * 0.001 / 0.9 * 0.001 + 0.01 * 0.999&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.0009 / 0.0009 + 0.00999&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.0009 / 0.01089&lt;/li&gt;
&lt;li&gt;P(D|P) ~ 0.08264 or 8.26%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interestingly, 
&lt;a href=&#34;https://twitter.com/karpathy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy&lt;/a&gt; actually 
&lt;a href=&#34;https://twitter.com/karpathy/status/1333217287155847169?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;responded in the thread&lt;/a&gt; and provided an intuitive way to arrive at the same result using Python.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s his code (with added comments):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from random import random, seed
seed(0)

pop = 10000000 # 10M people
counts = {}

for i in range(pop):
    has_covid = i % 1000 == 0 # one in 1,000 people have covid (priors or prevalence of disease)
    # The major assumption is that every person gets tested regardless of any symptoms
    if has_covid:                  # Has disease
        tests_positive = True      # True positive
        if random() &amp;lt; 0.1:     
            tests_positive = False # False negative
    else:                          # Does not have disease
        tests_positive = False     # True negative
        if random() &amp;lt; 0.01:    
            tests_positive = True  # False positive
    outcome = (has_covid, tests_positive)
    counts[outcome] = counts.get(outcome, 0) + 1
    
for (has_covid, tested_positive), n in counts.items():
    print(&#39;has covid: %6s, tests positive: %6s, count: %d&#39; % (has_covid, tested_positive, n))
    
n_positive = counts[(True, True)] + counts[(False, True)]

print(&#39;number of people who tested positive:&#39;, n_positive)
print(&#39;probability that a test-positive person actually has covid: %.2f&#39; % (100.0 * counts[(True, True)] / n_positive), )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first build a hypothetical population of 10 million. If the &lt;strong&gt;prior&lt;/strong&gt; or &lt;strong&gt;prevalence&lt;/strong&gt; of disease is 1 in 1,000, a population of 10 million should find 10000 people with covid. You can see how this works with this short snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pop = 10000000
counts = 0

for i in range(pop):
    has_covid = i % 1000 == 0
    if has_covid:
        counts = counts + 1
print(counts, &amp;quot;people have the disease in a population of 10 million&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nested in the &lt;code&gt;for-loop&lt;/code&gt; are &lt;code&gt;if-statements&lt;/code&gt; that segment the population (10M) into one of four categories True Positive, False Negative, True Negative, False Positive. Each category is counted and stored in a &lt;code&gt;dict&lt;/code&gt; called &lt;code&gt;counts&lt;/code&gt;. Then another &lt;code&gt;for-loop&lt;/code&gt; is used to loop through this dictionary to print out all the categories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;has covid:   True, tests positive:   True, count: 9033
has covid:  False, tests positive:  False, count: 9890133
has covid:  False, tests positive:   True, count: 99867
has covid:   True, tests positive:  False, count: 967

number of people who tested positive: 108900
probability that a test-positive person actually has covid: 8.29
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we want the number of people who &lt;em&gt;have&lt;/em&gt; the disease &lt;em&gt;and&lt;/em&gt; tested positive (True Positive, 9033) divided by the number of people who tested positive, regardless of whether they actually have the disease (True Positive (9033) + False Positive (99867) = 108,900) and this comes out to approximately 8.29.&lt;/p&gt;
&lt;p&gt;Although the 
&lt;a href=&#34;https://twitter.com/karpathy/status/1333217287155847169?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; was billed as &amp;ldquo;simple code to build intuition&amp;rdquo;, I found that Bayes&#39; Theorem &lt;em&gt;is&lt;/em&gt; the intuition.&lt;/p&gt;
&lt;h3 id=&#34;what-about-symptoms&#34;&gt;What about symptoms?&lt;/h3&gt;
&lt;p&gt;The key to Bayes&#39; Theorem is that it encourages us to update our beliefs when presented with new evidence. But what if there&amp;rsquo;s evidence we missed in the first place?&lt;/p&gt;
&lt;p&gt;If you look back at the 
&lt;a href=&#34;https://twitter.com/3blue1brown/status/1333121058824613889?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original tweet&lt;/a&gt;, there are important details about symptoms that, if we wanted to be more realistic, should be accounted for.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You feel fatigued and have a slight sore throat.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, instead of assuming that prevalence of the disease (1 in 1,000 people have covid-19) is the prior, we might ask what probability that someone who is symptomatic has the disease?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s suppose we change from 1 in 1,000 to 1 in 100. We could change just one line of code (while everything else remains the same):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(pop):
    has_covid = i % 100 == 0 # update info: 1/1000 have covid, but 1/100 with symptoms have covid
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability that someone with a positive test actually has the disease jumps from 8.29% to 47.61%&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;has covid:   True, tests positive:   True, count: 180224
has covid:  False, tests positive:  False, count: 19601715
has covid:  False, tests positive:   True, count: 198285
has covid:   True, tests positive:  False, count: 19776
number of people who tested positive: 378509
probability that a test-positive person with symptoms actually has covid: 47.61
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, being symptomatic means our &lt;strong&gt;priors&lt;/strong&gt; should be adjusted and our &lt;strong&gt;beliefs&lt;/strong&gt; about the likelihood that a positive test means we have the disease (&lt;code&gt;P(D|P)&lt;/code&gt;) should be updated accordingly (in this case, it goes way up).&lt;/p&gt;
&lt;h3 id=&#34;take-aways&#34;&gt;Take Aways&lt;/h3&gt;
&lt;p&gt;Hypothetically, if we have family or friends living in an area where 1 in 1,000 people have covid-19 and they (god forbid) got tested and got a positive result, you could tell them that their probability of actually having the disease, given a positive test was around 8.26â8.29%.&lt;/p&gt;
&lt;p&gt;However, whatâs useful about the Bayesian approach is that it encourages us to incorporate new information and update our beliefs accordingly. So if we find out our family or friend is also &lt;em&gt;symptomatic&lt;/em&gt;, we could advise them of the higher probability (~47.61%).&lt;/p&gt;
&lt;p&gt;Finally, we may also advise our family/friends to get tested &lt;strong&gt;again&lt;/strong&gt;, because as much as test-positive person would hope they got a âfalse positiveâ, chances are low. And even lower, is getting a false positive &lt;em&gt;twice&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./second_test.png&#34; alt=&#34;second_test&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll cover various distributions. This is a broad topic so we&amp;rsquo;ll sample a few concepts to get a feel for it. Borrowing from the previous post, we&amp;rsquo;ll chart our medical diagnostic outcomes.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll recall that each outcome is the combination of whether someone has a disease, &lt;code&gt;P(D)&lt;/code&gt;, or not, &lt;code&gt;P(not D)&lt;/code&gt;. Then, they&amp;rsquo;re given a diagnostic test that returns positive, &lt;code&gt;P(P)&lt;/code&gt; or negative, &lt;code&gt;P(not P)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;These are discrete outcomes so they can be represented with the &lt;strong&gt;probability mass function&lt;/strong&gt;, as opposed to a &lt;strong&gt;probability density function&lt;/strong&gt;, which represent a continuous distribution.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take another &lt;em&gt;hypothetical&lt;/em&gt; scenario of a city where 1 in 10 people have a disease and a diagnostic test has a True Positive of 95% and True Negative of 90%. The probability that a test-positive person &lt;em&gt;actually&lt;/em&gt; having the disease is 46.50%.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from random import random, seed

seed(0)
pop = 1000  # 1000 people
counts = {}
for i in range(pop):
    has_disease = i % 10 == 0  # one in 10 people have disease
    # assuming that every person gets tested regardless of any symptoms
    if has_disease:
        tests_positive = True       # True Positive  95%
        if random() &amp;lt; 0.05:
            tests_positive = False  # False Negative 5%
    else:
        tests_positive = False      # True Negative  90%
        if random() &amp;lt; 0.1:
            tests_positive = True   # False Positive 10%
    outcome = (has_disease, tests_positive)
    counts[outcome] = counts.get(outcome, 0) + 1

for (has_disease, tested_positive), n in counts.items():
    print(&#39;Has Disease: %6s, Test Positive: %6s, count: %d&#39; %
          (has_disease, tested_positive, n))

n_positive = counts[(True, True)] + counts[(False, True)]
print(&#39;Number of people who tested positive:&#39;, n_positive)
print(&#39;Probability that a test-positive person actually has disease: %.2f&#39; %
      (100.0 * counts[(True, True)] / n_positive),)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given the probability that someone has the disease (1 in 10), also called the &amp;lsquo;prior&amp;rsquo; in Bayesian terms. We modeled four scenarios where people were given a diagnostic test. Again, the big assumption here is that people get randomly tested. With the true positive and true negative rates stated above, here are the outcomes:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./hypothetical_outcome.png&#34; alt=&#34;hypothetical_outcome&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;probability-mass-function&#34;&gt;Probability Mass Function&lt;/h3&gt;
&lt;p&gt;Given these discrete events, we can chart a &lt;strong&gt;probability mass function&lt;/strong&gt;, also known as 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_mass_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discrete density function&lt;/a&gt;. We&amp;rsquo;ll import &lt;code&gt;pandas&lt;/code&gt; to help us create &lt;code&gt;DataFrames&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; to chart the &lt;strong&gt;probability mass function&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We first need to turn the counts of events into a &lt;code&gt;DataFrame&lt;/code&gt; and change the column to &lt;code&gt;item_counts&lt;/code&gt;. Then, we&amp;rsquo;ll calculate the probability of each event by dividing the count by the total number of people in our hypothetical city (i.e., population: 1000).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optional&lt;/strong&gt;: Create another column with abbreviations for test outcome (i.e., &amp;ldquo;True True&amp;rdquo; becomes &amp;ldquo;TT&amp;rdquo;). We&amp;rsquo;ll call this column &lt;code&gt;item2&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame.from_dict(counts, orient=&#39;index&#39;)
df = df.rename(columns={0: &#39;item_counts&#39;})
df[&#39;probability&#39;] = df[&#39;item_counts&#39;]/1000
df[&#39;item2&#39;] = [&#39;TT&#39;, &#39;FF&#39;, &#39;FT&#39;, &#39;TF&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the &lt;code&gt;DataFrame&lt;/code&gt; we have so far:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./pmf_df.png&#34; alt=&#34;pmf_df&#34;&gt;&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll note that the numbers in the &lt;code&gt;probability&lt;/code&gt; column adds up to 1.0 and that the &lt;code&gt;item_counts&lt;/code&gt; numbers are the same as the count above when we had calculated the probability of a test-positive person actually having the disease.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll use a simple bar chart to chart out the diagnostic probabilities and this is how we&amp;rsquo;d visually represent the probability mass function - probabilities of each discrete event; each &amp;lsquo;discrete event&amp;rsquo; is a conditional (e.g., probability that someone has a positive test, given that they &lt;em&gt;have&lt;/em&gt; the disease - TT or probability that someone has a negative test, given that they &lt;em&gt;don&amp;rsquo;t have&lt;/em&gt; the disease - FF, and so on).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./prob_mass_function.png&#34; alt=&#34;prob_mass_function.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.DataFrame.from_dict(counts, orient=&#39;index&#39;)
df = df.rename(columns={0: &#39;item_counts&#39;})
df[&#39;probability&#39;] = df[&#39;item_counts&#39;]/1000
df[&#39;item2&#39;] = [&#39;TT&#39;, &#39;FF&#39;, &#39;FT&#39;, &#39;TF&#39;]
plt.bar(df[&#39;item2&#39;], df[&#39;probability&#39;])
plt.title(&amp;quot;Probability Mass Function&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;cumulative-distribution-function&#34;&gt;Cumulative Distribution Function&lt;/h3&gt;
&lt;p&gt;While the probability mass function can tell us the probability of each discrete event (i.e., TT, FF, FT, and TF) we can also represent the same information as a &lt;strong&gt;cumulative distribution function&lt;/strong&gt; which allows us to see how the probability changes as we add events together.&lt;/p&gt;
&lt;p&gt;The cumulative distribution function simply adds the probability from the previous row in a &lt;code&gt;DataFrame&lt;/code&gt; in a cumulative fashion, like in the column &lt;code&gt;probability2&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cdf_df.png&#34; alt=&#34;cdf_df.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;cumsum()&lt;/code&gt; function to create the &lt;code&gt;cumsum&lt;/code&gt; column which is simply adding the &lt;code&gt;item_counts&lt;/code&gt;, with each successive row. When we create the corresponding probability column, &lt;code&gt;probability2&lt;/code&gt;, it gets larger until we reach 1.0.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cum_distri_function.png&#34; alt=&#34;cum_distri_function&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart tells us that the probability of getting both TT and FF (True, True = True Positive, and False, False = True Negative) is 88.6% which indicates that 11.4% (100 - 88.6) of the time, the diagnostic test will let us down.&lt;/p&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;p&gt;More often than not, you&amp;rsquo;ll be interested in &lt;em&gt;continuous&lt;/em&gt; distributions and you can see better see how the &lt;strong&gt;cumulative distribution function&lt;/strong&gt; works.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;re probably familiar with the bell shaped curve or the &lt;em&gt;normal distribution&lt;/em&gt;, defined solely by its mean (mu) and standard deviation (sigma). If you have a &lt;strong&gt;standard normal distribution&lt;/strong&gt; of probability values, the average would be 0 and the standard deviation would be 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./1_normal.png&#34; alt=&#34;1_normal&#34;&gt;&lt;/p&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math
SQRT_TWO_PI = math.sqrt(2 * math.pi)

def normal_pdf(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (SQRT_TWO_PI * sigma))
    
# plot
xs = [x / 10.0 for x in range(-50, 50)]
plt.plot(xs, [normal_pdf(x, sigma=1) for x in xs], &#39;-&#39;, label=&#39;mu=0, sigma=1&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;strong&gt;standard normal distribution&lt;/strong&gt; curve, you see the average probability is around 0.4. But if you add up the area under the curve (i.e., all probabilities of every possible outcome), you would get 1.0, just like with the medical diagnostic example.&lt;/p&gt;
&lt;p&gt;And if you split the bell in half, then flip over the left half, you&amp;rsquo;ll (visually) get the &lt;strong&gt;cumulative distribution function&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./1_cumu.png&#34; alt=&#34;1_cumu&#34;&gt;&lt;/p&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math

def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2
    
# plot
xs = [x / 10.0 for x in range(-50, 50)]
plt.plot(xs, [normal_cdf(x, sigma=1) for x in xs], &#39;-&#39;, label=&#39;mu=0,sigma=1&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In both cases, the area under the curve for the &lt;strong&gt;standard normal distribution&lt;/strong&gt; and the &lt;strong&gt;cumulative distribution function&lt;/strong&gt; is 1.0, thus summing the probabilities of all events is one.&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>End-to-End Projects</title>
      <link>/post/end_to_end/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/end_to_end/</guid>
      <description>&lt;h2 id=&#34;2021-goals&#34;&gt;2021 Goals&lt;/h2&gt;
&lt;p&gt;One of my goals for 2021 is to build up a portfolio of end-to-end machine learning projects. In this post, I&amp;rsquo;ll keep a running list of resources for inspiration:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.kdnuggets.com/2020/10/guide-authentic-data-science-portfolio-project.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science Portfolio Projects: A Step-by-Step Guide&lt;/a&gt; (by 
&lt;a href=&#34;https://www.linkedin.com/in/felix-vemmer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Felix Vemmer&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;This is a clear step-by-step guide. I like the emphasis on web scraping which is where I need to focus my skills on next.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;bit.ly/berkeleyfsdl&#34;&gt;Full Stack Deep Learning (at Berkeley)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This looks to be a promising course that covers: &amp;ldquo;a promising experiment to a shipped product: project structure, useful tooling, data management, best practices for deployment, social responsibility, and finding a job or starting a venture&amp;rdquo;. The course is &lt;strong&gt;entirely online&lt;/strong&gt;. See this 
&lt;a href=&#34;https://twitter.com/full_stack_dl/status/1329477077733609480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweet thread&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://madewithml.com/courses/applied-ml-in-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied ML in Production&lt;/a&gt; by 
&lt;a href=&#34;https://twitter.com/GokuMohandas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goku Mohandas&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This aims to be a &amp;ldquo;guide and code-driven case study on MLOps for software engineers, data scientists and product managers&amp;hellip;developing an end-to-end ML feature, from product &amp;ndash;&amp;gt; ML &amp;ndash;&amp;gt; production, with open source tools&amp;rdquo;. Sounds very promising.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://end-to-end-machine-learning.teachable.com/p/complete-course-library-full-end-to-end-machine-learning-catalog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;End-to-End Machine Learning Course Catalog&lt;/a&gt; by 
&lt;a href=&#34;https://twitter.com/_brohrer_&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brandon Rohrer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/PrasoonPratham/status/1330372876134912000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;First 30 days of Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tweet thread by 
&lt;a href=&#34;https://twitter.com/PrasoonPratham&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pratham Prasoon&lt;/a&gt;, as the title suggests, is for newcomers to ML, but I think by the end of the sequence (doesn&amp;rsquo;t have to be 30 days) there&amp;rsquo;s a Kaggle project to complete. &lt;em&gt;note&lt;/em&gt;: this is not ML-in-production like some of the other resources, but Kaggle projects are great for learning.&lt;/p&gt;
&lt;p&gt;He has another thread 
&lt;a href=&#34;https://twitter.com/PrasoonPratham/status/1325331515090219008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;worth checking out&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jangiacomelli/status/1331170945738760192&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Suggested Project from Jan Giacomelli&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a pretty ð¥ thread. He suggests:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Build an expense tracker CLI app:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each expensee should have the following: title (string), amount(float), created_at(date), tags(list of strings)&lt;/p&gt;
&lt;p&gt;2 Add Database&lt;/p&gt;
&lt;p&gt;Instead of storing/reading in/from TXT file, start using SQLite. Write script to copy all of the existing expenses from TXT file to database. Don&amp;rsquo;t use ORM at this point.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Start using Classes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Represent expense with class Expense having attributes: title(string), amount(float), created_at(date), tags(list of strings).&lt;/p&gt;
&lt;p&gt;Represent Database with class ExpenseRepository with methods: save, get_by_id, list, delete&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Re-write App to use Commands and Queries&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each command/query is a class with method execute.
At initialization you need to provide all required data for execution.&lt;/p&gt;
&lt;p&gt;Commands: AddExpense, EditExpense
Queries: GetById, ListAll&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/modern-tdd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modern Test-Driven Development in Python&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Add Tests&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Add tests for commands and queries&lt;/p&gt;
&lt;p&gt;Example:
GIVEN Valid data
WHEN execute method is called on AddExpense command
THEN record is created in database with same attributes as provided&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/modern-tdd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modern Test-Driven Development in Python&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Flask&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use Flask to build the web application for your expense tracker.
Reuse commands and queries inside views
Use Jinja2 for HTML templating
Add integration tests for endpoints&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;PostgreSQL&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Start using PostgreSQL instead of SQLite.
You should only edit ExpenseRepository.
Create script to copy all existing data from SQLite to Postgres&lt;/p&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Add sign up and login to your Flask application
Protect endpoints for expenses to allow only logged in users to use them
Allow user to only see own expenses.&lt;/p&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;Dockerize and Deploy&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dockerize your Flask application
Deploy to Heroku (don&amp;rsquo;t use DB in docker, use it on Heroku)&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/dockerizing-flask-with-postgres-gunicorn-and-nginx/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dockerizing Flask with Postgres, Gunicorn and Nginx&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;Start using your application for real&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Start tracking your expenses
Even the most little ones
Don&amp;rsquo;t forget to add them daily&lt;/p&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;Data Analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use Pandas and Matplotlib to analyze your expenses
Check frequency, check biggest amount, smallest amount, average amount, most frequent amount and most used tags&amp;hellip;&lt;/p&gt;
&lt;p&gt;Draw plots: Number of expenses per day, amount spent per day&lt;/p&gt;
&lt;ol start=&#34;12&#34;&gt;
&lt;li&gt;ML&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Build model which will predict tags based on the title of expense
Use your existing records
Although your data set is small, try to build model as precise as possible&lt;/p&gt;
&lt;ol start=&#34;13&#34;&gt;
&lt;li&gt;Congratulate yourself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Don&amp;rsquo;t forget to write a blog post for each of these steps.
Don&amp;rsquo;t forget to share your code in a public git repository (GitHub)
Don&amp;rsquo;t forget to tweet it out
Don&amp;rsquo;t forget to add all the skills to LinkedIn&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science from Scratch (ch5) - Statistics</title>
      <link>/post/dsfs_5/</link>
      <pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_5/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#describing&#34;&gt;Describing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#correlation&#34;&gt;Finding Relationships in Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This post is chapter 5 in continuation of my coverage of 
&lt;a href=&#34;https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science from Scratch by Joel Grus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It should be noted upfront that everything covered in this post can be done more expediently and efficiently in libraries like 
&lt;a href=&#34;https://numpy.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NumPy&lt;/a&gt; as well as the 
&lt;a href=&#34;https://github.com/python/cpython/blob/3.9/Lib/statistics.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statistics module in Python&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The primary value of this book, and by extension this post, in my opinion, is the emphasis on &lt;strong&gt;learning&lt;/strong&gt; how Python primitives can be used to build tools from the ground up.&lt;/p&gt;
&lt;p&gt;Specifically, we&amp;rsquo;ll examine how specific features of the Python language as well as functions we built in a previous post on 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linear algebra&lt;/a&gt; can be used to build tools used to &lt;em&gt;describe&lt;/em&gt; data and relationships within data (aka &lt;strong&gt;statistics&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;I think this is pretty cool. Hopefully you agree.&lt;/p&gt;
&lt;h4 id=&#34;example-data&#34;&gt;Example Data&lt;/h4&gt;
&lt;p&gt;This chapter continues the narrative of you as a newly hired data scientist at DataScienster, the social network for data scientists, and your job is to &lt;em&gt;describe&lt;/em&gt; how many friends members in this social network has. We have two &lt;code&gt;lists&lt;/code&gt; of &lt;code&gt;float&lt;/code&gt; to work with. We&amp;rsquo;ll work with &lt;code&gt;num_friends&lt;/code&gt; first, then &lt;code&gt;daily_minutes&lt;/code&gt; later.&lt;/p&gt;
&lt;p&gt;I wanted this post to be self-contained, and in order to do that we&amp;rsquo;ll have to read in a larger than average &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;floats&lt;/code&gt;. The alternative would be to get the data directly from the book&amp;rsquo;s 
&lt;a href=&#34;https://github.com/joelgrus/data-science-from-scratch/blob/master/scratch/statistics.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github repo (statistics.py)&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_friends = [100.0,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]

daily_minutes = [1,68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]

daily_hours = [dm / 60 for dm in daily_minutes]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;describing&#34;&gt;Describing&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;num_friends&lt;/code&gt; list is a list of numbers representing &amp;ldquo;number of friends&amp;rdquo; a person has, so for example, one person has 100 friends. The first thing we do to describe the data is to create a bar chart plotting the number of people who have 100 friends, 49 friends, 41 friends, and so on.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll import &lt;code&gt;Counter&lt;/code&gt; from &lt;code&gt;collections&lt;/code&gt; and import &lt;code&gt;matplotlib.pyplot&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll use &lt;code&gt;Counter&lt;/code&gt; to turn &lt;code&gt;num_friends&lt;/code&gt; list into a &lt;code&gt;defaultdict(int)&lt;/code&gt;-like object mapping keys to counts. For more info, please refer to this 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_2/#counters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; on the Counters.&lt;/p&gt;
&lt;p&gt;Once we use the &lt;code&gt;Counter&lt;/code&gt; collection, a 
&lt;a href=&#34;https://docs.python.org/2/library/collections.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;high-performance container datatype&lt;/a&gt;, we can use methods like &lt;code&gt;most_common&lt;/code&gt; to find the keys with the most common values. Here we see that the five most common &lt;em&gt;number of friends&lt;/em&gt; are 6, 1, 4, 3 and 9, respectively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter

import matplotlib.pyplot as plt

friend_counts = Counter(num_friends)

# the five most common values are: 6, 1, 4, 3 and 9 friends
# [(6, 22), (1, 22), (4, 20), (3, 20), (9, 18)]
friend_counts.most_common(5) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To proceed with plotting, we&amp;rsquo;ll use &lt;code&gt;friend_counts&lt;/code&gt; to create a &lt;code&gt;list comprehension&lt;/code&gt; that will loop through &lt;code&gt;friends_count&lt;/code&gt; and for all &lt;strong&gt;keys&lt;/strong&gt; from 0-101 (xs) and print a corresponding &lt;strong&gt;value&lt;/strong&gt; (if it exists). This becomes the y-axis to &lt;code&gt;num_friends&lt;/code&gt;, which is the x-axis:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xs = range(101)                     # x-axis: largest num_friend value is 100
ys = [friend_counts[x] for x in xs] # y-axis
plt.bar(xs, ys)
plt.axis([0, 101, 0, 25])
plt.title(&amp;quot;Histogram of Friend Counts&amp;quot;)
plt.xlabel(&amp;quot;# of friends&amp;quot;)
plt.ylabel(&amp;quot;# of people&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the plot below. You can see one person with 100 friends.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./histo_friend_counts.png&#34; alt=&#34;histo_friend_counts.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can also read more about data visualization 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Alternatively, we could generate simple statistics to describe the data using built-in Python methods: &lt;code&gt;len&lt;/code&gt;, &lt;code&gt;min&lt;/code&gt;, &lt;code&gt;max&lt;/code&gt; and &lt;code&gt;sorted&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_points = len(num_friends) # number of data points in num_friends: 204
largest_value = max(num_friends) # largest value in num_friends: 100
smallest_value = min(num_friends) # smallest value in num_friends: 1

sorted_values = sorted(num_friends) # sort the values in ascending order
second_largest_value = sorted_values[-2] # second largest value from the back: 49
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;central-tendencies&#34;&gt;Central Tendencies&lt;/h3&gt;
&lt;p&gt;The most common way of describing a set of data is to find it&amp;rsquo;s &lt;strong&gt;mean&lt;/strong&gt;, which is the sum of all the values, divided by the number of values. &lt;em&gt;note&lt;/em&gt; : we&amp;rsquo;ll continue to use type annotations. In my opinion, it helps you be a more deliberate and mindful Python programmer.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List

def mean(xs: List[float]) -&amp;gt; float:
    return sum(xs) / len(xs)
    
assert 7.3333 &amp;lt; mean(num_friends) &amp;lt; 7.3334
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the mean is &lt;strong&gt;notoriously sensitive to outliers&lt;/strong&gt; so statisticians often supplement with other measures of central tendencies like &lt;strong&gt;median&lt;/strong&gt;. Because the median is the &lt;em&gt;middle-most value&lt;/em&gt;, it matters whether there is an &lt;em&gt;even&lt;/em&gt; or &lt;em&gt;odd&lt;/em&gt; number of data points.&lt;/p&gt;
&lt;p&gt;Here, we&amp;rsquo;ll create two private functions for both situations - even and odd number of data points - in calculating the median. First, we&amp;rsquo;ll sort the data values. Then, for &lt;em&gt;even number&lt;/em&gt; values, we&amp;rsquo;ll find the two middle values and split them. For &lt;em&gt;odd number&lt;/em&gt; of values, we&amp;rsquo;ll divide the &lt;em&gt;length&lt;/em&gt; of the dataset by 2 (i.e., 50).&lt;/p&gt;
&lt;p&gt;Our median function will return either of the private function &lt;code&gt;_median_even&lt;/code&gt; or &lt;code&gt;_median_odd&lt;/code&gt; conditionally depending on if the length of a list of numbers is divisible (%2==0) by 2.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _median_even(xs: List[float]) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;If len(xs) is even, it&#39;s the average of the middle two elements&amp;quot;&amp;quot;&amp;quot;
    sorted_xs = sorted(xs)
    hi_midpoint = len(xs) // 2   # e.g. length 4 =&amp;gt; hi_midpoint 2
    return (sorted_xs[hi_midpoint - 1] + sorted_xs[hi_midpoint]) / 2
    
def _median_odd(xs: List[float]) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;If len(xs) is odd, its the middle element&amp;quot;&amp;quot;&amp;quot;
    return sorted(xs)[len(xs) // 2]
    
def median(v: List[float]) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Finds the &#39;middle-most&#39; value of v&amp;quot;&amp;quot;&amp;quot;
    return _median_even(v) if len(v) % 2 == 0 else _median_odd(v)
    
assert median([1,10,2,9,5]) == 5
assert median([1, 9, 2, 10]) == (2 + 9) / 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because the median is the &lt;em&gt;middle-most value&lt;/em&gt;, it does not fully depend on every value in the data. For illustration, hypothetically if we have a another list &lt;code&gt;num_friends2&lt;/code&gt; where one person had 10,000 friends, the &lt;strong&gt;mean&lt;/strong&gt; would be much more sensitive to that change than the &lt;strong&gt;median&lt;/strong&gt; would be.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_friends2 = [10000.0,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14
    ,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9
    ,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7
    ,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5
    ,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3
    ,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1
    ,1,1,1,1,1,1,1,1,1,1,1,1]
    
mean(num_friends2)   # more sensitive to outliers: 7.333 =&amp;gt; 55.86274509803921
median(num_friends2) # less sensitive to outliers: 6.0 =&amp;gt; 6.0

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may also used &lt;code&gt;quantiles&lt;/code&gt; to describe your data. Whenever you&amp;rsquo;ve heard &amp;ldquo;X percentile&amp;rdquo;, that is a description of quantiles relative to 100. In fact, the median is the 50th percentile (where 50% of the data lies below this point and 50% lies above).&lt;/p&gt;
&lt;p&gt;Because &lt;code&gt;quantile&lt;/code&gt; is a position from 0-100, the second argument is a float from 0.0 to 1.0. We&amp;rsquo;ll use that float to multiply with the length of the list. Then we&amp;rsquo;ll wrap in &lt;code&gt;int&lt;/code&gt; to create an integer index which we&amp;rsquo;ll use on a sorted xs to find the quantile.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def quantile(xs: List[float], p: float) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Returns the pth-percentile value in x&amp;quot;&amp;quot;&amp;quot;
    p_index = int(p * len(xs))  
    return sorted(xs)[p_index]
    
assert quantile(num_friends, 0.10) == 1
assert quantile(num_friends, 0.25) == 3
assert quantile(num_friends, 0.75) == 9
assert quantile(num_friends, 0.90) == 13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we have the &lt;strong&gt;mode&lt;/strong&gt;, which looks at the most common values. First, we use the &lt;code&gt;Counter&lt;/code&gt; method on our list parameter and since Counter is a subclass of &lt;code&gt;dict&lt;/code&gt; we have access to methods like &lt;code&gt;values()&lt;/code&gt; to find all the values and &lt;code&gt;items()&lt;/code&gt; to find key value pairs.&lt;/p&gt;
&lt;p&gt;We define &lt;code&gt;max_count&lt;/code&gt; to find the max value (22), then the function returns a list comprehension which loops through &lt;code&gt;counts.items()&lt;/code&gt; to find the key associated with the max_count (22). That is 1 and 6, meaning twenty-two people (the &lt;strong&gt;mode&lt;/strong&gt;) had one or six friends.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def mode(x: List[float]) -&amp;gt; List[float]:
    &amp;quot;&amp;quot;&amp;quot;Returns a list, since there might be more than one mode&amp;quot;&amp;quot;&amp;quot;
    counts = Counter(x)
    max_count = max(counts.values())
    return [x_i for x_i, count in counts.items() if count == max_count]
    

assert set(mode(num_friends)) == {1, 6}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we had already used Counter on &lt;code&gt;num_friends&lt;/code&gt; previously (see &lt;code&gt;friend_counts&lt;/code&gt;), we could have just called the &lt;code&gt;most_common(2)&lt;/code&gt; method to get the same results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mode(num_friends) # [6, 1]
friend_counts.most_common(2) # [(6, 22), (1, 22)]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dispersion&#34;&gt;Dispersion&lt;/h3&gt;
&lt;p&gt;Aside from our data&amp;rsquo;s central tendencies, we&amp;rsquo;ll also want to understand it&amp;rsquo;s spread or dispersion. The tools to do this are &lt;code&gt;data_range&lt;/code&gt;, &lt;code&gt;variance&lt;/code&gt;, &lt;code&gt;standard deviation&lt;/code&gt; and &lt;code&gt;interquartile range&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Range is a straightforward max value minus min value.&lt;/p&gt;
&lt;p&gt;Variance measures how far a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Variance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;set of numbers is from their average value&lt;/a&gt;. What&amp;rsquo;s more interesting, for our purpose, is how we need to borrow the functions we had previously built in the 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linear algebra&lt;/a&gt; post to create the variance function.&lt;/p&gt;
&lt;p&gt;If you look at its wikipedia page, &lt;strong&gt;variance&lt;/strong&gt; is the &lt;em&gt;squared deviation&lt;/em&gt; of a variable from its mean.&lt;/p&gt;
&lt;p&gt;First, we&amp;rsquo;ll need to create the &lt;code&gt;de_mean&lt;/code&gt; function that takes a list of numbers and subtract from all numbers in the list, the mean value (this gives us the deviation from the mean).&lt;/p&gt;
&lt;p&gt;Then, we&amp;rsquo;ll &lt;code&gt;sum_of_squares&lt;/code&gt; all those deviations, which means we&amp;rsquo;ll take all the values, multiply them with itself (square it), then add the values (and divide by length of the list minus one) to get the variance.&lt;/p&gt;
&lt;p&gt;Recall that the &lt;code&gt;sum_of_squares&lt;/code&gt; is a special case of the &lt;code&gt;dot&lt;/code&gt; product function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# variance

from typing import List

Vector = List[float]

# see vectors.py in chapter 4 for dot and sum_of_squares

def dot(v: Vector, w: Vector) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Computes v_1 * w_1 + ... + v_n * w_n&amp;quot;&amp;quot;&amp;quot;
    assert len(v) == len(w), &amp;quot;vectors must be the same length&amp;quot;
    return sum(v_i * w_i for v_i, w_i in zip(v,w))
    
def sum_of_squares(v: Vector) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Returns v_1 * v_1 + ... + v_n * v_n&amp;quot;&amp;quot;&amp;quot;
    return dot(v,v)
    
def de_mean(xs: List[float]) -&amp;gt; List[float]:
    &amp;quot;&amp;quot;&amp;quot;Translate xs by subtracting its mean (so the result has mean 0)&amp;quot;&amp;quot;&amp;quot;
    x_bar = mean(xs)
    return [x - x_bar for x in xs]
    
def variance(xs: List[float]) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Almost the average squared deviation from the mean&amp;quot;&amp;quot;&amp;quot;
    assert len(xs) &amp;gt;= 2, &amp;quot;variance requires at least two elements&amp;quot;
    n = len(xs)
    deviations = de_mean(xs)
    return sum_of_squares(deviations) / (n - 1)
    
assert 81.54 &amp;lt; variance(num_friends) &amp;lt; 81.55
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;variance&lt;/strong&gt; is &lt;code&gt;sum_of_squares&lt;/code&gt; deviations, which can be tricky to interpret. For example, we have a &lt;code&gt;num_friends&lt;/code&gt; with values ranging from 0 to 100.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What does a variance of 81.54 mean?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A more common alternative is the &lt;strong&gt;standard deviation&lt;/strong&gt;. Here we take the square root of the variance using Python&amp;rsquo;s &lt;code&gt;math&lt;/code&gt; module.&lt;/p&gt;
&lt;p&gt;With a standard deviation of 9.03, and we know the mean of &lt;code&gt;num_friends&lt;/code&gt; is 7.3, anything below 7 + 9 = 16 or 7 - 9 (0 friends) friends is still &lt;em&gt;within a standard deviation of the mean&lt;/em&gt;. And we can check by running &lt;code&gt;friend_counts&lt;/code&gt; that most people are within a standard deviation of the mean.&lt;/p&gt;
&lt;p&gt;On the other hand, we know that someone with 20 friends is &lt;strong&gt;more than one standard deviation&lt;/strong&gt; away from the mean.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math

def standard_deviation(xs: List[float]) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;The standard deviation is the square root of the variance&amp;quot;&amp;quot;&amp;quot;
    return math.sqrt(variance(xs))
    
assert 9.02 &amp;lt; standard_deviation(num_friends) &amp;lt; 9.04
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, because the &lt;strong&gt;standard deviation&lt;/strong&gt; builds on the &lt;strong&gt;variance&lt;/strong&gt;, which is dependent on the &lt;strong&gt;mean&lt;/strong&gt;, we know that just like the mean, it can be sensitive to outliers, we can use an alternative called the &lt;strong&gt;interquartile range&lt;/strong&gt;, which is based on the &lt;strong&gt;median&lt;/strong&gt; and less sensitive to outliers.&lt;/p&gt;
&lt;p&gt;Specifically, the interquartile range can be used to examine &lt;code&gt;num_friends&lt;/code&gt; between the 25th and 75th percentile. A large chunk of people are going to have &lt;em&gt;around 6 friends&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def interquartile_range(xs: List[float]) -&amp;gt; float: 
    &amp;quot;&amp;quot;&amp;quot;Returns the difference between the 75%-ile and the 25%-ile&amp;quot;&amp;quot;&amp;quot;
    return quantile(xs, 0.75) - quantile(xs, 0.25)
    
assert interquartile_range(num_friends) == 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we describe a single list of data, we&amp;rsquo;ll also want to look at potential relationship between two data sources. For example, we may have a hypothesis that the amount of time spent on the DataScienster social network is somehow related to the number of friends someone has.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll examine covariance and correlations next.&lt;/p&gt;
&lt;h2 id=&#34;correlation&#34;&gt;Correlation&lt;/h2&gt;
&lt;p&gt;If variance is how much a &lt;em&gt;single&lt;/em&gt; set of numbers deviates from its mean (i.e., see &lt;code&gt;de_mean&lt;/code&gt; above), then &lt;strong&gt;covariance&lt;/strong&gt; measures how two sets of numbers vary from &lt;em&gt;their&lt;/em&gt; means. With the idea that if they co-vary the same amount, then they could be related.&lt;/p&gt;
&lt;p&gt;Here we&amp;rsquo;ll borrow the &lt;code&gt;dot&lt;/code&gt; production function we developed in the 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linear algebra&lt;/a&gt; post.&lt;/p&gt;
&lt;p&gt;Moreover, we&amp;rsquo;ll examine if there&amp;rsquo;s a relationship between &lt;code&gt;num_friends&lt;/code&gt; and &lt;code&gt;daily_minutes&lt;/code&gt; and &lt;code&gt;daily_hours&lt;/code&gt; (see above).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def covariance(xs: List[float], ys: List[float]) -&amp;gt; float:
    assert len(xs) == len(ys), &amp;quot;xs and ys must have same number of elements&amp;quot;
    return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1)

assert 22.42 &amp;lt; covariance(num_friends, daily_minutes) &amp;lt; 22.43
assert 22.42 / 60 &amp;lt; covariance(num_friends, daily_hours) &amp;lt; 22.43 / 60
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with variance, a similar critique can be made of &lt;strong&gt;covariance&lt;/strong&gt;, you have to do extra steps to interpret it. For example, the covariance of &lt;code&gt;num_friends&lt;/code&gt; and &lt;code&gt;daily_minutes&lt;/code&gt; is 22.43.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What does that mean? Is that considered a strong relationship?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A more intuitive measure would be a &lt;strong&gt;correlation&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def correlation(xs: List[float], ys: List[float]) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Measures how much xs and ys vary in tandem about their means&amp;quot;&amp;quot;&amp;quot;
    stdev_x = standard_deviation(xs)
    stdev_y = standard_deviation(ys)
    if stdev_x &amp;gt; 0 and stdev_y &amp;gt; 0:
        return covariance(xs,ys) / stdev_x / stdev_y
    else:
        return 0 # if no variation, correlation is zero

assert 0.24 &amp;lt; correlation(num_friends, daily_minutes) &amp;lt; 0.25
assert 0.24 &amp;lt; correlation(num_friends, daily_hours) &amp;lt; 0.25
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By dividing out the standard deviation of both input variables, correlation is always between -1 (perfect (anti) correlation) and 1 (perfect correlation). A correlation of 0.24 is relatively weak correlation (although what is considered weak, moderate, strong depends on the context of the data).&lt;/p&gt;
&lt;p&gt;One thing to keep in mind is &lt;strong&gt;simpson&amp;rsquo;s paradox&lt;/strong&gt; or when the relationship between two variables change when accounting for a third, &lt;strong&gt;confounding&lt;/strong&gt; variable. Moreover, we should keep this clichÃ© in mind (it&amp;rsquo;s a clichÃ© for a reason): &lt;strong&gt;correlation does not imply causation&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;We are just five chapters in and we can begin to see how we&amp;rsquo;re building the tools &lt;em&gt;now&lt;/em&gt;, that we&amp;rsquo;ll use later on. Here&amp;rsquo;s a visual summary of what we&amp;rsquo;ve covered in this post and how it connects to previous posts, namely 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linear algebra&lt;/a&gt; and the 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;python crash course&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./summary.png&#34; alt=&#34;summary&#34;&gt;&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science from Scratch (ch4) - Linear Algebra</title>
      <link>/post/dsfs_4/</link>
      <pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_4/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#vectors&#34;&gt;Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#matrices&#34;&gt;Matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll see the &lt;strong&gt;from scratch&lt;/strong&gt; aspect of the book play out as we implement several building block functions to help us work towards defining the &lt;strong&gt;Euclidean Distance&lt;/strong&gt; in code:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./distance_vectors.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While we don&amp;rsquo;t see its application immediately, we can expect to see the &lt;strong&gt;Euclidean Distance&lt;/strong&gt; used for K-nearest neighbors (classication) or K-means (clustering) to find the &amp;ldquo;k closest points&amp;rdquo; (
&lt;a href=&#34;https://sebastianraschka.com/faq/docs/euclidean-distance.html#:~:text=Machine%20Learning%20FAQ&amp;amp;text=For%20example%2C%20picture%20it%20as,of%20a%20particular%20sample%20point.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;). (&lt;em&gt;note&lt;/em&gt; : there are other types of distance formulas used as well.)&lt;/p&gt;
&lt;p&gt;En route towards implementing the &lt;strong&gt;Euclidean Distance&lt;/strong&gt;, we also implement the &lt;strong&gt;sum of squares&lt;/strong&gt; which is a crucial piece for how &lt;strong&gt;regression&lt;/strong&gt; works.&lt;/p&gt;
&lt;p&gt;Thus, the &lt;strong&gt;from scratch&lt;/strong&gt; aspect of this book works on two levels. &lt;em&gt;Within&lt;/em&gt; this chapter, we&amp;rsquo;re building piece by piece up to an important &lt;strong&gt;distance&lt;/strong&gt; and &lt;strong&gt;sum of squares&lt;/strong&gt; formula. But we&amp;rsquo;re also building tools we&amp;rsquo;ll use in subsequent chapters.&lt;/p&gt;
&lt;h2 id=&#34;vectors&#34;&gt;Vectors&lt;/h2&gt;
&lt;p&gt;We start off with implementing functions to &lt;strong&gt;add&lt;/strong&gt; and &lt;strong&gt;subtract&lt;/strong&gt; two vectors. We also create a function for &lt;em&gt;component wise sum&lt;/em&gt; of a list of vectors, where a new vector is created whose first element is the sum of all the first elements in the list and so on.&lt;/p&gt;
&lt;p&gt;We then create a function to &lt;strong&gt;multiply&lt;/strong&gt; a vector by  scalar, which we use to compute the &lt;em&gt;component wise mean&lt;/em&gt; of a list of vectors.&lt;/p&gt;
&lt;p&gt;We also create the &lt;strong&gt;dot product&lt;/strong&gt; of two vectors or the &lt;em&gt;sum of their component wise product&lt;/em&gt;, and this is is the generalize version of the &lt;strong&gt;sum of squares&lt;/strong&gt;. At this point, we have enough to implement the &lt;strong&gt;Euclidean distance&lt;/strong&gt;. Let&amp;rsquo;s take a look at the code:&lt;/p&gt;
&lt;h4 id=&#34;example-vectors&#34;&gt;Example Vectors&lt;/h4&gt;
&lt;p&gt;Vectors are simply a list of numbers:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;height_weight_age = [70,170,40]

grades = [95,80,75,62]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;add&#34;&gt;Add&lt;/h3&gt;
&lt;p&gt;You&amp;rsquo;ll &lt;em&gt;note&lt;/em&gt; that we do &lt;strong&gt;type annotation&lt;/strong&gt; on our code throughout. This is a convention advocated by the author (and as a newcomer to Python, I like the idea of being explicit about data type for a function&amp;rsquo;s input and output).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List

Vector = List[float]

def add(v: Vector, w: Vector) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Adds corresponding elements&amp;quot;&amp;quot;&amp;quot;
    assert len(v) == len(w), &amp;quot;vectors must be the same length&amp;quot;
    return [v_i + w_i for v_i, w_i in zip(v,w)]
    
assert add([1,2,3], [4,5,6]) == [5,7,9]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s another view of what&amp;rsquo;s going on with the &lt;code&gt;add&lt;/code&gt; function:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./add.png&#34; alt=&#34;add&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;subtract&#34;&gt;Subtract&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def subtract(v: Vector, w: Vector) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Subtracts corresponding elements&amp;quot;&amp;quot;&amp;quot;
    assert len(v) == len(w), &amp;quot;vectors must be the same length&amp;quot;
    return [v_i - w_i for v_i, w_i in zip(v,w)]
    
assert subtract([5,7,9], [4,5,6]) == [1,2,3]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is pretty much the same as the previous:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./subtract.png&#34; alt=&#34;subtract&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;componentwise-sum&#34;&gt;Componentwise Sum&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vector_sum(vectors: List[Vector]) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Sum all corresponding elements (componentwise sum)&amp;quot;&amp;quot;&amp;quot;
    # Check that vectors is not empty
    assert vectors, &amp;quot;no vectors provided!&amp;quot;
    # Check the vectorss are all the same size
    num_elements = len(vectors[0])
    assert all(len(v) == num_elements for v in vectors), &amp;quot;different sizes!&amp;quot;
    # the i-th element of the result is the sum of every vector[i]
    return [sum(vector[i] for vector in vectors)
            for i in range(num_elements)]
            
assert vector_sum([[1,2], [3,4], [5,6], [7,8]]) == [16,20]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, a &lt;code&gt;list&lt;/code&gt; of vectors becomes &lt;em&gt;one&lt;/em&gt; vector. If you go back to the &lt;code&gt;add&lt;/code&gt; function, it takes &lt;strong&gt;two&lt;/strong&gt; vectors, so if we tried to give it four vectors, we&amp;rsquo;d get a &lt;code&gt;TypeError&lt;/code&gt;. So we wrap four vectors in a &lt;code&gt;list&lt;/code&gt; and provide &lt;em&gt;that&lt;/em&gt; as the argument for &lt;code&gt;vector_sum&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./vector_sum.png&#34; alt=&#34;vector_sum&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;multiply-vector-with-a-number&#34;&gt;Multiply Vector with a Number&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def scalar_multiply(c: float, v: Vector) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Multiplies every element by c&amp;quot;&amp;quot;&amp;quot;
    return [c * v_i for v_i in v]
    
assert scalar_multiply(2, [2,4,6]) == [4,8,12]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One number is multiplied with &lt;em&gt;all&lt;/em&gt; numbers in the vector, with the vector retaining its length:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./multiply.png&#34; alt=&#34;multiply&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;componentwise-mean&#34;&gt;Componentwise Mean&lt;/h3&gt;
&lt;p&gt;This is similar to componentwise sum (see above); a list of vectors becomes one vector.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vector_mean(vectors: List[Vector]) -&amp;gt; Vector: 
    &amp;quot;&amp;quot;&amp;quot;Computes the element-wise average&amp;quot;&amp;quot;&amp;quot;
    n = len(vectors)
    return scalar_multiply(1/n, vector_sum(vectors))
    
assert vector_mean([ [1,2], [3,4], [5,6] ]) == [3,4]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dot-product&#34;&gt;Dot Product&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def dot(v: Vector, w: Vector) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Computes v_1 * w_1 + ... + v_n * w_n&amp;quot;&amp;quot;&amp;quot;
    assert len(v) == len(w), &amp;quot;vectors must be the same length&amp;quot;
    return sum(v_i * w_i for v_i, w_i in zip(v,w))
    
assert dot([1,2,3], [4,5,6]) == 32
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we multiply the elements, then sum their results. Two vectors becomes a single number (&lt;code&gt;float&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./dot_product.png&#34; alt=&#34;dot_product&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;sum-of-squares&#34;&gt;Sum of Squares&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sum_of_squares(v: Vector) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Returns v_1 * v_1 + ... + v_n * v_n&amp;quot;&amp;quot;&amp;quot;
    return dot(v,v)
    
assert sum_of_squares([1,2,3]) == 14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In fact, &lt;code&gt;sum_of_squares&lt;/code&gt; is a special case of &lt;strong&gt;dot product&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./sum_of_squares.png&#34; alt=&#34;sum_of_squares&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;magnitude&#34;&gt;Magnitude&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def magnitude(v: Vector) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Returns  the magnitude (or length) of v&amp;quot;&amp;quot;&amp;quot;
    return math.sqrt(sum_of_squares(v)) # math.sqrt is the square root function
    
assert magnitude([3,4]) == 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;magnitude&lt;/code&gt; we square root the &lt;code&gt;sum_of_squares&lt;/code&gt;. This is none other than the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pythagorean_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pythagorean theorem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./magnitude.png&#34; alt=&#34;magnitude&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;squared-distance&#34;&gt;Squared Distance&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def squared_distance(v: Vector, w: Vector) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2&amp;quot;&amp;quot;&amp;quot;
    return sum_of_squares(subtract(v,w))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the distance &lt;em&gt;between&lt;/em&gt; two vectors, &lt;em&gt;squared&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./squared_distance.png&#34; alt=&#34;squared_distance&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;euclidean-distance&#34;&gt;(Euclidean) Distance&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math

def distance(v: Vector, w: Vector) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Also computes the distance between v and w&amp;quot;&amp;quot;&amp;quot;
    return math.sqrt(squared_distance(v,w))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we square root the &lt;code&gt;squared_distance&lt;/code&gt; to get the (euclidean) distance:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./distance_vectors.png&#34; alt=&#34;distance_vectors&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;We literally built from scratch, albeit with some help from Python&amp;rsquo;s &lt;code&gt;math&lt;/code&gt; module, the blocks needed for essential functions that we&amp;rsquo;ll expect to use later, namely: the &lt;code&gt;sum_of_squares&lt;/code&gt; and &lt;code&gt;distance&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s pretty cool to see these foundational concepts set us up to understand more complex machine learning algorithms like &lt;strong&gt;regression&lt;/strong&gt;, &lt;strong&gt;k-nearest neighbors (classification)&lt;/strong&gt;, &lt;strong&gt;k-means (clustering)&lt;/strong&gt; and even touch on the &lt;strong&gt;pythagorean theorem&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll examine matrices next.&lt;/p&gt;
&lt;h2 id=&#34;matrices&#34;&gt;Matrices&lt;/h2&gt;
&lt;p&gt;The first thing to note is that &lt;code&gt;matrices&lt;/code&gt; are represented as &lt;code&gt;lists&lt;/code&gt; of &lt;code&gt;lists&lt;/code&gt; which is explicit with type annotation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List

Matrix = List[List[float]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might bet wondering if a &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;lists&lt;/code&gt; is somehow different from a &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;vectors&lt;/code&gt; we saw previously with the &lt;code&gt;vector_sum&lt;/code&gt; function. To see, I used &lt;strong&gt;type annotation&lt;/strong&gt; to try to define the arguments &lt;em&gt;differently&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the &lt;code&gt;vector_sum&lt;/code&gt; function we defined previously:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vector_sum(vectors: List[Vector]) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Sum all corresponding elements (componentwise sum)&amp;quot;&amp;quot;&amp;quot;
    # Check that vectors is not empty
    assert vectors, &amp;quot;no vectors provided!&amp;quot;
    # Check the vectorss are all the same size
    num_elements = len(vectors[0])
    assert all(len(v) == num_elements for v in vectors), &amp;quot;different sizes!&amp;quot;
    # the i-th element of the result is the sum of every vector[i]
    return [sum(vector[i] for vector in vectors)
            for i in range(num_elements)]
            
assert vector_sum([[1,2], [3,4], [5,6], [7,8]]) == [16,20]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s a &lt;strong&gt;new&lt;/strong&gt; function, &lt;code&gt;vector_sum2&lt;/code&gt; defined differently with type annotation:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vector_sum2(lists: List[List[float]]) -&amp;gt; List:
   &amp;quot;&amp;quot;&amp;quot;Sum all corresponding list (componentwise sum?)&amp;quot;&amp;quot;&amp;quot;
   assert lists, &amp;quot;this list is empty!&amp;quot;
   # check that lists are the same size
   num_lists = len(lists[0])
   assert all(len(l) == num_lists for l in lists), &amp;quot;different sizes!&amp;quot;
   # the i-th list is the sum of every list[i]
   return [sum(l[i] for l in lists)
           for i in range(num_lists)]

assert vector_sum2([[1,2], [3,4], [5,6], [7,8]]) == [16,20]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I did a variety of things to see if &lt;code&gt;vector_sum&lt;/code&gt; and &lt;code&gt;vector_sum2&lt;/code&gt; behaved differently, but they appear to be identical:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# both are functions
assert callable(vector_sum) == True
assert callable(vector_sum2) == True

# when taking the same argument, they both return a list
type(vector_sum([[1,2], [3,4], [5,6], [7,8]])) #list
type(vector_sum2([[1,2], [3,4], [5,6], [7,8]])) #list

# the same input yields the same output
vector_sum([[1,2],[3,4]])    # [4,6]
vector_sum2([[1,2],[3,4]])   # [4,6]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To keep it simple, in the context of &lt;strong&gt;matrices&lt;/strong&gt;, you can think of &lt;strong&gt;vectors&lt;/strong&gt; as the &lt;em&gt;rows of the matrix&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For example, if we represent the small dataset below as a &lt;strong&gt;matrix&lt;/strong&gt;, we can think of &lt;em&gt;columns&lt;/em&gt; as variables like: height, weight, age; and &lt;em&gt;each row&lt;/em&gt; as a person:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample_data = [[70, 170, 40],
               [65, 120, 26],
               [77, 250, 19]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By extension of &lt;strong&gt;rows&lt;/strong&gt; and &lt;strong&gt;columns&lt;/strong&gt;, we can write a function for the shape of a matrix. This below &lt;code&gt;shape&lt;/code&gt; function takes in a matrix and returns a &lt;code&gt;tuple&lt;/code&gt; with two integers, number of rows and number of columns:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import Tuple
  
def shape(A: Matrix) -&amp;gt; Tuple[int, int]:
    &amp;quot;&amp;quot;&amp;quot;Returns (# of rows of A, # of columns of A)&amp;quot;&amp;quot;&amp;quot;
    num_rows = len(A)
    num_cols = len(A[0]) if A else 0  # number of elements in first row
    return num_rows, num_cols
    
assert shape([[1,2,3], [4,5,6]]) == (2,3) # 2 rows, 3 columns
assert shape(sample_data) == (3,3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can actually write functions to grab either a specific &lt;em&gt;row&lt;/em&gt; or a specific &lt;em&gt;columns&lt;/em&gt; :&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Vector = List[float]

# rows
def get_row(A: Matrix, i: int) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Returns the i-th row of A (as a Vector)&amp;quot;&amp;quot;&amp;quot;
    return A[i]  # A[i] is already the ith row

# column
def get_column(A: Matrix, i: int) -&amp;gt; Vector:
    &amp;quot;&amp;quot;&amp;quot;Returns the j-th column of A (as a Vector)&amp;quot;&amp;quot;&amp;quot;
    return [A_i[j]
            for A_i in A]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, going beyond finding the shape, rows and columns of an existing matrix, we&amp;rsquo;ll also want to &lt;strong&gt;create&lt;/strong&gt; matrices and we&amp;rsquo;ll do that using &lt;strong&gt;nested list comprehensions&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import Callable

def make_matrix(num_rows: int,
                num_cols: int,
                entry_fn: Callable[[int, int], float]) -&amp;gt; Matrix:
    &amp;quot;&amp;quot;&amp;quot;
    Returns a num_rows x num_cols matrix
    whose (i,j)-th entry is entry_fn(i, j)
    &amp;quot;&amp;quot;&amp;quot;
    return [[entry_fn(i,j)            # given i, create a list
            for j in range(num_cols)] # [entry_fn(i, 0), ...]
            for i in range(num_rows)] # create one list for each i
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we&amp;rsquo;ll actually &lt;em&gt;use&lt;/em&gt; the &lt;code&gt;make_matrix&lt;/code&gt; function to create a special type of matrix called the &lt;code&gt;identity matrix&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def identity_matrix(n: int) -&amp;gt; Matrix:
    &amp;quot;&amp;quot;&amp;quot;Returns the n x n identity matrix&amp;quot;&amp;quot;&amp;quot;
    return make_matrix(n, n, lambda i, j: 1 if i == j else 0)

assert identity_matrix(5) == [[1, 0, 0, 0, 0],
                              [0, 1, 0, 0, 0],
                              [0, 0, 1, 0, 0],
                              [0, 0, 0, 1, 0],
                              [0, 0, 0, 0, 1]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;summary-1&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;To be sure there are 
&lt;a href=&#34;https://machinelearningmastery.com/introduction-to-types-of-matrices-in-linear-algebra/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other types of matrices&lt;/a&gt;, but in this chapter we&amp;rsquo;re only briefly exploring its construction to prime us.&lt;/p&gt;
&lt;p&gt;We know matrices can be used to represent data, each &lt;em&gt;row&lt;/em&gt; in the dataset being a &lt;strong&gt;vector&lt;/strong&gt;. Because we can also know a matrices&#39; &lt;em&gt;column&lt;/em&gt;, we&amp;rsquo;ll use it to represent linear functions that &lt;strong&gt;map k-dimensional vectors to n-dimensional vectors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, matrices can also be used to map &lt;em&gt;binary relationships&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;flashback-to-ch1&#34;&gt;Flashback to Ch.1&lt;/h3&gt;
&lt;p&gt;On our first day at DataSciensterâ¢ we were given &lt;code&gt;friendship_pairs&lt;/code&gt; data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;friendship_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (3,4),
                    (4,5), (5,6), (5,7), (6,8), (7,8), (8,9)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These &lt;code&gt;friendship_pairs&lt;/code&gt; can also be represented in matrix form:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#            user 0  1  2  3  4  5  6  7  8  9
friend_matrix = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0], # user 0
                 [1, 0, 1, 1, 0, 0, 0, 0, 0, 0], # user 1
                 [1, 1, 0, 1, 0, 0, 0, 0, 0, 0], # user 2
                 [0, 1, 1, 0, 1, 0, 0, 0, 0, 0], # user 3
                 [0, 0, 0, 1, 0, 1, 0, 0, 0, 0], # user 4
                 [0, 0, 0, 0, 1, 0, 1, 1, 0, 0], # user 5
                 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # user 6
                 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # user 7
                 [0, 0, 0, 0, 0, 0, 1, 1, 0, 1], # user 8
                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]] # user 9
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This allows us to check very quickly whether two users are friends or not:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;assert friend_matrix[0][2] == 1, &amp;quot;0 and 2 are friends&amp;quot;
assert friend_matrix[0][8] == 0, &amp;quot;0 and 8 are not friends&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if we wanted to check for each user&amp;rsquo;s friend, we could:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;friends_of_five = [i
                  for i, is_friend in enumerate(friend_matrix[5])
                  if is_friend]
                  
friends_of_zero = [i
                   for i, is_friend in enumerate(friend_matrix[0])
                   if is_friend]
                   
assert friends_of_five == [4,6,7]
assert friends_of_zero == [1,2]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Supplementing lists with data frames</title>
      <link>/post/list-to-df/</link>
      <pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/list-to-df/</guid>
      <description>&lt;h2 id=&#34;beyond-collections-and-comprehensions&#34;&gt;Beyond Collections and Comprehensions&lt;/h2&gt;
&lt;p&gt;A couple days back I wrote a post summarizing how much Collections and Comprehension were used. Data was provided in the form of &lt;code&gt;lists&lt;/code&gt;, either lists of &lt;code&gt;dictionaries&lt;/code&gt; or &lt;code&gt;tuples&lt;/code&gt;. And to answer questions &lt;em&gt;about&lt;/em&gt; the data, the author often used &lt;code&gt;list comprehensions&lt;/code&gt; - iterating through lists with a for-loop. I am beginning to see this as a very Python-centric way of approaching problems.&lt;/p&gt;
&lt;p&gt;While &lt;strong&gt;not&lt;/strong&gt; all data is tabular, so much of it &lt;em&gt;is&lt;/em&gt; so its reasonable to assume that, more often that not, you&amp;rsquo;ll be dealing with spreadsheet-like tabular data (&lt;strong&gt;note&lt;/strong&gt;: I&amp;rsquo;m open to other perspectives here, feel free to leave a comment below!).&lt;/p&gt;
&lt;p&gt;In any case, I had this &lt;strong&gt;itch&lt;/strong&gt; to go back to that chapter and ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How would I approach the same problem using data frames?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So that&amp;rsquo;s what this post is about. You can reference these 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; for context; also keep in mind, this is a brief detour and deviation from Joel Grus&#39; book (for example, I&amp;rsquo;ll be using 
&lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pandas&lt;/a&gt; here and a 
&lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jupyter notebook&lt;/a&gt; here, both of which are not covered in the book).&lt;/p&gt;
&lt;p&gt;For review, here&amp;rsquo;s the data you are given as a newly hired data scientist at Data Sciensterâ¢&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# users in the network
# stored as a list of dictionaries
users = [
    {&amp;quot;id&amp;quot;: 0, &amp;quot;name&amp;quot;: &amp;quot;Hero&amp;quot;},
    {&amp;quot;id&amp;quot;: 1, &amp;quot;name&amp;quot;: &amp;quot;Dunn&amp;quot;},
    {&amp;quot;id&amp;quot;: 2, &amp;quot;name&amp;quot;: &amp;quot;Sue&amp;quot;},
    {&amp;quot;id&amp;quot;: 3, &amp;quot;name&amp;quot;: &amp;quot;Chi&amp;quot;},
    {&amp;quot;id&amp;quot;: 4, &amp;quot;name&amp;quot;: &amp;quot;Thor&amp;quot;},
    {&amp;quot;id&amp;quot;: 5, &amp;quot;name&amp;quot;: &amp;quot;Clive&amp;quot;},
    {&amp;quot;id&amp;quot;: 6, &amp;quot;name&amp;quot;: &amp;quot;Hicks&amp;quot;},
    {&amp;quot;id&amp;quot;: 7, &amp;quot;name&amp;quot;: &amp;quot;Devin&amp;quot;},
    {&amp;quot;id&amp;quot;: 8, &amp;quot;name&amp;quot;: &amp;quot;Kate&amp;quot;},
    {&amp;quot;id&amp;quot;: 9, &amp;quot;name&amp;quot;: &amp;quot;Klein&amp;quot;}
]

# friendship pairings in the network
# stored as a list of tuples
friendship_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (3,4),
                    (4,5), (5,6), (5,7), (6,8), (7,8), (8,9)]
                    
# interests data
# stored as another list of tuples
interests = [
    (0, &amp;quot;Hadoop&amp;quot;), (0, &amp;quot;Big Data&amp;quot;), (0, &amp;quot;HBase&amp;quot;), (0, &amp;quot;Java&amp;quot;),
    (0, &amp;quot;Spark&amp;quot;), (0, &amp;quot;Storm&amp;quot;), (0, &amp;quot;Cassandra&amp;quot;),
    (1, &amp;quot;NoSQL&amp;quot;), (1, &amp;quot;MongoDB&amp;quot;), (1, &amp;quot;Cassandra&amp;quot;), (1, &amp;quot;HBase&amp;quot;),
    (1, &amp;quot;Postgres&amp;quot;), (2, &amp;quot;Python&amp;quot;), (2, &amp;quot;scikit-learn&amp;quot;), (2, &amp;quot;scipy&amp;quot;),
    (2, &amp;quot;numpy&amp;quot;), (2, &amp;quot;statsmodels&amp;quot;), (2, &amp;quot;pandas&amp;quot;), (3, &amp;quot;R&amp;quot;), (3, &amp;quot;Python&amp;quot;),
    (3, &amp;quot;statistics&amp;quot;), (3, &amp;quot;regression&amp;quot;), (3, &amp;quot;probability&amp;quot;),
    (4, &amp;quot;machine learning&amp;quot;), (4, &amp;quot;regression&amp;quot;), (4, &amp;quot;decision trees&amp;quot;),
    (4, &amp;quot;libsvm&amp;quot;), (5, &amp;quot;Python&amp;quot;), (5, &amp;quot;R&amp;quot;), (5, &amp;quot;Java&amp;quot;), (5, &amp;quot;C++&amp;quot;),
    (5, &amp;quot;Haskell&amp;quot;), (5, &amp;quot;programming langauges&amp;quot;), (6, &amp;quot;statistics&amp;quot;),
    (6, &amp;quot;probability&amp;quot;), (6, &amp;quot;mathematics&amp;quot;), (6, &amp;quot;theory&amp;quot;),
    (7, &amp;quot;machine learning&amp;quot;), (7, &amp;quot;scikit-learn&amp;quot;), (7, &amp;quot;Mahout&amp;quot;),
    (7, &amp;quot;neural networks&amp;quot;), (8, &amp;quot;neural networks&amp;quot;), (8, &amp;quot;deep learning&amp;quot;),
    (8, &amp;quot;Big Data&amp;quot;), (8, &amp;quot;artificial intelligence&amp;quot;), (9, &amp;quot;Hadoop&amp;quot;),
    (9, &amp;quot;Java&amp;quot;), (9, &amp;quot;MapReduce&amp;quot;), (9, &amp;quot;Big Data&amp;quot;)
    ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given just these pieces of data, we can create &lt;strong&gt;functions&lt;/strong&gt;, use &lt;strong&gt;for-loops&lt;/strong&gt; and &lt;strong&gt;list comprehensions&lt;/strong&gt; to answer some questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who are each user friends with?&lt;/li&gt;
&lt;li&gt;What are the total and average number of connections?&lt;/li&gt;
&lt;li&gt;Which users share the same interest?&lt;/li&gt;
&lt;li&gt;What are the most popular topics in this network?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, the chapter ends with lists, functions and comprehension. What about &lt;strong&gt;storing data in data frames?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First we&amp;rsquo;ll store &lt;code&gt;users&lt;/code&gt; as a data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# convert list of dict into dataframe
users_df = pd.DataFrame(users)
users_df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just visually, a &lt;code&gt;data frame&lt;/code&gt; looks different from a &lt;code&gt;list of dictionaries&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./list_to_df.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Your mileage may vary, but &lt;em&gt;I make sense of the data&lt;/em&gt; very differently when I&amp;rsquo;m looking at a list vs a data frame. &lt;strong&gt;Rows and columns&lt;/strong&gt; are ingrained in how I think about data.&lt;/p&gt;
&lt;p&gt;Next, we&amp;rsquo;re given a &lt;code&gt;list of tuples&lt;/code&gt; representing friendship pairs and we proceed to turn that into a &lt;code&gt;dictionary&lt;/code&gt; by using a &lt;code&gt;dictionary comprehension&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# list of tuples
friendship_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (3,4),
                    (4,5), (5,6), (5,7), (6,8), (7,8), (8,9)]
                    
# create a dict, where keys are users id, 
# dictionary comprehension
friendships = {user[&amp;quot;id&amp;quot;]: [] for user in users}

for i, j in friendship_pairs:
    friendships[i].append(j)
    friendships[j].append(i)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similar to the previous example, I find that viewing the data as a &lt;code&gt;data frame&lt;/code&gt; is &lt;em&gt;different&lt;/em&gt; from viewing it as a &lt;code&gt;dictionary&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./dict_to_df.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;From this point, I&amp;rsquo;m doing several operations in 
&lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pandas&lt;/a&gt; to &lt;strong&gt;join&lt;/strong&gt; the first two tables, such that I have a column with the user&amp;rsquo;s id, user&amp;rsquo;s name and the id of their first, second or, in some cases, third friends (at most people in this network have 3 direct connections).&lt;/p&gt;
&lt;p&gt;If you want to know the specific 
&lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pandas&lt;/a&gt; operation, here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The users_df is fine as is with two columns: id and name (see above)

# We&#39;ll transform the friendships_df

# reset_index allows us to add an index column
friendships_df.reset_index(inplace=True)
# add index column
friendships_df = friendships_df.rename(columns = {&amp;quot;id&amp;quot;:&amp;quot;new column name&amp;quot;})
# change index column to &#39;id&#39;
friendships_df = friendships_df.rename(columns = {&#39;index&#39;:&#39;id&#39;})
# join with users_df so we get each person&#39;s name
users_friendships = pd.merge(users_df, friendships_df, on=&#39;id&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we&amp;rsquo;ve joined &lt;code&gt;users_df&lt;/code&gt; and &lt;code&gt;friendships_df&lt;/code&gt;, we have:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./users_friendships.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since we have &lt;code&gt;users&lt;/code&gt; and &lt;code&gt;friendships&lt;/code&gt; data, we could write a function to help us answer &amp;ldquo;how many friends does each user have?&amp;rdquo;. In addition, we&amp;rsquo;ll have to create a &lt;code&gt;list comprehension&lt;/code&gt; so we loop through each &lt;code&gt;user&lt;/code&gt; within &lt;code&gt;users&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# function to count how many friend each user has
def number_of_friends(user):
    &amp;quot;&amp;quot;&amp;quot;How many friends does _user_ have?&amp;quot;&amp;quot;&amp;quot;
    user_id = user[&amp;quot;id&amp;quot;]
    friend_ids = friendships[user_id]
    return len(friend_ids)

# list comprehension to apply the function for each user
num_friends_by_id = [(user[&amp;quot;id&amp;quot;], number_of_friends(user)) for user in users]

# this gives us a list of tuples
num_friends_by_id

[(0, 2),
 (1, 3),
 (2, 3),
 (3, 3),
 (4, 2),
 (5, 3),
 (6, 2),
 (7, 2),
 (8, 3),
 (9, 1)]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, viewing the data as a  &lt;code&gt;list of tuples&lt;/code&gt; is different from a &lt;code&gt;data frame&lt;/code&gt;, so let&amp;rsquo;s go ahead and turn that into a pandas data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# when converting to data frame, we can set the name of the columns to id and num_friends; this sets us up for another join
num_friends_by_id = pd.DataFrame(num_friends_by_id, columns = [&#39;id&#39;, &#39;num_friends&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we have an &amp;lsquo;id&amp;rsquo; column, we can join this with our previously created &lt;code&gt;users_friendships&lt;/code&gt; data frame:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./num_friends_by_id.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once joined with &lt;code&gt;users_friendships&lt;/code&gt; using the &lt;code&gt;merge&lt;/code&gt; function, we get (&lt;code&gt;users_friendships2&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./users_friendships2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;By now you&amp;rsquo;re familiar with the process. We have a Python &lt;strong&gt;collection&lt;/strong&gt;, generally a &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;dictionaries&lt;/code&gt; or &lt;code&gt;tuples&lt;/code&gt; and we want to convert them to a &lt;code&gt;data frame&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll repeat this process for the &lt;code&gt;interests&lt;/code&gt; variable which is a long &lt;code&gt;list of tuples&lt;/code&gt; (see above). We&amp;rsquo;ll convert to data frame, then join with &lt;code&gt;users_friendships_2&lt;/code&gt; to get a longer data frame with &lt;code&gt;interests&lt;/code&gt; as one of the columns (&lt;em&gt;note&lt;/em&gt; : picture is cut for space):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./interests.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The nice thing about &lt;strong&gt;pandas&lt;/strong&gt; is that once you have all your data &lt;strong&gt;joined&lt;/strong&gt; together in a data frame, you can &lt;strong&gt;query&lt;/strong&gt; the data.&lt;/p&gt;
&lt;p&gt;For example, I may want to see all users have an interest in &amp;ldquo;Big Data&amp;rdquo;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./big_data.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Previously, we would have had to create a function that returns a &lt;code&gt;list comprehension&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def data_scientists_who_like(target_interest):
    &amp;quot;&amp;quot;&amp;quot;Find the ids of all users who like the target interests.&amp;quot;&amp;quot;&amp;quot;
    return [user_id
            for user_id, user_interest in interests
            if user_interest == target_interest]
            
data_scientists_who_like(&amp;quot;Big Data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data frame has other advantages, you could also query columns on multiple conditions, here are two ways to query multiple topics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Option One: Use .query()
user_friendship_topics.query(&#39;topic == &amp;quot;machine learning&amp;quot; | topic == &amp;quot;regression&amp;quot; | topic == &amp;quot;decision trees&amp;quot; | topic == &amp;quot;libsvm&amp;quot;&#39;)

# Option Two: Use .isin()
user_friendship_topics[user_friendship_topics[&#39;topic&#39;].isin([&amp;quot;machine learning&amp;quot;, &amp;quot;regression&amp;quot;, &amp;quot;decision trees&amp;quot;, &amp;quot;libsvm&amp;quot;])]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both options return this data frame:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./multi_condition.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;By querying the data frame, we learned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all users interested in these four topics&lt;/li&gt;
&lt;li&gt;users that have interests in common with Thor&lt;/li&gt;
&lt;li&gt;(if needed) the &lt;code&gt;num_friends&lt;/code&gt; that each user has&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also find out the most popular topics within this network:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# groupby topic, tally(count), then reset_index(), then sort
user_friendship_topics.groupby([&#39;topic&#39;]).count().reset_index().sort_values(&#39;id&#39;, ascending=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can even &lt;code&gt;groupby&lt;/code&gt; two columns (name &amp;amp; topic) to see topic of interests listed by each user:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_friendship_topics.groupby([&#39;name&#39;, &#39;topic&#39;]).count()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./user_by_topic.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Hopefully you&amp;rsquo;re convinced that &lt;strong&gt;data frames&lt;/strong&gt; are a powerful supplement to the more familiar operations in Python like &lt;strong&gt;for-loops&lt;/strong&gt; and/or &lt;strong&gt;list comprehensions&lt;/strong&gt;; that both are worth knowing well to manipulate data in a variety of formats. (e.g., to access JSON data, Python dictionaries are 
&lt;a href=&#34;https://www.freecodecamp.org/news/python-read-json-file-how-to-load-json-from-a-file-and-parse-dumps/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ideal&lt;/a&gt;).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making sense of matplotlib</title>
      <link>/post/dsfs_3/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_3/</guid>
      <description>&lt;h2 id=&#34;data-visualization&#34;&gt;Data Visualization&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;./viz_cover.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Chapter 3 of Data Science from Scratch introduces us to visualizing data using 
&lt;a href=&#34;https://matplotlib.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matplotlib&lt;/a&gt;. This is widely used in the Python ecosystem, although my sense is that people are &lt;em&gt;just as happy, if not more&lt;/em&gt;, to use other libraries like 
&lt;a href=&#34;https://seaborn.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;seaborn&lt;/a&gt;, 
&lt;a href=&#34;https://altair-viz.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Altair&lt;/a&gt; and 
&lt;a href=&#34;https://bokeh.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bokeh&lt;/a&gt;. (&lt;strong&gt;note&lt;/strong&gt;: seaborn is built on top of matplotlib).&lt;/p&gt;
&lt;p&gt;This chapter is fairly brief and is meant as a quick introduction to matplotlib - to get readers familiar with basic charts. Whole books can be written on &lt;strong&gt;data visualization&lt;/strong&gt; alone, so this is meant more as an appetizer, rather than a full-course.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a fair amount of detail involved in using matplotlib, so we&amp;rsquo;ll break it down to demystify it.&lt;/p&gt;
&lt;h4 id=&#34;basic-plotting&#34;&gt;Basic Plotting&lt;/h4&gt;
&lt;p&gt;This chapter goes through the main basic charts including Line, Bar, Histograms, and Scatter Plots. &lt;strong&gt;At first glance&lt;/strong&gt;, they follow a similar pattern. Data is provided as a &lt;code&gt;list&lt;/code&gt; of numbers (usually more than one list). &lt;code&gt;pyplot&lt;/code&gt; is imported from &lt;code&gt;matplotlib&lt;/code&gt; as &lt;code&gt;plt&lt;/code&gt;. The &lt;code&gt;plt&lt;/code&gt; module has several &lt;strong&gt;functions&lt;/strong&gt; which are accessed to create the plot.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example line chart visualizing growth in GDP over time:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./line_gdp.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from matplotlib import pyplot as plt

# the data
years = [1950, 1960, 1970, 1980, 1990, 2000, 2010]
gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3]

# the plot
plt.plot(years, gdp, color=&amp;quot;green&amp;quot;, marker=&#39;o&#39;, linestyle=&#39;solid&#39;)
plt.title(&amp;quot;Nominal GDP&amp;quot;)
plt.ylabel(&amp;quot;Billions of $&amp;quot;)
plt.xlabel(&amp;quot;Years&amp;quot;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can somewhat get by with just knowing this. Briefly consulting the documentation will let you see some other &lt;em&gt;chart types&lt;/em&gt; like so:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./plot_doc.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we wanted to convert our &lt;strong&gt;line chart&lt;/strong&gt; into a &lt;strong&gt;stacked area chart&lt;/strong&gt;, we can just change one line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib import pyplot as plt

years = [1950, 1960, 1970, 1980, 1990, 2000, 2010]
gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3]

plt.stackplot(years, gdp, color=&amp;quot;green&amp;quot;) # this is the only line we changed

plt.title(&amp;quot;Nominal GDP&amp;quot;)
plt.ylabel(&amp;quot;Billions of $&amp;quot;)
plt.xlabel(&amp;quot;Years&amp;quot;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s what the &lt;strong&gt;stacked area chart&lt;/strong&gt; version of the previous graph looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./stack_gdp.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To keep things simple, we can change the chart type with just one line and we just need to remember that when converting from chart to chart, we have to be mindful of the parameters that each chart type takes. For example, a &lt;strong&gt;stacked area chart&lt;/strong&gt; takes in different parameters than &lt;strong&gt;line charts&lt;/strong&gt; (for example, you&amp;rsquo;ll get an &lt;code&gt;AttributionError&lt;/code&gt; if you try to use &lt;code&gt;marker&lt;/code&gt; in a stacked area chart.)&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example bar chart comparing movies by the number of Academy awards they&amp;rsquo;ve won:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bar_movies.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a &lt;strong&gt;stem plot&lt;/strong&gt; version:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./stem_movies.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As with the previous example, changing just one &lt;strong&gt;function&lt;/strong&gt; from &lt;code&gt;plt.bar&lt;/code&gt; to &lt;code&gt;plt.stem&lt;/code&gt; gave us a different plot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#---- Original Bar Chart ----#

movies = [&amp;quot;Annie Hall&amp;quot;, &amp;quot;Ben-Hur&amp;quot;, &amp;quot;Casablanca&amp;quot;, &amp;quot;Gandhi&amp;quot;, &amp;quot;West Side Story&amp;quot;]
num_oscars = [5,11,3,8,10]

plt.bar(range(len(movies)), num_oscars)
plt.title(&amp;quot;My Favorite Movies&amp;quot;)
plt.ylabel(&amp;quot;# of Academy Awards&amp;quot;)
plt.xticks(range(len(movies)), movies)
plt.show()

# ---- Stem Chart ---- #

movies = [&amp;quot;Annie Hall&amp;quot;, &amp;quot;Ben-Hur&amp;quot;, &amp;quot;Casablanca&amp;quot;, &amp;quot;Gandhi&amp;quot;, &amp;quot;West Side Story&amp;quot;]
num_oscars = [5,11,3,8,10]

plt.stem(range(len(movies)), num_oscars) # the only change
plt.title(&amp;quot;My Favorite Movies&amp;quot;)
plt.ylabel(&amp;quot;# of Academy Awards&amp;quot;)
plt.xticks(range(len(movies)), movies)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;there-are-levels-to-this-hierarchy&#34;&gt;There are levels to this: Hierarchy&lt;/h4&gt;
&lt;p&gt;I&amp;rsquo;m all for keeping &lt;strong&gt;matplotlib&lt;/strong&gt; as simple as possible but one thing the above examples gloss over is the &lt;strong&gt;matplotlib object hierarchy&lt;/strong&gt;, which is something worth understanding to get a feel for how the various functions operate.&lt;/p&gt;
&lt;p&gt;This next figure is borrowed from 
&lt;a href=&#34;https://realpython.com/python-matplotlib-guide/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Real Python&lt;/a&gt; and it nicely highlights the hierarchy inherent in &lt;em&gt;every&lt;/em&gt; plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./hierarchy.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll note the levels: Figure, Axes and Axis. When digging into 
&lt;a href=&#34;https://matplotlib.org/api/axes_api.html?&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matplotlib documentation on axes&lt;/a&gt;, these levels are brought to the foreground.&lt;/p&gt;
&lt;p&gt;To really see this in action, we&amp;rsquo;ll need to code our plot &lt;em&gt;slightly&lt;/em&gt; differently. For the last chart this chapter examines the &lt;strong&gt;bias-variance tradeoff&lt;/strong&gt; which is something we&amp;rsquo;ll learn more about in future chapters, but it highlights the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;trade-off&lt;/a&gt; in trying to simultanenously minimize two sources of error so our algorithm generalizes to new situations.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# BOOK version
variance = [1,2,4,8,16,32,64,128,256]
bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1]
total_error = [x + y for x,y in zip(variance, bias_squared)]
xs = [i for i, _ in enumerate(variance)]

plt.plot(xs, variance, &#39;g-&#39;, label=&#39;variance&#39;)
plt.plot(xs, bias_squared, &#39;r-&#39;, label=&#39;bias^2&#39;)
plt.plot(xs, total_error, &#39;b:&#39;, label=&#39;total error&#39;)
plt.legend(loc=9)
plt.xlabel(&amp;quot;model complexity&amp;quot;)
plt.xticks([])
plt.title(&amp;quot;The Bias-Variance Tradeoff&amp;quot;)
plt.show()

# ALTERNATE version
variance = [1,2,4,8,16,32,64,128,256]
bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1]
total_error = [x + y for x,y in zip(variance, bias_squared)]
xs = [i for i, _ in enumerate(variance)]

fig, ax = plt.subplots(figsize=(8,5))
ax.plot(xs, variance, &#39;g-&#39;, label=&#39;variance&#39;)
ax.plot(xs, bias_squared, &#39;r-&#39;, label=&#39;bias^2&#39;)
ax.plot(xs, total_error, &#39;b:&#39;, label=&#39;total error&#39;)
ax.legend(loc=&#39;upper center&#39;)
ax.set_xlabel(&amp;quot;model complexity&amp;quot;)
ax.set_title(&amp;quot;The Bias-Variance Tradeoff: Alt Version&amp;quot;)
fig.tight_layout()
fig.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead using the &lt;code&gt;plt&lt;/code&gt; module, we use &lt;code&gt;fig&lt;/code&gt; and &lt;code&gt;ax&lt;/code&gt;, here are their data types:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;type(fig) # matplotlib.figure.Figure
type(ax)  # matplotlib.axes._subplots.AxesSubplot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This makes &lt;em&gt;explicit&lt;/em&gt; the &lt;strong&gt;matplotlib object hierarchy&lt;/strong&gt;, particularly as we see how we access function at the &lt;code&gt;axes._subplits.AxesSubplot&lt;/code&gt; level (the 
&lt;a href=&#34;https://matplotlib.org/api/axes_api.html?&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation has much more detail&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bias_var_alt.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In summary, we learned that matplotlib &lt;em&gt;can&lt;/em&gt; be fairly simple to use for static, simple plots, but we&amp;rsquo;re better served having &lt;em&gt;some&lt;/em&gt; understanding of &lt;strong&gt;matplotlib&amp;rsquo;s object hierarchy&lt;/strong&gt;. We&amp;rsquo;ll examine more chart types as we proceed with the rest of the chapters.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science from Scratch (ch1)</title>
      <link>/post/dsfs_1/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_1/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Table of Content:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#datascienster_pt1&#34;&gt;Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#datascienster_pt2&#34;&gt;Part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;datascienster_pt1&#34;&gt;DataScienster_pt1&lt;/h2&gt;
&lt;h4 id=&#34;collections-and-comprehensions&#34;&gt;Collections and Comprehensions&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science from Scratch&lt;/a&gt; opens with a narrative motivating example where you, dear reader, are newly hired to lead data science at &lt;em&gt;DataSciencester&lt;/em&gt;, a social network exclusively for data scientists.&lt;/p&gt;
&lt;p&gt;Joel Grus, the author, explains:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Throughout the book, we&amp;rsquo;ll be learning about data science concepts by solving problems that you encounter at work. Sometimes we&amp;rsquo;ll look at data explicitly supplied by users, sometimes we&amp;rsquo;ll look at data generated through their interactions with the site, and sometimes we&amp;rsquo;ll even look at data from experiments that we&amp;rsquo;ll design&amp;hellip;we&amp;rsquo;ll be building our tools from scratch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This chapter is meant as a teaser for the rest of the book, but I wanted to revisit this chapter with our 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;python crash course&lt;/a&gt; fresh on our minds to highlight some &lt;em&gt;frequently&lt;/em&gt; used concepts we can expect to see for the rest of the book.&lt;/p&gt;
&lt;p&gt;You are just hired as &amp;ldquo;VP of Networking&amp;rdquo; and are tasked with finding out which data scientist is the most well connected in the DataSciencster network, you&amp;rsquo;re giving a data dump ð. It&amp;rsquo;s a list of users, each with a unique id.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;users = [
    {&amp;quot;id&amp;quot;: 0, &amp;quot;name&amp;quot;: &amp;quot;Hero&amp;quot;},
    {&amp;quot;id&amp;quot;: 1, &amp;quot;name&amp;quot;: &amp;quot;Dunn&amp;quot;},
    {&amp;quot;id&amp;quot;: 2, &amp;quot;name&amp;quot;: &amp;quot;Sue&amp;quot;},
    {&amp;quot;id&amp;quot;: 3, &amp;quot;name&amp;quot;: &amp;quot;Chi&amp;quot;},
    {&amp;quot;id&amp;quot;: 4, &amp;quot;name&amp;quot;: &amp;quot;Thor&amp;quot;},
    {&amp;quot;id&amp;quot;: 5, &amp;quot;name&amp;quot;: &amp;quot;Clive&amp;quot;},
    {&amp;quot;id&amp;quot;: 6, &amp;quot;name&amp;quot;: &amp;quot;Hicks&amp;quot;},
    {&amp;quot;id&amp;quot;: 7, &amp;quot;name&amp;quot;: &amp;quot;Devin&amp;quot;},
    {&amp;quot;id&amp;quot;: 8, &amp;quot;name&amp;quot;: &amp;quot;Kate&amp;quot;},
    {&amp;quot;id&amp;quot;: 9, &amp;quot;name&amp;quot;: &amp;quot;Klein&amp;quot;}
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of &lt;strong&gt;note&lt;/strong&gt; here is that the &lt;code&gt;users&lt;/code&gt; variable is a &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;dict&lt;/code&gt; (dictionaries).&lt;/p&gt;
&lt;p&gt;Moving along, we also receive &amp;ldquo;friendship&amp;rdquo; data. Of &lt;strong&gt;note&lt;/strong&gt; here that this is a &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;tuples&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;friendship_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (3,4),
                    (4,5), (5,6), (5,7), (6,8), (7,8), (8,9)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I had initially (and erroneously) thought of &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;dict&lt;/code&gt; and &lt;code&gt;tuple&lt;/code&gt; as &lt;strong&gt;data types&lt;/strong&gt; (like &lt;code&gt;int64&lt;/code&gt;, &lt;code&gt;float64&lt;/code&gt;, &lt;code&gt;string&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;They&amp;rsquo;re rather &lt;strong&gt;collections&lt;/strong&gt;, and somewhat unique to Python and more importantly, &lt;em&gt;informs the way Pythonistas approach and solve problems&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You may feel that having &amp;ldquo;friendship&amp;rdquo; data in a &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;tuple&lt;/code&gt; is not the easiest way to work with data (nor may it be the best way to represent data, but we&amp;rsquo;ll suspend those thoughts for now). Our first task is to convert this &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;tuple&lt;/code&gt; into a form that&amp;rsquo;s more workable; the author proposes we turn it into a &lt;code&gt;dict&lt;/code&gt; where the &lt;code&gt;keys&lt;/code&gt; are user_ids and the &lt;code&gt;values&lt;/code&gt; are &lt;code&gt;list&lt;/code&gt; of friends.&lt;/p&gt;
&lt;p&gt;The argument is that its faster to look things up in a &lt;code&gt;dict&lt;/code&gt; rather than a &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;tuple&lt;/code&gt; (where we&amp;rsquo;d have to iterate over every &lt;code&gt;tuple&lt;/code&gt;). Here&amp;rsquo;s how we&amp;rsquo;d do that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize the dict with an empty list for each user id
friendships = { user[&amp;quot;id&amp;quot;]: [] for user in users }

# Loop over friendship pairs 
# This operation grabs the first, then second integer in each tuple
# It then appends each integer to the newly initialized friendships dict
for i, j in friendship_pairs:
    friendships[i].append(j)
    friendships[j].append(i)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;re &lt;em&gt;initializing&lt;/em&gt; a &lt;code&gt;dict&lt;/code&gt; (called &lt;code&gt;friendships&lt;/code&gt;), then looping over &lt;code&gt;friendship_pairs&lt;/code&gt; to populate &lt;code&gt;friendships&lt;/code&gt;. This is the outcome:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;friendships

{
 0: [1, 2],
 1: [0, 2, 3],
 2: [0, 1, 3],
 3: [1, 2, 4],
 4: [3, 5],
 5: [4, 6, 7],
 6: [5, 8],
 7: [5, 8],
 8: [6, 7, 9],
 9: [8]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each &lt;code&gt;key&lt;/code&gt; in friendships is matched with a &lt;code&gt;value&lt;/code&gt; that is initially an empty list, which then gets populated as we loop over &lt;code&gt;friendship_pairs&lt;/code&gt; and systematically append the user_id that is paired together.&lt;/p&gt;
&lt;p&gt;To understand how the looping happends and, specifically how each &lt;strong&gt;pair&lt;/strong&gt; of user_ids are connected to each other, I created my own mini-toy example. Let&amp;rsquo;s say we&amp;rsquo;re just going to focus on looping through &lt;code&gt;friendship_pairs&lt;/code&gt; for the user &lt;strong&gt;Hero&lt;/strong&gt; whose id is 0.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we&#39;ll set hero to an empty list
hero = []

# for every friendship_pair, if the first integer is 0, which is Hero&#39;s id,
# then append the second integer
for x, y in friendship_pairs:
    if x == 0:
        hero.append(y)
        
# outcome: we can confirm that Hero is connected to  Dunn and Sue
hero # [1,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above gave me better intuition for how this works:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, j in friendship_pairs:
    friendships[i].append(j)  # Add j as a friend of user i
    friendships[j].append(i)  # Add i as a friend of user j
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are some other questions we may be interested in:&lt;/p&gt;
&lt;h4 id=&#34;what-is-the-total-number-of-connections&#34;&gt;What is the total number of connections?&lt;/h4&gt;
&lt;p&gt;Look at how the problem is solved. What&amp;rsquo;s notable to me is how we first define a function &lt;code&gt;number_of_friends(user)&lt;/code&gt; that returns the number of friends for a particular user.&lt;/p&gt;
&lt;p&gt;Then, &lt;code&gt;total_connections&lt;/code&gt; is calculated using a &lt;strong&gt;comprehension&lt;/strong&gt; (tuple?):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def number_of_friends(user):
    &amp;quot;&amp;quot;&amp;quot;How many friends does _user_ have?&amp;quot;&amp;quot;&amp;quot;
    user_id = user[&amp;quot;id&amp;quot;]
    friend_ids = friendships[user_id]
    return len(friend_ids)

total_connections = sum(number_of_friends(user) for user in users)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To be clear, the &lt;strong&gt;(tuple) comprehension&lt;/strong&gt; is a pattern where a function is applied over a for-loop, in one line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# (2, 3, 3, 3, 2, 3, 2, 2, 3, 1)
tuple((number_of_friends(user) for user in users))

# you can double check by calling friendships dict and counting the number of friends each user has
friendships

{
 0: [1, 2],
 1: [0, 2, 3],
 2: [0, 1, 3],
 3: [1, 2, 4],
 4: [3, 5],
 5: [4, 6, 7],
 6: [5, 8],
 7: [5, 8],
 8: [6, 7, 9],
 9: [8]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This pattern of using a one-line for-loop (aka comprehension) will come up often. If we add up all the connections, we get 24 and to find the average, we simply divide by the number of users (10) for 2.4, this part is straight-forward.&lt;/p&gt;
&lt;h4 id=&#34;can-we-sort-who-has-most-to-least-friends-to-find-the-most-connected-individuals&#34;&gt;Can we sort who has most-to-least friends to find the most connected individuals?&lt;/h4&gt;
&lt;p&gt;To answer this question, again, a &lt;strong&gt;list comprehension&lt;/strong&gt; is used. The cool thing is that we re-use functions we had previously created (&lt;code&gt;number_of_friends(user)&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a list that loops over users dict, applying a previously defined function
num_friends_by_id = [(user[&amp;quot;id&amp;quot;], number_of_friends(user)) for user in users]

# Then sort
num_friends_by_id.sort(                                 # Sort the list
    key=lambda id_and_friends: id_and_friends[1],       # by number friends
    reverse=True)                                       # descending order
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have just identified how &lt;em&gt;central&lt;/em&gt; an individual is to the network, and we can expect to explore &lt;strong&gt;degree centrality&lt;/strong&gt; and &lt;strong&gt;networks&lt;/strong&gt; more in future chapters, but for the purposes of &lt;em&gt;this&lt;/em&gt; post, we have identified the central role that &lt;strong&gt;collections&lt;/strong&gt; (lists, dictionaries, tuples) as well as &lt;strong&gt;comprehensions&lt;/strong&gt; play in Python operations.&lt;/p&gt;
&lt;p&gt;In the next post, we&amp;rsquo;ll examing how friendship connections may or may not overlap with interests.&lt;/p&gt;
&lt;h2 id=&#34;datascienster_pt2&#34;&gt;DataScienster_pt2&lt;/h2&gt;
&lt;p&gt;In the previous section, we began examining a toy data set see what kind of Python concepts from the crash course we&amp;rsquo;d see in action.&lt;/p&gt;
&lt;p&gt;What stands out is the use of collections and comprehension. We&amp;rsquo;ll see this trend continue as data is given to us in the form of a list of dict or tuples.&lt;/p&gt;
&lt;p&gt;Often time, we&amp;rsquo;re manipulating the data to make it faster and more efficient to iterate through the data. The tool that comes up quite often is using defaultdict to initialize an empty list. Followed by list comprehensions to iterate through data.&lt;/p&gt;
&lt;p&gt;Indeed, either we&amp;rsquo;re seeing how the author, specifically, approaches problem or how problems are approached in Python, in general.&lt;/p&gt;
&lt;p&gt;What I&amp;rsquo;m keeping in mind is that there are more than one way to approach data science problems and this is one of them.&lt;/p&gt;
&lt;p&gt;With that said, let&amp;rsquo;s pick up where the previous section left off.&lt;/p&gt;
&lt;h4 id=&#34;friends-you-may-know&#34;&gt;Friends you may know&lt;/h4&gt;
&lt;p&gt;We have a sense of the &lt;em&gt;total number of connections&lt;/em&gt; and a sorting of the &lt;em&gt;most connected&lt;/em&gt; individuals. Now, we may want to design a &amp;ldquo;people you may know&amp;rdquo; suggester.&lt;/p&gt;
&lt;p&gt;Quick recap, here&amp;rsquo;s what the friendship dictionary looks like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;friendships

{
 0: [1, 2],
 1: [0, 2, 3],
 2: [0, 1, 3],
 3: [1, 2, 4],
 4: [3, 5],
 5: [4, 6, 7],
 6: [5, 8],
 7: [5, 8],
 8: [6, 7, 9],
 9: [8]
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the first step is to &lt;em&gt;iterate&lt;/em&gt; over friends and collect friends&#39; friend. The following function returns a &lt;strong&gt;list comprehension&lt;/strong&gt;. Let&amp;rsquo;s examine this function line-by-line to understand how it works. It returns friend_of_a_friend (foaf) id for each of the  individuals&#39; id, then grabing the id of &lt;em&gt;their&lt;/em&gt; friends.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll break it down in code below this function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def foaf_ids_bad(user):
    &amp;quot;&amp;quot;&amp;quot;foaf is short for &#39;friend of a friend&#39; &amp;quot;&amp;quot;&amp;quot;
    return [foaf_id
            for friend_id in friendships[user[&amp;quot;id&amp;quot;]]
            for foaf_id in friendships[friend_id]]

# Let&#39;s take Hero, to see Hero&#39;s friends 
# we&#39;ll call the first key of the friendships dict
# Hero has two friends with ids 1 and 2
friendships[0]  # [1,2]

# then we&#39;ll loop over *each* of the friends
friendships[1]  # [0, 2, 3]
friendships[2]  # [0, 1, 3]

# assert that function works
assert foaf_ids_bad(users[0]) == [0, 2, 3, 0, 1, 3]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;can-we-count-mutual-friends&#34;&gt;Can we count mutual friends?&lt;/h4&gt;
&lt;p&gt;To answer this we&amp;rsquo;ll use a &lt;code&gt;Counter&lt;/code&gt;, which we 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_2/#counters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learned&lt;/a&gt; is a &lt;code&gt;dict&lt;/code&gt; subclass. Moreover, the function &lt;code&gt;friends_of_friends(user)&lt;/code&gt;,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter

def friends_of_friends(user):
    user_id = user[&amp;quot;id&amp;quot;]
    return Counter(
        foaf_id
        for friend_id in friendships[user_id]    # for each of my friends,
        for foaf_id in friendships[friend_id]    # find their friends
        if foaf_id != user_id                    # who aren&#39;t me
        and foaf_id not in friendships[user_id]  # and aren&#39;t my friends
    )

# lets look at Hero
# he has two common friends with Chi 
# Chi is neither Hero nor his direct friends
friends_of_friends(users[0])  # Counter({3: 2})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to friendship data, we also have &lt;strong&gt;interest&lt;/strong&gt; data. Here we see a &lt;code&gt;list&lt;/code&gt; of &lt;code&gt;tuples&lt;/code&gt;, containing a user_id and a string representing a specific of technology.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interests = [
    (0, &amp;quot;Hadoop&amp;quot;), (0, &amp;quot;Big Data&amp;quot;), (0, &amp;quot;HBase&amp;quot;), (0, &amp;quot;Java&amp;quot;),
    (0, &amp;quot;Spark&amp;quot;), (0, &amp;quot;Storm&amp;quot;), (0, &amp;quot;Cassandra&amp;quot;),
    (1, &amp;quot;NoSQL&amp;quot;), (1, &amp;quot;MongoDB&amp;quot;), (1, &amp;quot;Cassandra&amp;quot;), (1, &amp;quot;HBase&amp;quot;),
    (1, &amp;quot;Postgres&amp;quot;), (2, &amp;quot;Python&amp;quot;), (2, &amp;quot;scikit-learn&amp;quot;), (2, &amp;quot;scipy&amp;quot;),
    (2, &amp;quot;numpy&amp;quot;), (2, &amp;quot;statsmodels&amp;quot;), (2, &amp;quot;pandas&amp;quot;), (3, &amp;quot;R&amp;quot;), (3, &amp;quot;Python&amp;quot;),
    (3, &amp;quot;statistics&amp;quot;), (3, &amp;quot;regression&amp;quot;), (3, &amp;quot;probability&amp;quot;),
    (4, &amp;quot;machine learning&amp;quot;), (4, &amp;quot;regression&amp;quot;), (4, &amp;quot;decision trees&amp;quot;),
    (4, &amp;quot;libsvm&amp;quot;), (5, &amp;quot;Python&amp;quot;), (5, &amp;quot;R&amp;quot;), (5, &amp;quot;Java&amp;quot;), (5, &amp;quot;C++&amp;quot;),
    (5, &amp;quot;Haskell&amp;quot;), (5, &amp;quot;programming langauges&amp;quot;), (6, &amp;quot;statistics&amp;quot;),
    (6, &amp;quot;probability&amp;quot;), (6, &amp;quot;mathematics&amp;quot;), (6, &amp;quot;theory&amp;quot;),
    (7, &amp;quot;machine learning&amp;quot;), (7, &amp;quot;scikit-learn&amp;quot;), (7, &amp;quot;Mahout&amp;quot;),
    (7, &amp;quot;neural networks&amp;quot;), (8, &amp;quot;neural networks&amp;quot;), (8, &amp;quot;deep learning&amp;quot;),
    (8, &amp;quot;Big Data&amp;quot;), (8, &amp;quot;artificial intelligence&amp;quot;), (9, &amp;quot;Hadoop&amp;quot;),
    (9, &amp;quot;Java&amp;quot;), (9, &amp;quot;MapReduce&amp;quot;), (9, &amp;quot;Big Data&amp;quot;)
    ]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First thing we&amp;rsquo;ll do is find users with a specific interest. This is function returns a &lt;strong&gt;list comprehension&lt;/strong&gt;. It first split each &lt;code&gt;tuple&lt;/code&gt; into &lt;code&gt;user_id&lt;/code&gt; (integer) and &lt;code&gt;user_interest&lt;/code&gt; (string), then conditionally check if the &lt;code&gt;string&lt;/code&gt; in the &lt;code&gt;tuple&lt;/code&gt; matches the input parameter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def data_scientists_who_like(target_interest):
    &amp;quot;&amp;quot;&amp;quot;Find the ids of all users who like the target interests.&amp;quot;&amp;quot;&amp;quot;
    return [user_id
            for user_id, user_interest in interests
            if user_interest == target_interest]
            
# let&#39;s see all user_id who likes &amp;quot;statistics&amp;quot;
data_scientists_who_like(&amp;quot;statistics&amp;quot;)   # [3, 6]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may also want to count the number of times a specific interest comes up. Here&amp;rsquo;s a function for that. We use a basic for-loop and if-statement to check 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_2/#truthiness&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;truthiness&lt;/a&gt; of &lt;code&gt;user_interest == target_interest&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def num_user_with_interest_in(target_interest):
    interest_count = 0
    for user_id, user_interest in interests:
        if user_interest == target_interest:
            interest_count += 1
    return interest_count
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A concern is having to examine a whole list of interests for every search. The author proposes building an index from interests to users. Here, a 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_2/#defaultdict&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;defaultdict&lt;/a&gt; is imported, then populated with user_id&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import defaultdict

# user_ids matched to specific interest
user_ids_by_interest = defaultdict(list)

for user_id, interest in interests:
    user_ids_by_interest[interest].append(user_id)

# three users interested in Python
assert user_ids_by_interest[&amp;quot;Python&amp;quot;] == [2,3,5]

# list of interests by user_id
interests_by_user_id = defaultdict(list)

for user_id, interest in interests:
    interests_by_user_id[user_id].append(interest)

# check all of Hero&#39;s interests
assert interests_by_user_id[0] == [&#39;Hadoop&#39;, &#39;Big Data&#39;, &#39;HBase&#39;, &#39;Java&#39;, &#39;Spark&#39;, &#39;Storm&#39;, &#39;Cassandra&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can find who has the most interests in common with a given user. Looks like Klein (#9) has the most common interests with Hero (#0). Here we return a Counter with for-loops and an if-statement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def most_common_interests_with(user):
    return Counter(
        interested_user_id
        for interest in interests_by_user_id[user[&amp;quot;id&amp;quot;]]
        for interested_user_id in user_ids_by_interest[interest]
        if interested_user_id != user[&amp;quot;id&amp;quot;]
        )
        
# let&#39;s check to see who has the most common interest with Hero
most_common_interests_with(users[0]) # Counter({9: 3, 8: 1, 1: 2, 5: 1})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can also find which topics are most popular among the network. Previously, we calculated the number of users interested in a particular topic, but now we want to compare the whole list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;words_and_counts = Counter(word
                           for user, interest in interests
                           for word in interest.lower().split())
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;salaries-and-experience-data&#34;&gt;Salaries and Experience Data&lt;/h4&gt;
&lt;p&gt;We&amp;rsquo;re also given anonymous salary and tenure (number of years work experience) data, let&amp;rsquo;s see what we can do with that information. First we&amp;rsquo;ll find the average salary. Again, we&amp;rsquo;ll start by creating a list (defaultdict), then loop through &lt;code&gt;salaries_and_tenures&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;salaries_and_tenures = [(83000, 8.7), (88000, 8.1),
                        (48000, 0.7), (76000, 6),
                        (69000, 6.5), (76000, 7.5),
                        (60000, 2.5), (83000, 10),
                        (48000, 1.9), (63000, 4.2)]
                        
salary_by_tenure = defaultdict(list)

for salary, tenure in salaries_and_tenures:
    salary_by_tenure[tenure].append(salary)

# find average salary by tenure
average_salary_by_tenure = {
    tenure: sum(salaries) / len(salaries)
    for tenure, salaries in salary_by_tenure.items()
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem is that this is not terribly informative as each tenure value has a different salary. Not even the &lt;code&gt;average_salary_by_tenure&lt;/code&gt; is informative, so our next move is to group similar tenure values together.&lt;/p&gt;
&lt;p&gt;First, we&amp;rsquo;ll create the groupings/categories using a 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_2/#controlflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;control-flow&lt;/a&gt;, then we&amp;rsquo;ll create a &lt;code&gt;list&lt;/code&gt;(&lt;code&gt;defaultdict&lt;/code&gt;), and loop through &lt;code&gt;salaries_and_tenures&lt;/code&gt; to populate the newly created &lt;code&gt;salary_by_tenure_bucket&lt;/code&gt;. Finally calculate the average.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def tenure_bucket(tenure):
    if tenure &amp;lt; 2:
        return &amp;quot;less than two&amp;quot;
    elif tenure &amp;lt; 5:
        return &amp;quot;between two and five&amp;quot;
    else:
        return &amp;quot;more than five&amp;quot;
        
salary_by_tenure_bucket = defaultdict(list)

for salary, tenure in salaries_and_tenures:
    bucket = tenure_bucket(tenure)
    salary_by_tenure_bucket[bucket].append(salary)
    
# finally calculate average
average_salary_by_bucket = {
    tenure_bucket: sum(salaries) / len(salaries)
    for tenure_bucket, salaries in salary_by_tenure_bucket.items()
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing to note is that the &amp;ldquo;given&amp;rdquo; data, in this hypothetical toy example is either in a list of dictionaries or tuples, which may be atypical if we&amp;rsquo;re used to working with tabular data in dataFrame (pandas) or native data.frame in R.&lt;/p&gt;
&lt;p&gt;Again, we are reminded that the higher purpose of this book - Data Science from Scratch (by Joel Grus; 2nd Ed) is to eschew libraries in favor of plain python to build everything from the ground up.&lt;/p&gt;
&lt;p&gt;Should your goal be to learn how various algorithms work by building them up from scratch, and in the process learn how data problems can be solved with python and minimal libraries, this is your book.&lt;/p&gt;
&lt;p&gt;Joel Grus does make clear that you would use libraries and frameworks (pandas, scikit-learn, matplotlib etc), rather than coded-from-scratch algorithms when working in production environments and will point out resource for further reading at the end of the chapters.&lt;/p&gt;
&lt;p&gt;In the next post, we&amp;rsquo;ll get into visualizing data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science from Scratch (ch2)</title>
      <link>/post/dsfs_2/</link>
      <pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_2/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Table of Content:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#setup&#34;&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#functions&#34;&gt;Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#strings&#34;&gt;Strings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#exceptions&#34;&gt;Exceptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#lists&#34;&gt;Lists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#tuples&#34;&gt;Tuples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#dictionaries&#34;&gt;Dictionaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#defaultdict&#34;&gt;defaultdict&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#counters&#34;&gt;Counters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#sets&#34;&gt;Sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#controlflow&#34;&gt;Control Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#truthiness&#34;&gt;Truthiness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#sorting&#34;&gt;Sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#list_comprehensions&#34;&gt;List Comprehensions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#assert&#34;&gt;Assert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#object-oriented_programming&#34;&gt;Object-Oriented Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#iterables_and_generators&#34;&gt;Iterables &amp;amp; Generators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#pseudorandomness&#34;&gt;Pseudorandomness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#regex&#34;&gt;Regular Expression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;chapter-2-a-crash-course-in-python&#34;&gt;Chapter 2: A Crash Course in Python&lt;/h2&gt;
&lt;p&gt;This is the first of many chapters i&amp;rsquo;ll be covering from Joel Grus&#39; Data Science from Scratch book (2nd edition). This chapter provides a quick survey of python features needed for &amp;ldquo;doing&amp;rdquo; data science from scratch, including essential setup of virtual environments and other tooling.&lt;/p&gt;
&lt;p&gt;While the chapter is not meant to be comprehensive, I may supplement certain sections with external content for greater detail in certain parts.&lt;/p&gt;
&lt;p&gt;My goal is twofold. First, to go through this book and, as a byproduct, learn python. Second, to look out for and highlight the areas where the &lt;em&gt;pythonic&lt;/em&gt; way of doing things is necessary to accomplish something in the data science process.&lt;/p&gt;
&lt;p&gt;At several sections throughout this chapter, the author emphasises how much a particular feature will be used later in the book (e.g., functions, dictionaries, list, list comprehensions (and for-loops), assert, iterables and generators, randomness, type annotations). Things &lt;em&gt;not&lt;/em&gt; used as much (e.g., sets, automated test, subclasses that inherit functionality from a parent class, zip and argument unpacking, args, kwargs).&lt;/p&gt;
&lt;p&gt;Additional code can be found in this 
&lt;a href=&#34;https://github.com/PaulApivat/dsfs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;installation-virtual-environment-and-modules&#34;&gt;Installation, Virtual Environment and Modules&lt;/h3&gt;
&lt;p&gt;These section takes the reader through installing a virtual environment using Anaconda Python distribution. The author points out a best practice, &amp;ldquo;you should always work in a virtual environment and never use &amp;lsquo;base&amp;rsquo; Python installation&amp;rdquo;. Moreover, the author favors IPython over jupyter notebooks (he&amp;rsquo;s a noted 
&lt;a href=&#34;https://www.youtube.com/watch?v=7jiPeIFXb6U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;critic of the notebook&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Jeremy Howard of fast.ai offers a contrasting perspective. He &lt;em&gt;does&lt;/em&gt; 
&lt;a href=&#34;https://www.youtube.com/watch?v=9Q6sLbz37gk&amp;amp;feature=emb_title&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;like notebooks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first time I installed Python, it took me awhile to get things right and eventually I relied on jupyter notebooks through Anaconda. As we go through this book, I&amp;rsquo;ll be using virtual environments and IPython as much as I can (although I may sprinkle in a notebook here and there). My IDE for interacting with the conda virtual environment and IPython will be VSCode.&lt;/p&gt;
&lt;p&gt;Fortunately, I had a relatively painless process setting up a virtual environment and IPython, although I had to take a slight detour to setup the &lt;code&gt;code&lt;/code&gt; command for VSCode.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a summary of the commands I used for setup:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./python_virtual_env.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;functions&#34;&gt;Functions&lt;/h2&gt;
&lt;p&gt;Three things are emphasized here: passing functions as arguments for other functions, lambda functions and default parameter values.&lt;/p&gt;
&lt;p&gt;The illustration of functions being passed as arguments is demonstrated below. A function &lt;code&gt;double&lt;/code&gt; is created. A function &lt;code&gt;apply_to_one&lt;/code&gt; is created. The &lt;code&gt;double&lt;/code&gt; function is pointed at &lt;code&gt;my_double&lt;/code&gt;. We pass &lt;code&gt;my_double&lt;/code&gt; into the &lt;code&gt;apply_to_one&lt;/code&gt; function and set that to &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Whatever function is passed to &lt;code&gt;apply_to_one&lt;/code&gt;, &lt;em&gt;its&lt;/em&gt; argument is 1. So passing &lt;code&gt;my_double&lt;/code&gt; means we are doubling 1, so x is 2.&lt;/p&gt;
&lt;p&gt;But the important thing is that a function got passed to another function (aka higher order functions).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def double(x):
    &amp;quot;&amp;quot;&amp;quot;
    this function doubles and returns the argument
    &amp;quot;&amp;quot;&amp;quot;
    return x * 2
    
def apply_to_one(f):
    &amp;quot;&amp;quot;&amp;quot;Calls the function f with 1 as its argument&amp;quot;&amp;quot;&amp;quot;
    return f(1)
    
my_double = double

# x is 2 here
x = apply_to_one(my_double)

# extending this example
def apply_five_to(e):
    &amp;quot;&amp;quot;&amp;quot;returns the function e with 5 as its argument&amp;quot;&amp;quot;&amp;quot;
    return e(5)

# doubling 5 is 10
w = apply_five_to(my_double)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since functions are going to be used extensively, here&amp;rsquo;s another more complicated example. I found this from 
&lt;a href=&#34;https://treyhunner.com/2020/01/passing-functions-as-arguments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trey Hunner&amp;rsquo;s site&lt;/a&gt;. Two functions are defined - &lt;code&gt;square&lt;/code&gt; and &lt;code&gt;cube&lt;/code&gt;. Both functions are saved to a list called &lt;code&gt;operations&lt;/code&gt;. Another list, &lt;code&gt;numbers&lt;/code&gt; is created.&lt;/p&gt;
&lt;p&gt;Finally, a for-loop is used to iterate through &lt;code&gt;numbers&lt;/code&gt;, and the &lt;code&gt;enumerate&lt;/code&gt; property allows access to both index and item in numbers. That&amp;rsquo;s used to find whether the &lt;code&gt;action&lt;/code&gt; is a &lt;code&gt;square&lt;/code&gt; or &lt;code&gt;cube&lt;/code&gt; (operations[0] is &lt;code&gt;square&lt;/code&gt;, operations[1] is &lt;code&gt;cube&lt;/code&gt;), which is then given as its argument, the items inside the &lt;code&gt;numbers&lt;/code&gt; list.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# create two functions
def square(n): return n**2
def cube(n): return n**3

# store those functions inside a list, operations, to reference later
operations = [square, cube]

# create a list of numbers
numbers = [2,1,3,4,7,11,18,29]

# loop through the numbers list
# using enumerate the identify index and items
# [i % 2] results in either 0 or 1, that&#39;s pointed at action
# using the dunder, name, retrieves the name of the function - either square or cube - from the operations list
# print __name__ along with the item from the numbers list
# action is either a square or cube

for i, n in enumerate(numbers):
    action = operations[i % 2]
    print(f&amp;quot;{action.__name__}({n}):&amp;quot;, action(n))

# print
square(2): 4
cube(1): 1
square(3): 9
cube(4): 64
square(7): 49
cube(11): 1331
square(18): 324
cube(29): 24389

# more explicit, yet verbose way to write the for-loop
for index, num in enumerate(numbers):
    action = operations[index % 2]
    print(f&amp;quot;{action.__name__}({num}):&amp;quot;, action(num))

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This section also introduces &lt;code&gt;lambda&lt;/code&gt; functions (aka anonymous functions) to demonstrate how functions, being first-class in Python, can, like any variable, be passed into the argument of another function. However, with &lt;code&gt;lambda&lt;/code&gt; instead of defining functions with &lt;code&gt;def&lt;/code&gt;, it is defined inside another function. Here&amp;rsquo;s an illustration:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# we&#39;ll reuse apply_five_to, which takes in a function and provides &#39;5&#39; as the argument
def apply_five_to(e):
    &amp;quot;&amp;quot;&amp;quot;returns the function e with 5 as its argument&amp;quot;&amp;quot;&amp;quot;
    return e(5)

# this lambda function adds &#39;4&#39; to any argument
# when passing this lambda function to apply_five_to
# you get y = 5 + 4
y = apply_five_to(lambda x: x + 4)

# we can also change what the lambda function does without defining a separate function
# here the lambda function multiplies the argument by 4
# y = 20
y = apply_five_to(lambda x: x * 4)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lambda functions are convenient in that you can pass it into another function &lt;em&gt;immediately&lt;/em&gt; without having to define it separately, but the consensus seems to be that you should just use &lt;code&gt;def&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an external example of &lt;code&gt;lambda&lt;/code&gt; functions from 
&lt;a href=&#34;https://treyhunner.com/2020/01/passing-functions-as-arguments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trey Hunner&lt;/a&gt;. In this example, a &lt;code&gt;lambda&lt;/code&gt; function is used within a &lt;code&gt;filter&lt;/code&gt; function that takes in two arguments.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# calling help(filter) displays an explanation

class filter(object)
 |  filter(function or None, iterable) --&amp;gt; filter object

# create a list of numbers
numbers = [2,1,3,4,7,11,18,29]

# the lambda function will return n if it is an even number
# we filter the numbers list using the lambda function
# wrapped in a list, this returns [2,4,18]
list(filter(lambda n: n % 2 == 0, numbers))

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are whole books, or at least whole chapters, that can be written about Python functions, but we&amp;rsquo;ll limit our discussion for now to the idea that &lt;strong&gt;functions can be passed as arguments to other functions&lt;/strong&gt;. I&amp;rsquo;ll report back on this section as we progress through the book.&lt;/p&gt;
&lt;h2 id=&#34;strings&#34;&gt;Strings&lt;/h2&gt;
&lt;p&gt;Strings may not be terribly exciting for data science or machine learning, unless you&amp;rsquo;re getting into natural language processing, so we&amp;rsquo;ll keep it brief here. The key take aways are that &lt;em&gt;backslashes&lt;/em&gt; encode special characters and that &lt;strong&gt;f-strings&lt;/strong&gt; is the most updated way to do string interpolation. Here are some examples:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# point strings to variables (we&#39;ll use my name)
first_name = &amp;quot;Paul&amp;quot;
last_name = &amp;quot;Apivat&amp;quot;

# f-string (recommended), &#39;Paul Apivat&#39;
f_string = f&amp;quot;{first_name} {last_name}&amp;quot;

# string addition, &#39;Paul Apivat&#39;
string_addition = first_name + &amp;quot; &amp;quot; + last_name

# string format, &#39;Paul Apivat&#39;
string_format = &amp;quot;{0} {1}&amp;quot;.format(first_name, last_name)

# percent format (NOT recommended), &#39;Paul Apivat&#39;
pct_format = &amp;quot;%s %s&amp;quot; %(&#39;Paul&#39;,&#39;Apivat&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;exceptions&#34;&gt;Exceptions&lt;/h2&gt;
&lt;p&gt;The author covers exceptions to make the point that they&amp;rsquo;re not all that bad in Python and it&amp;rsquo;s worth handling exceptions yourself to make code more readable. Here&amp;rsquo;s my own example that&amp;rsquo;s slightly different from the book:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;integer_list = [1,2,3]

heterogeneous_list = [&amp;quot;string&amp;quot;, 0.1, True]

# you can sum a list of integers, here 1 + 2 + 3 = 6
sum(integer_list)

# but you cannot sum a list of heterogeneous data types
# doing so raises a TypeError
sum(heterogeneous_list)

# the error crashes your program and is not fun to look at
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&amp;lt;ipython-input-12-3287dd0c6c22&amp;gt; in &amp;lt;module&amp;gt;
----&amp;gt; 1 sum(heterogeneous_list)

TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39;

# so the idea is to handle the exceptions with your own messages
try:
    sum(heterogeneous_list)
except TypeError:
    print(&amp;quot;cannot add objects of different data types&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, the primary benefits to handling exceptions yourself is for code readability, so we&amp;rsquo;ll come back to this section if we see more useful examples.&lt;/p&gt;
&lt;h2 id=&#34;lists&#34;&gt;Lists&lt;/h2&gt;
&lt;p&gt;Lists are fundamental to Python so I&amp;rsquo;m going to spend some time exploring their features. For data science, &lt;code&gt;NumPy arrays&lt;/code&gt; are used frequently, so I thought it&amp;rsquo;d be good to implement all &lt;code&gt;list&lt;/code&gt; operations covered in this section in &lt;code&gt;Numpy arrays&lt;/code&gt; to &lt;em&gt;tease apart their similarities and differences&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Below are the similarities.&lt;/p&gt;
&lt;p&gt;This implies that whatever can be done in python &lt;code&gt;lists&lt;/code&gt; can also be done in numpy &lt;code&gt;arrays&lt;/code&gt;, including: getting the &lt;em&gt;nth&lt;/em&gt; element in the list/array with square brackets, slicing the list/array, iterating through the list/array with &lt;em&gt;start, stop, step&lt;/em&gt;, using the &lt;code&gt;in&lt;/code&gt; operator to find list/array membership, checking length and unpacking list/arrays.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# setup
import numpy as np

# create comparables
python_list = [1,2,3,4,5,6,7,8,9]
numpy_array = np.array([1,2,3,4,5,6,7,8,9])

# bracket operations

# get nth element with square bracket
python_list[0] # 1
numpy_array[0] # 1
python_list[8] # 9
numpy_array[8] # 9
python_list[-1] # 9
numpy_array[-1] # 9

# square bracket to slice 
python_list[:3] # [1, 2, 3]
numpy_array[:3] # array([1, 2, 3])

python_list[1:5] # [2, 3, 4, 5]
numpy_array[1:5] # array([2, 3, 4, 5])

# start, stop, step
python_list[1:8:2] # [2, 4, 6, 8]
numpy_array[1:8:2] # array([2, 4, 6, 8])

# use in operator to check membership
1 in python_list # true
1 in numpy_array # true

0 in python_list # false
0 in numpy_array # false

# finding length
len(python_list) # 9
len(numpy_array) # 9

# unpacking
x,y = [1,2] # now x is 1, y is 2
w,z = np.array([1,2]) # now w is 1, z is 2


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, here are the differences.&lt;/p&gt;
&lt;p&gt;These tasks can be done in python &lt;code&gt;lists&lt;/code&gt;, but require a different approach for NumPy &lt;code&gt;array&lt;/code&gt; including: modification (extend in list, append for array). Finally, lists can store mixed data types, while NumPy array will convert to string.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# python lists can store mixed data types
heterogeneous_list = [&#39;string&#39;, 0.1, True]
type(heterogeneous_list[0]) # str
type(heterogeneous_list[1]) # float
type(heterogeneous_list[2]) # bool

# numpy arrays cannot store mixed data types
# numpy arrays turn all data types into strings
homogeneous_numpy_array = np.array([&#39;string&#39;, 0.1, True]) # saved with mixed data types
type(homogeneous_numpy_array[0]) # numpy.str_
type(homogeneous_numpy_array[1]) # numpy.str_
type(homogeneous_numpy_array[2]) # numpy.str_


# modifying list vs numpy array

# lists can use extend to modify list in place
python_list.extend([10,12,13])  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13]
numpy_array.extend([10,12,13]) # AttributeError: &#39;numpy.ndarray&#39;

# numpy array must use append, instead of extend
numpy_array = np.append(numpy_array,[10,12,13])

# python lists can be added with other lists
new_python_list = python_list + [14,15] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15]
numpy_array + [14,15] # ValueError

# numpy array cannot be added (use append instead)
# array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15])
new_numpy_array = np.append(numpy_array, [14,15]) 

# python lists have the append attribute
python_list.append(0) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 0]

# the append attribute for numpy array is used differently
numpy_array = np.append(numpy_array, [0])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python &lt;code&gt;lists&lt;/code&gt; and NumPy &lt;code&gt;arrays&lt;/code&gt; have much in common, but there are meaningful differences as well.&lt;/p&gt;
&lt;h4 id=&#34;python-lists-vs-numpy-arrays-whats-the-difference&#34;&gt;Python Lists vs NumPy Arrays: What&amp;rsquo;s the difference&lt;/h4&gt;
&lt;p&gt;Now that we know that there &lt;em&gt;are&lt;/em&gt; meaningful differences, what can we attribute these differences to? This 
&lt;a href=&#34;https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;explainer from UCF&lt;/a&gt; highlights &lt;strong&gt;performance&lt;/strong&gt; differences including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Size&lt;/li&gt;
&lt;li&gt;Performance&lt;/li&gt;
&lt;li&gt;Functionality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&amp;rsquo;m tempted to go down this ð ð³ï¸ of further &lt;code&gt;lists&lt;/code&gt; vs &lt;code&gt;array&lt;/code&gt; comparisons, but we&amp;rsquo;ll hold off for now.&lt;/p&gt;
&lt;h2 id=&#34;tuples&#34;&gt;Tuples&lt;/h2&gt;
&lt;p&gt;Similar to &lt;code&gt;lists&lt;/code&gt;, but &lt;code&gt;tuples&lt;/code&gt; are immutable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
my_list = [1,2]   # check type(my_list)
my_tuple = (1,2)  # check type(my_tuple)
other_tuple = 3,4 # tuples don&#39;t require parentheses

my_list[1] = 3    # lists ARE mutable, my_list is now [1,3]

# exception handling when trying to change tuple
try:
    my_tuple[1] = 3
except TypeError:
    print(&amp;quot;tuples are immutable&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Tuples&lt;/code&gt; are good at returning multiple values from functions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# use tuple to return multiple values
def sum_and_product(x,y):
    &amp;quot;&amp;quot;&amp;quot;you can return multiple values from functions using tuples&amp;quot;&amp;quot;&amp;quot;
    return (x + y), (x * y)
    
sp = sum_and_product(4,5)  # sp is (9,20), a tuple

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, &lt;code&gt;lists&lt;/code&gt; can also be used to return multiple values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def sum_and_product_list(x,y):
    return [(x + y), (x * y)]

spl = sum_and_product_list(5,6)  # [11, 30]
type(spl) # list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, both &lt;code&gt;tuples&lt;/code&gt; and &lt;code&gt;lists&lt;/code&gt; can be used for multiple assignments, here&amp;rsquo;s a pythonic way to swap variables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x, y = 1,2
x,y = y,x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tuples, for the most part, seem to be redundant with &lt;code&gt;lists&lt;/code&gt;, but we&amp;rsquo;ll see if there are special use-cases for immutability down the line.&lt;/p&gt;
&lt;h2 id=&#34;dictionaries&#34;&gt;Dictionaries&lt;/h2&gt;
&lt;p&gt;Dictionaries are good for storing structured data. They have a key/value pair so you can look up values of certain keys. The author provides some ways to initialize a dictionary, with comments about what is &lt;em&gt;more or less pythonic&lt;/em&gt; (I&amp;rsquo;ll take the author&amp;rsquo;s word for it, but open to other perspectives).&lt;/p&gt;
&lt;p&gt;Some of the things you can do with &lt;code&gt;dictionaries&lt;/code&gt; are query keys, values, assign new key/value pairs, check for existence of keys or retrieve certain values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
empty_dict = {}                   # most pythonic
empty_dict2 = dict()              # less pythonic
grades = {&amp;quot;Joel&amp;quot;: 80, &amp;quot;Grus&amp;quot;: 99} # dictionary literal

type(grades)  # type check, dict

# use bracket to look up values
grades[&amp;quot;Grus&amp;quot;]  # 99
grades[&amp;quot;Joel&amp;quot;]  # 80

# KeyError for looking up non-existent keys
try:
   kate_grades = grades[&amp;quot;Kate&amp;quot;]
except KeyError:
   print(&amp;quot;That key doesn&#39;t exist&amp;quot;)
   
# use in operator to check existence of key
joe_has_grade = &amp;quot;Joel&amp;quot; in grades  
joe_has_grade # true

kate_does_not = &amp;quot;Kate&amp;quot; in grades
kate_does_not # false

# use &#39;get&#39; method to get values in dictionaries
grades.get(&amp;quot;Joel&amp;quot;) # 80
grades.get(&amp;quot;Grus&amp;quot;) # 99
grades.get(&amp;quot;Kate&amp;quot;) # default: None

# assign new key/value pair using brackets
grades[&amp;quot;Tim&amp;quot;] = 93

grades # {&#39;Joel&#39;: 80, &#39;Grus&#39;: 99, &#39;Tim&#39;: 93}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dictionaries are good for representing structured data that can be queries. The key take-away here is that in order to iterate through &lt;code&gt;dictionaries&lt;/code&gt; to get either &lt;code&gt;keys&lt;/code&gt;, &lt;code&gt;values&lt;/code&gt; or both, we&amp;rsquo;ll need to use specific methods likes &lt;code&gt;keys()&lt;/code&gt;, &lt;code&gt;values()&lt;/code&gt; or &lt;code&gt;items()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
tweet = {
    &amp;quot;user&amp;quot;: &amp;quot;paulapivat&amp;quot;,
    &amp;quot;text&amp;quot;: &amp;quot;Reading Data Science from Scratch&amp;quot;,
    &amp;quot;retweet_count&amp;quot;: 100,
    &amp;quot;hashtags&amp;quot;: [&amp;quot;#66daysofdata&amp;quot;, &amp;quot;datascience&amp;quot;, &amp;quot;machinelearning&amp;quot;, &amp;quot;python&amp;quot;, &amp;quot;R&amp;quot;]
    }
    
# query specific values
tweet[&amp;quot;retweet_count&amp;quot;] # 100

# query values within a list
tweet[&amp;quot;hashtags&amp;quot;] # [&#39;#66daysofdata&#39;, &#39;datascience&#39;, &#39;machinelearning&#39;, &#39;python&#39;, &#39;R&#39;]
tweet[&amp;quot;hashtags&amp;quot;][2] # &#39;machinelearning&#39;

# retrieve ALL keys
tweet_keys = tweet.keys()
tweet_keys              # dict_keys([&#39;user&#39;, &#39;text&#39;, &#39;retweet_count&#39;, &#39;hashtags&#39;])
type(tweet_keys)        # different data type: dict != dict_keys

# retrieve ALL values
tweet_values = tweet.values() 
tweet_values  # dict_values([&#39;paulapivat&#39;, &#39;Reading Data Science from Scratch&#39;, 100, [&#39;#66daysofdata&#39;, &#39;datascience&#39;, &#39;machinelearning&#39;, &#39;python&#39;, &#39;R&#39;]])

type(tweet_values)      # different data type: dict != dict_values

# create iterable for Key-Value pairs (in tuple)
tweet_items = tweet.items()

# iterate through tweet_items()
for key,value in tweet_items:
    print(&amp;quot;These are the keys:&amp;quot;, key)
    print(&amp;quot;These are the values:&amp;quot;, value)
    
# cannot iterate through original tweet dictionary
# ValueError: too many values to unpack (expected 2)
for key, value in tweet:
    print(key)
    
# cannot use &#39;enumerate&#39; because that only provides index and key (no value)
for key, value in enumerate(tweet):
    print(key)   # print 0 1 2 3 - index values
    print(value) # user text retweet_count hashtags (incorrectly print keys)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like in &lt;code&gt;lists&lt;/code&gt; and &lt;code&gt;tuples&lt;/code&gt;, you can use the &lt;code&gt;in&lt;/code&gt; operator to find membership. The one caveat is you cannot look up &lt;em&gt;values&lt;/em&gt; that are in &lt;code&gt;lists&lt;/code&gt;, unless you use bracket notation to help.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# search keys
&amp;quot;user&amp;quot; in tweet # true
&amp;quot;bball&amp;quot; in tweet # false

&amp;quot;paulapivat&amp;quot; in tweet_values # true
&#39;python&#39; in tweet_values # false (python is nested in &#39;hashtags&#39;)
&amp;quot;hashtags&amp;quot; in tweet  # true

# finding values inside a list requires brackets to help
&#39;python&#39; in tweet[&#39;hashtags&#39;]  # true

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;What is or is not hashable?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Dictionary&lt;/code&gt; keys must be hashable.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Strings&lt;/code&gt; are hashable. So we can use &lt;code&gt;strings&lt;/code&gt; as dictionary keys, but we &lt;strong&gt;cannot&lt;/strong&gt; use &lt;code&gt;lists&lt;/code&gt; because they are not hashable.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
paul = &amp;quot;paul&amp;quot;
type(paul)        # check type, str

hash(paul)        # -3897810863245179227 ; strings are hashable
paul.__hash__()   # -3897810863245179227 ; another way to find the hash

jake = [&#39;jake&#39;]   # this is a list
type(jake)        # check type, list

# lists are not hashable - cannot be used as dictionary keys
try:
   hash(jake)
except TypeError:
   print(&#39;lists are not hashable&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;defaultdict&#34;&gt;defaultdict&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;defaultdict&lt;/code&gt; is a &lt;strong&gt;subclass&lt;/strong&gt; of dictionaries (&lt;code&gt;dict&lt;/code&gt;, see previous post), so it &lt;em&gt;inherits&lt;/em&gt; most of its behavior from &lt;code&gt;dict&lt;/code&gt; with additional features. To understand how those features make it different, and more convenient in some cases, we&amp;rsquo;ll need to run into some errors.&lt;/p&gt;
&lt;p&gt;If we try to count words in a document, the general approach is to create a dictionary where the dictionary &lt;code&gt;keys&lt;/code&gt; are words and the dictionary &lt;code&gt;values&lt;/code&gt; are counts of those words.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try do do this with a regular dictionary.&lt;/p&gt;
&lt;p&gt;First, to setup, we&amp;rsquo;ll take a list of words and &lt;code&gt;split()&lt;/code&gt; into individual words. I took this paragraph from 
&lt;a href=&#34;https://rpubs.com/paulapivat/vintage_nba_seasons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;another project&lt;/a&gt; i&amp;rsquo;m working on and artificially added some extra words to ensure that certain words appeared more than once (it&amp;rsquo;ll be apparent why soon).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# paragraph
lines = [&amp;quot;This table highlights 538&#39;s new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes minutes the table Wins NBA NBA RAPTOR more players&amp;quot;]

# split paragraphy into individual words
lines = &amp;quot; &amp;quot;.join(lines).split()

type(lines) # list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our &lt;code&gt;lines&lt;/code&gt; list, we&amp;rsquo;ll create an empty &lt;code&gt;dict&lt;/code&gt; called &lt;code&gt;word_counts&lt;/code&gt; and have each word be the &lt;code&gt;key&lt;/code&gt; and each &lt;code&gt;value&lt;/code&gt; be the count of that word.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# empty list
word_counts = {}

# loop through lines to count each word
for word in lines:
    word_counts[word] += 1
    
# KeyError: &#39;This&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We received a &lt;code&gt;KeyError&lt;/code&gt; for the very first word in &lt;code&gt;lines&lt;/code&gt; (i.e. &amp;lsquo;This&amp;rsquo;) because the &lt;strong&gt;list tried to count a key that didn&amp;rsquo;t exist&lt;/strong&gt;. We&amp;rsquo;ve learned to handle exceptions so we can use &lt;code&gt;try&lt;/code&gt; and &lt;code&gt;except&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here, we&amp;rsquo;re looping through &lt;code&gt;lines&lt;/code&gt; and when we try to count a key that doesn&amp;rsquo;t exist, like we did previously, we&amp;rsquo;re &lt;em&gt;now&lt;/em&gt; anticipating a &lt;code&gt;KeyError&lt;/code&gt; and will set the initial count to 1, then it can continue to loop-through and count the word, which now exists, so it can be incremented up.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# empty list
word_counts = {}

# exception handling
for word in lines:
    try:
        word_counts[word] += 1
    except KeyError:
        word_counts[word] = 1

# call word_counts
# abbreviated for space
word_counts

{&#39;This&#39;: 1,
 &#39;table&#39;: 3,
 &#39;highlights&#39;: 1,
 &amp;quot;538&#39;s&amp;quot;: 1,
 &#39;new&#39;: 1,
 &#39;NBA&#39;: 3,
 &#39;statistic,&#39;: 1,
 &#39;RAPTOR,&#39;: 1,
 &#39;in&#39;: 2,
 &#39;addition&#39;: 1,
 &#39;to&#39;: 3,
 &#39;the&#39;: 5,
 &#39;more&#39;: 2,
 ...
 &#39;top-100&#39;: 1,
 &#39;players&#39;: 2,
 &#39;who&#39;: 1,
 &#39;have&#39;: 1,
 &#39;played&#39;: 1,
 &#39;at&#39;: 1,
 &#39;least&#39;: 1,
 &#39;1,000&#39;: 1,
 &#39;minutes&#39;: 2,
 &#39;RAPTOR&#39;: 1}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, there are other ways to achieve the above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# use conditional flow
word_counts = {}

for word in lines:
    if word in word_counts:
        word_counts[word] += 1
    else:
        word_counts[word] = 1
        
# use get
for word in lines:
    previous_count = word_counts.get(word, 0)
    word_counts[word] = previous_count + 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s where the author makes the case for &lt;code&gt;defaultdict&lt;/code&gt;, arguing that the two aforementioned approaches are unweildy. We&amp;rsquo;ll come back full circle to try our first approach, using &lt;code&gt;defaultdict&lt;/code&gt; instead of the traditional &lt;code&gt;dict&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;defaultdict&lt;/code&gt; is a subclass of &lt;code&gt;dict&lt;/code&gt; and must be imported from &lt;code&gt;collections&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import defaultdict

word_counts = defaultdict(int)

for word in lines:
    word_counts[word] += 1
    
# we no longer get a KeyError
# abbreviated for space
defaultdict(int,
            {&#39;This&#39;: 1,
             &#39;table&#39;: 3,
             &#39;highlights&#39;: 1,
             &amp;quot;538&#39;s&amp;quot;: 1,
             &#39;new&#39;: 1,
             &#39;NBA&#39;: 3,
             &#39;statistic,&#39;: 1,
             &#39;RAPTOR,&#39;: 1,
             &#39;in&#39;: 2,
             &#39;addition&#39;: 1,
             &#39;to&#39;: 3,
             &#39;the&#39;: 5,
             &#39;more&#39;: 2,
             ...
             &#39;top-100&#39;: 1,
             &#39;players&#39;: 2,
             &#39;who&#39;: 1,
             &#39;have&#39;: 1,
             &#39;played&#39;: 1,
             &#39;at&#39;: 1,
             &#39;least&#39;: 1,
             &#39;1,000&#39;: 1,
             &#39;minutes&#39;: 2,
             &#39;RAPTOR&#39;: 1})

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike a regular dictionary, when &lt;code&gt;defaultdict&lt;/code&gt; tries to look up a key it doesn&amp;rsquo;t contain, it&amp;rsquo;ll automatically add a value for it using the argument we provided when we first created the &lt;code&gt;defaultdict&lt;/code&gt;. If you see above, we entered an &lt;code&gt;int&lt;/code&gt; as the argument, which allows it to automatically &lt;em&gt;add an integer value&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If you want your &lt;code&gt;defaultdict&lt;/code&gt; to have &lt;code&gt;values&lt;/code&gt; be &lt;code&gt;lists&lt;/code&gt;, you can pass a &lt;code&gt;list&lt;/code&gt; as argument. Then, when you &lt;code&gt;append&lt;/code&gt; a value, it is automatically contained in a &lt;code&gt;list&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dd_list = defaultdict(list) # defaultdict(list, {})

dd_list[2].append(1)        # defaultdict(list, {2: [1]})

dd_list[4].append(&#39;string&#39;) # defaultdict(list, {2: [1], 4: [&#39;string&#39;]})

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also pass a &lt;code&gt;dict&lt;/code&gt; into &lt;code&gt;defaultdict&lt;/code&gt;, ensuring that all appended values are contained in a &lt;code&gt;dict&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
dd_dict = defaultdict(dict) # defaultdict(dict, {})

# match key-with-value
dd_dict[&#39;first_name&#39;] = &#39;lebron&#39; # defaultdict(dict, {&#39;first_name&#39;: &#39;lebron&#39;})
dd_dict[&#39;last_name&#39;] = &#39;james&#39;   

# match key with dictionary containing another key-value pair
dd_dict[&#39;team&#39;][&#39;city&#39;] = &#39;Los Angeles&#39;

# defaultdict(dict,
#            {&#39;first_name&#39;: &#39;lebron&#39;,
#             &#39;last_name&#39;: &#39;james&#39;,
#             &#39;team&#39;: {&#39;city&#39;: &#39;Los Angeles&#39;}})

&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;application-grouping-with-defaultdict&#34;&gt;Application: Grouping with defaultdict&lt;/h4&gt;
&lt;p&gt;The follow example is from 
&lt;a href=&#34;https://realpython.com/python-defaultdict/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Real Python&lt;/a&gt;, a fantastic resource for all things Python.&lt;/p&gt;
&lt;p&gt;It is common to use &lt;code&gt;defaultdict&lt;/code&gt; to group items in a sequence or collection, setting the initial parameter (aka &lt;code&gt;.default_factory&lt;/code&gt;) set to &lt;code&gt;list&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dep = [(&#39;Sales&#39;, &#39;John Doe&#39;),
       (&#39;Sales&#39;, &#39;Martin Smith&#39;),
       (&#39;Accounting&#39;, &#39;Jane Doe&#39;),
       (&#39;Marketing&#39;, &#39;Elizabeth Smith&#39;),
       (&#39;Marketing&#39;, &#39;Adam Doe&#39;)]
       
from collections import defaultdict

dep_dd = defaultdict(list)

for department, employee in dep:
    dep_dd[department].append(employee)
    
dep_dd
#defaultdict(list,
#            {&#39;Sales&#39;: [&#39;John Doe&#39;, &#39;Martin Smith&#39;],
#             &#39;Accounting&#39;: [&#39;Jane Doe&#39;],
#             &#39;Marketing&#39;: [&#39;Elizabeth Smith&#39;, &#39;Adam Doe&#39;]})

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happens when you have &lt;strong&gt;duplicate&lt;/strong&gt; entries? We&amp;rsquo;re jumping ahead slightly to use &lt;code&gt;set&lt;/code&gt; handle duplicates and only group unique entries:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# departments with duplicate entries
dep = [(&#39;Sales&#39;, &#39;John Doe&#39;),
       (&#39;Sales&#39;, &#39;Martin Smith&#39;),
       (&#39;Accounting&#39;, &#39;Jane Doe&#39;),
       (&#39;Marketing&#39;, &#39;Elizabeth Smith&#39;),
       (&#39;Marketing&#39;, &#39;Elizabeth Smith&#39;),
       (&#39;Marketing&#39;, &#39;Adam Doe&#39;),
       (&#39;Marketing&#39;, &#39;Adam Doe&#39;),
       (&#39;Marketing&#39;, &#39;Adam Doe&#39;)]

# use defaultdict with set
dep_dd = defaultdict(set)

# set object has no attribute &#39;append&#39;
# so use &#39;add&#39; to achieve the same effect
for department, employee in dep:
    dep_dd[department].add(employee)
    
dep_dd
#defaultdict(set,
#            {&#39;Sales&#39;: {&#39;John Doe&#39;, &#39;Martin Smith&#39;},
#             &#39;Accounting&#39;: {&#39;Jane Doe&#39;},
#             &#39;Marketing&#39;: {&#39;Adam Doe&#39;, &#39;Elizabeth Smith&#39;}})
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;application-accumulating-with-defaultdict&#34;&gt;Application: Accumulating with defaultdict&lt;/h4&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll use &lt;code&gt;defaultdict&lt;/code&gt; to accumulate values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;incomes = [(&#39;Books&#39;, 1250.00),
           (&#39;Books&#39;, 1300.00),
           (&#39;Books&#39;, 1420.00),
           (&#39;Tutorials&#39;, 560.00),
           (&#39;Tutorials&#39;, 630.00),
           (&#39;Tutorials&#39;, 750.00),
           (&#39;Courses&#39;, 2500.00),
           (&#39;Courses&#39;, 2430.00),
           (&#39;Courses&#39;, 2750.00),]

# enter float as argument        
dd = defaultdict(float)  # collections.defaultdict

# defaultdict(float, {&#39;Books&#39;: 3970.0, &#39;Tutorials&#39;: 1940.0, &#39;Courses&#39;: 7680.0})
for product, income in incomes:
    dd[product] += income
    
for product, income in dd.items():
    print(f&amp;quot;Total income for {product}: ${income:,.2f}&amp;quot;)

# Total income for Books: $3,970.00
# Total income for Tutorials: $1,940.00
# Total income for Courses: $7,680.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can see that &lt;code&gt;defaultdict&lt;/code&gt; and &lt;code&gt;dictionaries&lt;/code&gt; can be handy for grouping, counting and accumulating values in a column. We&amp;rsquo;ll come back to revisit these foundational concepts once the data science applications are clearer.&lt;/p&gt;
&lt;p&gt;In summary, &lt;code&gt;dictionaries&lt;/code&gt; and &lt;code&gt;defaultdict&lt;/code&gt; can be used to group items, accumulate items and count items. Both can be used even when the &lt;code&gt;key&lt;/code&gt; doesn&amp;rsquo;t (yet) exist, but its &lt;code&gt;defaultdict&lt;/code&gt; handles this more succintly. For now, we&amp;rsquo;ll stop here and proceed to the next topic: counters.&lt;/p&gt;
&lt;h2 id=&#34;counters&#34;&gt;Counters&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Counter&lt;/code&gt; is a &lt;code&gt;dict&lt;/code&gt; &lt;strong&gt;subclass&lt;/strong&gt; for counting hashable objects (see 
&lt;a href=&#34;https://docs.python.org/3/library/collections.html#collections.Counter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;).
Back to our example in the previous section, we can use &lt;code&gt;Counter&lt;/code&gt; instead of &lt;code&gt;dict&lt;/code&gt;, specifically for counting:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter

# we can count the letters in this paragraph
count_letters = Counter(&amp;quot;This table highlights 538&#39;s new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes minutes the table Wins NBA NBA RAPTOR more players&amp;quot;)

# call count_letters
count_letters

# returns
Counter({&#39;T&#39;: 4,
         &#39;h&#39;: 19,
         &#39;i&#39;: 22,
         &#39;s&#39;: 24,
         &#39; &#39;: 61,
         &#39;t&#39;: 29,
         &#39;a&#39;: 20,
         &#39;b&#39;: 5,
         &#39;l&#39;: 14,
         &#39;e&#39;: 35,
         &#39;g&#39;: 5,
         &#39;5&#39;: 1,
         &#39;3&#39;: 1,
         &#39;8&#39;: 1,
         &amp;quot;&#39;&amp;quot;: 1,
         &#39;n&#39;: 13,
         &#39;w&#39;: 3,
         &#39;N&#39;: 3,
         &#39;B&#39;: 3,
         &#39;A&#39;: 8,
         &#39;c&#39;: 3,
         &#39;,&#39;: 6,
         &#39;R&#39;: 6,
         &#39;P&#39;: 4,
         &#39;O&#39;: 3,
         &#39;d&#39;: 7,
         &#39;o&#39;: 15,
         &#39;m&#39;: 8,
         &#39;r&#39;: 13,
         &#39;W&#39;: 4,
         &#39;v&#39;: 3,
         &#39;p&#39;: 8,
         &#39;(&#39;: 2,
         &#39;)&#39;: 2,
         &#39;.&#39;: 2,
         &#39;x&#39;: 1,
         &#39;u&#39;: 3,
         &#39;y&#39;: 4,
         &#39;f&#39;: 3,
         &#39;/&#39;: 1,
         &#39;-&#39;: 2,
         &#39;k&#39;: 1,
         &#39;1&#39;: 2,
         &#39;0&#39;: 5})

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Counter&lt;/code&gt; very easily did what &lt;code&gt;defaultdict(int)&lt;/code&gt; did previously. We can even call the &lt;code&gt;most_common&lt;/code&gt; method to get the most common letters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# get the thirteen most common letters
for letter, count in count_letters.most_common(13):
   print(letter, count)
   
# returns - 13 items
  61
e 35
t 29
s 24
i 22
a 20
h 19
o 15
l 14
n 13
r 13
A 8
m 8
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sets&#34;&gt;Sets&lt;/h2&gt;
&lt;p&gt;We had a glimpse of &lt;code&gt;set&lt;/code&gt; previously. There are two things the author emphasize with &lt;code&gt;set&lt;/code&gt;. First, they&amp;rsquo;re faster than lists for checking membership:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
lines_list = [&amp;quot;This table highlights 538&#39;s new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes minutes the table Wins NBA NBA RAPTOR more players&amp;quot;]

&amp;quot;zip&amp;quot; in lines_list # False, but have to check every element

lines_set = set(lines_list)
type(lines_set) # set

&amp;quot;zip&amp;quot; in lines_set # Very fast to check
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because this was an arbitrary example, it&amp;rsquo;s not obvious that checking membership in &lt;code&gt;set&lt;/code&gt; is faster than &lt;code&gt;list&lt;/code&gt; so we&amp;rsquo;ll take the author&amp;rsquo;s word for it.&lt;/p&gt;
&lt;p&gt;The second highlight for &lt;code&gt;set&lt;/code&gt; is to find &lt;strong&gt;distinct&lt;/strong&gt; items in a collection:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;number_list = [1,2,3,1,2,3] # list with six items
item_set = set(number_list) # turn it into a set

item_set # now has three items {1, 2, 3}
turn_into_list = list(item_set) # turn into distinct item list
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;controlflow&#34;&gt;Controlflow&lt;/h2&gt;
&lt;p&gt;I believe the main take away from this section is to briefly highlight the various control flows possible.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a traditional if-else statement:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = 5

if x % 2 == 0:
    parity = &amp;quot;even&amp;quot;
else:
    parity = &amp;quot;odd&amp;quot;
    
parity # &#39;odd&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The author may, from time to time, opt to use a shorter &lt;em&gt;ternary&lt;/em&gt; if-else one-liner, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;parity = &amp;quot;even&amp;quot; if x % 2 == 0 else &amp;quot;odd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The author points out that while &lt;strong&gt;while-loops&lt;/strong&gt; exist:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = 0

while x &amp;lt; 10:
    print(f&amp;quot;{x} is less than 10&amp;quot;)
    x += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;for&lt;/strong&gt; and &lt;strong&gt;in&lt;/strong&gt; will be used more often (the code below is both shorter and more readable):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for x in range(10):
    print(f&amp;quot;{x} is less than 10&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll also &lt;strong&gt;note&lt;/strong&gt; that &lt;code&gt;range(x)&lt;/code&gt; also goes up to &lt;code&gt;x-1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, more complex logic &lt;em&gt;is&lt;/em&gt; possible, although we&amp;rsquo;ll have to revisit exactly when more complex logic is used in a data science context.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for x in range(10):
    if x == 3:
        continue
    if x == 5:
        break
    print(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;truthiness&#34;&gt;Truthiness&lt;/h2&gt;
&lt;p&gt;Booleans in Python, &lt;code&gt;True&lt;/code&gt; and &lt;code&gt;False&lt;/code&gt;, are only have the first letter capitalized. And Python uses &lt;code&gt;None&lt;/code&gt; to indicate a nonexistent value. We&amp;rsquo;ll try to handle the exception below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;1 &amp;lt; 2 # True (not TRUE)
1 &amp;gt; 2 # False (not FALSE)

x = 1
try:
    assert x is None
except AssertionError:
    print(&amp;quot;There was an AssertionError because x is not &#39;None&#39;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A major takeaway for me is the concept of &amp;ldquo;truthy&amp;rdquo; and &amp;ldquo;falsy&amp;rdquo;. The first thing to note is that anything &lt;em&gt;after&lt;/em&gt; &lt;code&gt;if&lt;/code&gt; implies &amp;ldquo;is true&amp;rdquo; which is why if-statements can be used to &lt;strong&gt;check&lt;/strong&gt; is a list, string or dictionary is empty:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = [1]
y = []

# if x...is true
# Truthy
if x:
    print(&amp;quot;Truthy&amp;quot;)
else:
    print(&amp;quot;Falsy&amp;quot;)

# if y...is true    
# Falsy
print(&amp;quot;Truthy&amp;quot;) if y else print(&amp;quot;Falsy&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You&amp;rsquo;ll note the &lt;em&gt;ternary&lt;/em&gt; version here is slightly less readable. Here are more examples to understand &amp;ldquo;truthiness&amp;rdquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Truthy example

# create a function that returns a string
def some_func():
    return &amp;quot;a string&amp;quot;
    
# set s to some_func 
s = some_func()

# use if-statement to check truthiness - returns &#39;a&#39;
if s:
    first_char = s[0]
else:
    first_char = &amp;quot;&amp;quot;
    
## Falsy example

# another function return empty string
def another_func():
    return &amp;quot;&amp;quot;

# set another_func to y (falsy example)
y = another_func()

# when &#39;truthy&#39; return second value,
# when &#39;falsy&#39; return first value
first_character = y and y[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the author brings up &lt;strong&gt;all&lt;/strong&gt; and &lt;strong&gt;any&lt;/strong&gt; functions. The former returns &lt;code&gt;True&lt;/code&gt; when &lt;em&gt;every&lt;/em&gt; element is truthy; the latter returns &lt;code&gt;True&lt;/code&gt; when &lt;em&gt;at least one&lt;/em&gt; element is truthy:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
all([True, 1, {3}]) # True

all([True, 1, {}])  # False

any([True, 1, {}])  # True

all([])             # True

any([])             # False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You&amp;rsquo;ll note that the truthiness &lt;strong&gt;within&lt;/strong&gt; the list is being evaluated. So &lt;code&gt;all([])&lt;/code&gt; suggests there are no &amp;lsquo;falsy&amp;rsquo; elements within the list, because it&amp;rsquo;s empty, so it evaluates to &lt;code&gt;True&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;code&gt;any([])&lt;/code&gt; suggests not even one (or at least one) element is &amp;lsquo;truthy&amp;rsquo;, because the list is empty, so it evaluates to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;sorting&#34;&gt;Sorting&lt;/h2&gt;
&lt;p&gt;Sorting is generally straight forward with either &lt;code&gt;sorted()&lt;/code&gt; or &lt;code&gt;sort()&lt;/code&gt;. Here&amp;rsquo;s a more complex example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create a list containing one paragraph
lines = [&amp;quot;This table highlights 538&#39;s new NBA statistic, RAPTOR, in addition to the more established Wins Above Replacement (WAR). An extra column, Playoff (P/O) War, is provided to highlight stars performers in the post-season, when the stakes are higher. The table is limited to the top-100 players who have played at least 1,000 minutes minutes the table Wins NBA NBA RAPTOR more players&amp;quot;]

# split paragraph into individual words
lines = &amp;quot; &amp;quot;.join(lines_list).split()

# import Counter
from collections import Counter

# count words in lines
word_counts = Counter(lines)

# sort words and count from largest to smallest
wc = sorted(word_counts.items(),
            key=lambda x: x[1],   # key line
            reverse=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s another example involving coffee:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
coffee_prices = {
   &#39;cappuccino&#39;: 54,
   &#39;latte&#39;: 56,
   &#39;espresso&#39;: 72,
   &#39;americano&#39;: 48,
   &#39;cortado&#39;: 41
}

# .items() access dictionary key-value pairs
# key is what the sorted() function will sort by
# reverse indicates descending or ascending 
sorted(coffee_prices.items(), key=lambda x: x[1], reverse=False)

# [(&#39;cortado&#39;, 41),
# (&#39;americano&#39;, 48),
# (&#39;cappuccino&#39;, 54),
# (&#39;latte&#39;, 56),
# (&#39;espresso&#39;, 72)]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;list_comprehensions&#34;&gt;list_comprehensions&lt;/h2&gt;
&lt;p&gt;Previously, we saw &lt;strong&gt;if-statements&lt;/strong&gt; expressed in one-line, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = []

# Falsy
print(&amp;quot;Truthy&amp;quot;) if y else print(&amp;quot;Falsy&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also write &lt;strong&gt;for-loops&lt;/strong&gt; in one-line. And thats a way to think about &lt;code&gt;list comprehensions&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# traditional for-loop
num = []
for x in range(5):
    if x % 2 == 0:
        num.append(x)

num # call num

# list comprehension, provides the same thing
[x for x in range(5) if x % 2 == 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are some examples from Data Science from Scratch:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# [0, 2, 4]
even_numbers = [x for x in range(5) if x % 2 == 0] 

# [0, 1, 4, 9, 16]
squares = [x * x for x in range(5)]

# [0, 4, 16]
even_squares = [x * x for x in even_numbers]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://dbader.org/blog/list-dict-set-comprehensions-in-python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dan Bader provides&lt;/a&gt; a helpful way to conceptualizing &lt;code&gt;list comprehensions&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(values) = [ (expression) for (item) in (collections) ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A good way to understand &lt;code&gt;list comprehensions&lt;/code&gt; is to de-construct it back to a regular for-loop:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# recreation of even_numbers
even_bracket = []
for x in range(5):
    if x % 2 == 0:
       even_bracket.append(x)
       
# recreation of squares
square_bracket = []
for x in range(5):
    square_bracket.append(x * x)

# recreate even_squares
square_even_bracket = []
for x in even_bracket:
    square_even_bracket.append(x * x)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;List comprehensions also allow for &lt;strong&gt;filtering with conditions&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# traditional for-loop
filtered_bracket = []

for x in range(10):
    if x &amp;gt; 5:
        filtered_bracket.append(x * x)
        
# list comprehension
filtered_comprehension = [x * x
                          for x in range(10)
                          if x &amp;gt; 5]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key take-away here is that &lt;code&gt;list comprehensions&lt;/code&gt; follow a pattern:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;values = [expression
          for item in collection
          if condition]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python also supports dictionaries or sets comprehension, although we&amp;rsquo;ll have to revisit this post as to &lt;strong&gt;why&lt;/strong&gt; we would want to do this in a data wrangling, transformation or analysis context.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}
square_dict = {x: x * x for x in range(5)}

# {1}
square_set = {x * x for x in [1,-1]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, comprehensions can include nested for-loops:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pairs = [(x,y)
         for x in range(10)
         for y in range(10)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will expect to use &lt;code&gt;list comprehensions&lt;/code&gt; often, so we&amp;rsquo;ll revisit this section as we see more applications in context.&lt;/p&gt;
&lt;h4 id=&#34;map-filter-reduce-partial&#34;&gt;Map, Filter, Reduce, Partial&lt;/h4&gt;
&lt;p&gt;In the first edition of this book the author introduced these functions, but has since reached enlightenment ð§, he states:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;On my journey toward enlightenment I have realized that these functions (i.e., map, filter, reduce, partial) are best avoided, and their uses in the book have been replaced with list comprehensions, for loops and other, more Pythonic constructs.&amp;rdquo; (p.36)&lt;/p&gt;
&lt;p&gt;He&amp;rsquo;s being facetious, but I was intrigued anyways. So here&amp;rsquo;s an example replacing &lt;strong&gt;map&lt;/strong&gt; with &lt;strong&gt;list comprehensions&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create list of names
names = [&#39;Russel&#39;, &#39;Kareem&#39;, &#39;Jordan&#39;, &#39;James&#39;]

# use map function to loop over names and apply an anonymous function
greeted = map(lambda x: &#39;Hi &#39; + x, names)

# map returns an iterator (see also lazy evaluation)
print(greeted) # &amp;lt;map object at 0x7fc667c81f40&amp;gt;

# because lazy evaluation, won&#39;t do anything unless iterate over it
for name in greeted:
     print(name)

#Hi Russel
#Hi Kareem
#Hi Jordan
#Hi James

## List Comprehension way to do this operation
greeted2 = [&#39;Hi &#39; + name for name in names]

# non-lazy evaluation (or eager)
print(greeted2) # [&#39;Hi Russel&#39;, &#39;Hi Kareem&#39;, &#39;Hi Jordan&#39;, &#39;Hi James&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s another example replacing &lt;strong&gt;filter&lt;/strong&gt; with &lt;strong&gt;list comprehensions&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create list of integers
numbers = [13, 4, 18, 35]

# filter creates an interator
div_by_5 = filter(lambda num: num % 5 == 0, numbers)

print(div_by_5) # &amp;lt;filter object at 0x7fc667c9ad30&amp;gt;
print(list(div_by_5)) # must convert iterator into a list - [35]

# using list comprehension to achieve the same thing
another_div_by_5 = [num for num in numbers if num % 5 == 0]

# lists do not use lazy evaluation, so it will print out immediately
print(another_div_by_5) # [35]

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;assert&#34;&gt;Assert&lt;/h2&gt;
&lt;h4 id=&#34;automated-testing-and-assert&#34;&gt;Automated Testing and Assert&lt;/h4&gt;
&lt;p&gt;One of the many cool things about Data Science from Scratch (by Joel Grus) is his use of assertions as a way to &amp;ldquo;test&amp;rdquo; code. This is a software engineering practice (see 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Test-driven_development&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;test-driven development&lt;/a&gt;) that may not be as pervasive in data science, but I suspect, will see 
&lt;a href=&#34;https://www.kdnuggets.com/2020/08/unit-test-data-pipeline-thank-yourself-later.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;growth in usage&lt;/a&gt; and will soon become best practice, if we&amp;rsquo;re not 
&lt;a href=&#34;https://www.datacamp.com/courses/unit-testing-for-data-science-in-python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;already there&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While there are testing frameworks that deserve their own chapters, throughout &lt;em&gt;this&lt;/em&gt; book, fortunately the author has provided a simple way to test by way of the &lt;code&gt;assert&lt;/code&gt; key word, here&amp;rsquo;s an example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create function to return the largest value in a list
def largest_item(x):
    return max(x)
    
# assert that our function is working properly
# we will see &#39;nothing&#39; if things are working properly
assert largest_item([10, 20, 5, 40, 99]) == 99

# an AssertionError will pop up if any other value is used
assert largest_item([10, 20, 5, 40, 99]) == 40
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&amp;lt;ipython-input-21-12dc291d091e&amp;gt; in &amp;lt;module&amp;gt;
----&amp;gt; 1 assert largest_item([10, 20, 5, 40, 99]) == 40

# we can also create an assertion for input values
def largest_item(x):
    assert x, &amp;quot;empty list has no largest value&amp;quot;
    return max(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;object-oriented_programming&#34;&gt;Object-Oriented_Programming&lt;/h2&gt;
&lt;p&gt;Object-oriented programming could be it&amp;rsquo;s own chapter, so we won&amp;rsquo;t try to shoot for comprehensiveness here. Instead, we&amp;rsquo;ll try to understand it&amp;rsquo;s basics and the &lt;code&gt;assert&lt;/code&gt; function is going to help us understand it even better.&lt;/p&gt;
&lt;p&gt;Object-oriented programming could be it&amp;rsquo;s own chapter, so we&amp;rsquo;ll go over a toy example from the book and tie it to the previous section on assert.&lt;/p&gt;
&lt;p&gt;First, we&amp;rsquo;ll create a &lt;strong&gt;class&lt;/strong&gt; &lt;code&gt;CountingClicker&lt;/code&gt; that initializes at count 0, has several methods including a &lt;code&gt;click&lt;/code&gt; method to increment the count, a &lt;code&gt;read&lt;/code&gt; method to read the present number of count and a &lt;code&gt;reset&lt;/code&gt; method to reset the count back to 0.&lt;/p&gt;
&lt;p&gt;Then we&amp;rsquo;ll write some &lt;code&gt;assert&lt;/code&gt; statements to test that our class method is working as intended.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll &lt;strong&gt;note&lt;/strong&gt; that there are private methods and public methods. Private methods have the &lt;strong&gt;double underscore&lt;/strong&gt; (aka dunder methods), they&amp;rsquo;re generally not called, but python won&amp;rsquo;t stop you. Then we have the more familiar &lt;em&gt;public&lt;/em&gt; methods. Also, all the methods have to be written &lt;strong&gt;within&lt;/strong&gt; the scope of the class &lt;code&gt;CountingClicker&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class CountingClicker:
    &amp;quot;&amp;quot;&amp;quot;A class can/should have a docstring, just like a function&amp;quot;&amp;quot;&amp;quot;
    def __init__(self, count = 0):
        self.count = count
    def __repr__(self):
        return f&amp;quot;CountingClicker(count = {self.count})&amp;quot;
    def click(self, num_times = 1):
        &amp;quot;&amp;quot;&amp;quot;Click the clicker some number of times.&amp;quot;&amp;quot;&amp;quot;
        self.count += num_times
    def read(self):
        return self.count
    def reset(self):
        self.count = 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After we&amp;rsquo;ve written the class and associated methods, we can write &lt;code&gt;assert&lt;/code&gt; statements to test them. You&amp;rsquo;ll want to write the below statements &lt;strong&gt;in this order&lt;/strong&gt; because we&amp;rsquo;re testing the &lt;em&gt;behavior&lt;/em&gt; of our &lt;code&gt;CountingClicker&lt;/code&gt; class.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;clicker = CountingClicker()

assert clicker.read() == 0, &amp;quot;clicker should start with count 0&amp;quot;

clicker.click()

clicker.click()

assert clicker.read() == 2, &amp;quot;after two clicks, clicker should have count of 2&amp;quot;

clicker.reset()

assert clicker.read() == 0, &amp;quot;after reset, clicker should be back to 0&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In summary, we created a class &lt;code&gt;CountingClicker&lt;/code&gt; whose methods allow it to display in text (&lt;code&gt;__repr__&lt;/code&gt;), &lt;code&gt;click&lt;/code&gt;, &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;reset&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;All these methods belong to the &lt;code&gt;class&lt;/code&gt; CountingClicker and will be passed along to new instances of classes - we have yet to see what this will look like as it relates to tasks in data science so we&amp;rsquo;ll revisit this post when we have updates on the applied end.&lt;/p&gt;
&lt;h2 id=&#34;iterables_and_generators&#34;&gt;Iterables_and_Generators&lt;/h2&gt;
&lt;h3 id=&#34;a-brief-forey-into-lazy-evaluation&#34;&gt;A Brief Forey into Lazy Evaluation&lt;/h3&gt;
&lt;p&gt;A key concept that is introduced when discussing the creation of &amp;ldquo;generators&amp;rdquo; is using &lt;code&gt;for&lt;/code&gt; and &lt;code&gt;in&lt;/code&gt; to &lt;strong&gt;iterate&lt;/strong&gt; over generators (like lists), but &lt;strong&gt;lazily on demand&lt;/strong&gt;. This is formally called 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Lazy_evaluation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lazy evaluation&lt;/a&gt; or &amp;lsquo;call-by-need&amp;rsquo; which delays the evaluation of an expression until the value is needed. We can think of this as a form of optimization - avoiding repeating function calls when not needed.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a graphic borrowed from 
&lt;a href=&#34;https://towardsdatascience.com/what-is-lazy-evaluation-in-python-9efb1d3bfed0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Xiaoxu Gao&lt;/a&gt;, check out her post 
&lt;a href=&#34;https://towardsdatascience.com/what-is-lazy-evaluation-in-python-9efb1d3bfed0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./lazy_eval.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll create some &lt;code&gt;generators&lt;/code&gt; (customized function/class), but bear in mind that it will be redundant with &lt;code&gt;range()&lt;/code&gt;, both of which illustrate lazy evaluation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Example 1: create natural_numbers() function that incrementally counts numbers
def natural_numbers():
    &amp;quot;&amp;quot;&amp;quot;returns 1, 2, 3, ...&amp;quot;&amp;quot;&amp;quot;
    n = 1
    while True:
        yield n
        n += 1

# check it&#39;s type
type(natural_numbers()) # generator

# call it, you get: &amp;lt;generator object natural_numbers at 0x7fb4d787b2e0&amp;gt;
natural_numbers()

# the point of lazy evaluation is that it won&#39;t do anything
# until you iterate over it (but avoid infinite loop with logic breaks)
for i in natural_numbers():
    print(i)
    if i == 37:
        break
print(&amp;quot;exit loop&amp;quot;)

# result 1...37 exit loop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s another example using &lt;code&gt;range&lt;/code&gt;, a built-in python function that also uses &lt;strong&gt;lazy evaluation&lt;/strong&gt;. Even when you call this &lt;code&gt;generator&lt;/code&gt;, it &lt;strong&gt;won&amp;rsquo;t do anything until you iterate over it&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;evens_below_30 = (i for i in range(30) if i % 2 == 0)

# check its type - generator
type(evens_below_30)

# call it, you get: &amp;lt;generator object &amp;lt;genexpr&amp;gt; at 0x7fb4d70ef580&amp;gt;
# calling it does nothing
evens_below_30

# now iterate over it with for and in - now it does something
# prints: 0, 2, 4, 6 ... 28
for i in evens_below_30:
    print(i)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, this section brings up another important key word &lt;strong&gt;enumerate&lt;/strong&gt; for when we want to iterate over a &lt;code&gt;generator&lt;/code&gt; or &lt;code&gt;list&lt;/code&gt; and get both &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create list of names
names = [&#39;Alice&#39;, &#39;Lebron&#39;, &#39;Kobe&#39;, &#39;Bob&#39;, &#39;Charles&#39;, &#39;Shaq&#39;, &#39;Kenny&#39;]

# Pythonic way
for i, name in enumerate(names):
    print(f&amp;quot;index: {i}, name: {name}&amp;quot;)
    
# NOT pythonic
for i in range(len(names)):
    print(f&amp;quot;index: {i}, name: {names[i]}&amp;quot;)
    
# Also NOT pythonic
i = 0
for name in names:
    print(f&amp;quot;index {i} is {names[i]}&amp;quot;)
    i += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In my view, the &lt;em&gt;pythonic way&lt;/em&gt; is much more readable here.&lt;/p&gt;
&lt;h2 id=&#34;pseudorandomness&#34;&gt;Pseudorandomness&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;random&lt;/code&gt; module is used extensively in data science. Particularly when random numbers need to be generated and we want &lt;strong&gt;reproducible&lt;/strong&gt; results the next time we run our model (in Python its &lt;code&gt;random.seed(x)&lt;/code&gt;, in R its &lt;code&gt;set.seed(x)&lt;/code&gt;), where x is any integer we decide (we just need to be consistent when we revisit our model).&lt;/p&gt;
&lt;p&gt;Technically, the module produces &lt;strong&gt;deterministic&lt;/strong&gt; results, hence it&amp;rsquo;s pseudorandom, here&amp;rsquo;s an example to highlight how the randomness is deterministic:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random
random.seed(10) # say we use 10

# this variable is from the book
four_randoms = [random.random() for _ in range(4)]

# call four_randoms - same result from Data Science from Scratch
# because the book also uses random.seed(10)
[0.5714025946899135,
 0.4288890546751146,
 0.5780913011344704,
 0.20609823213950174]

# if we use x instead of underscore
# a different set of four &amp;quot;random&amp;quot; numbers is generated
another_four_randoms = [random.random() for x in range(4)]

[0.81332125135732, 
 0.8235888725334455, 
 0.6534725339011758, 
 0.16022955651881965]

&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;brief-detour-into-_&#34;&gt;Brief detour into _&lt;/h4&gt;
&lt;p&gt;Reading around from other sources suggests that the underscore &amp;ldquo;_&amp;rdquo; is used in a for loop when we don&amp;rsquo;t care about the variable (its a throwaway) and have no plans to use it, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# prints &#39;hello&#39; five times
for _ in range(5):
    print(&amp;quot;hello&amp;quot;)
    
# we could use x as well
for x in range(5):
    print(&amp;quot;hello&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above example, either &lt;code&gt;_&lt;/code&gt; or &lt;code&gt;x&lt;/code&gt; could have been used and there doesn&amp;rsquo;t seem to be much difference. We could technically &lt;em&gt;call&lt;/em&gt; &lt;code&gt;_&lt;/code&gt;, but its considered bad practice:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# bad practice, but prints 0, 1, 2, 3, 4
for _ in range(5):
    print(_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nevertheless, &lt;code&gt;_&lt;/code&gt; matters in the context of pseudorandomness because it yields a &lt;em&gt;different&lt;/em&gt; result:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random
random.seed(10)

# these two yield different results, even with the same random.seed(10)
four_randoms = [random.random() for _ in range(4)]
another_four_randoms = [random.random() for x in range(4)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But back to determinism, or pseudorandomness, we need to &lt;em&gt;change&lt;/em&gt; the &lt;code&gt;random.seed(11)&lt;/code&gt;, then back to &lt;code&gt;random.seed(10)&lt;/code&gt; to see this play out:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# new random.seed()
random.seed(11)

# reset four_randoms
four_randoms = [random.random() for _ in range(4)]
[0.4523795535098186, 
0.559772386080496, 
0.9242105840237294, 
0.4656500700997733]

# change to previous random.seed()
random.seed(10)

# reset four_randoms (again)
four_randoms = [random.random() for _ in range(4)]

# get previous result (see above)
[0.5714025946899135,
 0.4288890546751146,
 0.5780913011344704,
 0.20609823213950174]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other features of the &lt;code&gt;random&lt;/code&gt; module include: &lt;code&gt;random.randrange&lt;/code&gt;, &lt;code&gt;random.shuffle&lt;/code&gt;, &lt;code&gt;random.choice&lt;/code&gt; and &lt;code&gt;random.sample&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;random.randrange(3,6) # choose randomly between [3,4,5]

# random shuffle
one_to_ten = [1,2,3,4,5,6,7,8,9,10]
random.shuffle(one_to_ten)
print(one_to_ten)  # example: [8, 7, 9, 3, 5, 2, 10, 1, 6, 4]
random.shuffle(one_to_ten) # again
print(one_to_ten)  # example: [3, 10, 8, 6, 9, 2, 7, 1, 4, 5]

# random choice
list_of_people = ([&amp;quot;Bush&amp;quot;, &amp;quot;Clinton&amp;quot;, &amp;quot;Obama&amp;quot;, &amp;quot;Biden&amp;quot;, &amp;quot;Trump&amp;quot;])
random.choice(list_of_people) # first time, &#39;Clinton&#39;
random.choice(list_of_people) # second time, &#39;Biden&#39;

# random sample
lottery_numbers = range(60) # get a range of 60 numbers
winning_numbers = random.sample(lottery_numbers, 6) # get a random sample of 6 numbers
winning_numbers # example: [39, 24, 2, 37, 0, 15]

# because its pseudorandom, if you want a different set of 6 numbers
# reset the winning_numbers
winning_numbers = random.sample(lottery_numbers, 6)
winning_numbers # a different set of numbers [8, 12, 19, 34, 23, 49]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;regex&#34;&gt;Regex&lt;/h2&gt;
&lt;h4 id=&#34;regular-expressions&#34;&gt;Regular Expressions&lt;/h4&gt;
&lt;p&gt;Whole books can be written about &lt;code&gt;regular expressions&lt;/code&gt; so the author briefly highlights a couple features that may come in handy, &lt;code&gt;re.match&lt;/code&gt;, &lt;code&gt;re.search&lt;/code&gt;, &lt;code&gt;re.split&lt;/code&gt; and &lt;code&gt;re.sub&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re

re_examples = [
    not re.match(&amp;quot;a&amp;quot;, &amp;quot;cat&amp;quot;),                   # re.match check the word cat &#39;starts&#39; letter &#39;a&#39;
    re.search(&amp;quot;a&amp;quot;, &amp;quot;cat&amp;quot;),                      # re.search check if word cat &#39;contains&#39; letter &#39;a&#39;
    not re.search(&amp;quot;c&amp;quot;, &amp;quot;dog&amp;quot;),                  # &#39;dog&#39; does not contain &#39;c&#39;
    3 == len(re.split(&amp;quot;[ab]&amp;quot;, &amp;quot;carbs&amp;quot;)),        # 3 equals length of &amp;quot;carbs&amp;quot; once you split out [ab]
    &amp;quot;R-D-&amp;quot; == re.sub(&amp;quot;[0-9]&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;R2D2&amp;quot;)      # sub out numbers in &#39;R2D2&#39; with hyphen &amp;quot;-&amp;quot;
    ]

# test that all examples are true
assert all(re_examples), &amp;quot;all the regex examples should be True&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final line reviews our understanding of testing (&lt;code&gt;assert&lt;/code&gt;) and truthiness (&lt;code&gt;all&lt;/code&gt;) applied to our regular expression examples, pretty neat.&lt;/p&gt;
&lt;h2 id=&#34;functional_programming&#34;&gt;Functional_Programming&lt;/h2&gt;
&lt;p&gt;see 
&lt;a href=&#34;#list_comprehensions&#34;&gt;List Comprehensions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;zip-and-argument-unpacking&#34;&gt;zip and Argument Unpacking&lt;/h2&gt;
&lt;p&gt;The author states, &amp;ldquo;it is rare that we&amp;rsquo;ll find this useful.&amp;rdquo; (p.37) So we&amp;rsquo;ll circle back if it comes up.&lt;/p&gt;
&lt;h2 id=&#34;args-and-kwargs&#34;&gt;args and kwargs&lt;/h2&gt;
&lt;p&gt;The authors states, &amp;ldquo;it is more correct and readable if you&amp;rsquo;re explicit about what sorts of arguments your functions require; accordingly, we will use &lt;strong&gt;args&lt;/strong&gt; and &lt;strong&gt;kwargs&lt;/strong&gt; only when we have no other option.&amp;rdquo; (p. 38) So we&amp;rsquo;ll circle back if it comes up.&lt;/p&gt;
&lt;h3 id=&#34;type-annotations&#34;&gt;Type Annotations&lt;/h3&gt;
&lt;h3 id=&#34;how-to-write-type-annotations&#34;&gt;How to Write Type Annotations&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Machine learning as a service</title>
      <link>/post/mlaas/</link>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/mlaas/</guid>
      <description>&lt;h2 id=&#34;preparing-api-endpoints-in-python-with-flask&#34;&gt;Preparing API endpoints in Python with Flask&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll create a minimal API endpoint that allows users to make request to calculate the area of a rectangle. The following code sets up an API endpoint locally. We&amp;rsquo;ll import &lt;code&gt;Flask&lt;/code&gt;, a lightweight web application framework and &lt;code&gt;CORS&lt;/code&gt; (cross-origin resource sharing) which allows for various HTTP requests.&lt;/p&gt;
&lt;p&gt;We have two endpoints, one basic &amp;ldquo;hello world&amp;rdquo; and the other calculate the area (i.e., width x height).&lt;/p&gt;
&lt;p&gt;This is saved in &lt;code&gt;App.py&lt;/code&gt;. The command to run this file is &lt;code&gt;$ python3 App.py&lt;/code&gt;. The last line ensures the API is running locally on &lt;code&gt;localhost:5000&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from flask import Flask, request
from flask_cors import CORS, cross_origin
import joblib
import numpy as np 

app = Flask(__name__)
CORS(app)

@app.route(&#39;/&#39;)
def helloworld():
    return &#39;Helloworld&#39;

# Example request: http://localhost:5000/area?w=50&amp;amp;h=3
@app.route(&#39;/area&#39;, methods=[&#39;GET&#39;])
@cross_origin()
def area():
    w = float(request.values[&#39;w&#39;])
    h = float(request.values[&#39;h&#39;])
    return str(w * h)

if __name__ == &#39;__main__&#39;:
    app.run(host=&#39;0.0.0.0&#39;, port=5000, debug=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can just run &lt;code&gt;localhost:5000&lt;/code&gt; and get &lt;code&gt;Helloworld&lt;/code&gt; or make a request to get the &lt;strong&gt;area&lt;/strong&gt;, for example: &lt;code&gt;http://localhost:5000/area?w=20&amp;amp;h=33&lt;/code&gt; (this yeilds 660)&lt;/p&gt;
&lt;h2 id=&#34;training-a-logistic-regression-classification-model&#34;&gt;Training a Logistic Regression classification model&lt;/h2&gt;
&lt;p&gt;After setting up some API endpoints, it&amp;rsquo;s time to create a basic machine learning model. We&amp;rsquo;ll create a logistic regression model to classify flowers from the &lt;strong&gt;Iris&lt;/strong&gt; dataset. This will be created in &lt;em&gt;one&lt;/em&gt; &lt;code&gt;jupyter notebook&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll load all required libraries.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we&amp;rsquo;ll load the Iris dataset that comes with scikit learn, &lt;code&gt;sklearn&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;iris = load_iris()

# assign two variables at once
X, y = iris[&#39;data&#39;], iris[&#39;target&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll reshape the data using &lt;code&gt;numpy&lt;/code&gt;, then split the data into training and validation sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# reshape data for logistic regression
dataset = np.hstack((X, y.reshape(-1,1)))
np.random.shuffle(dataset)

# split data into training, validation sets
X_train, X_test, y_train, y_test = train_test_split(dataset[:, :4],
                                                    dataset[:, 4],
                                                    test_size=0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll then fit a logistic regression model by fitting the training set to the validation set.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model = LogisticRegression()
model.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we&amp;rsquo;ll use the model to predict on the validation data (&lt;em&gt;note&lt;/em&gt;: in a real project a distinction is made between &lt;code&gt;validation&lt;/code&gt; and &lt;code&gt;testing&lt;/code&gt; sets, but we&amp;rsquo;ll blur that distinction for this demo). You can also test the model to make a prediction on a single observation.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s also a good idea to get the &lt;code&gt;accuracy_score()&lt;/code&gt;, although it may not be ideal for classification models.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# make a prediction
y_pred = model.predict(X_test)

# get accuracy score
accuracy_score(y_test, y_pred)

# make prediction on single Iris obervation
model.predict([[5.1, 3.5, 1.4, 0.2]])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we need to use &lt;code&gt;joblib&lt;/code&gt; to save an &lt;code&gt;iris.model&lt;/code&gt; to our directory, this will be used to connect to the API.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;joblib.dump(model, &#39;iris.model&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;creating-an-api-endpoint-for-the-logistic-regression-model&#34;&gt;Creating an API endpoint for the Logistic Regression model&lt;/h2&gt;
&lt;p&gt;Back in the &lt;code&gt;App.py&lt;/code&gt; file, we&amp;rsquo;ll &lt;em&gt;add&lt;/em&gt; this section to create an endpoint, the &lt;code&gt;predict_species()&lt;/code&gt; function that loads the &lt;code&gt;iris.model&lt;/code&gt;, then sends a Post request of the four parameter values from &lt;code&gt;iris[&#39;data&#39;]&lt;/code&gt;. The &lt;code&gt;predict_species()&lt;/code&gt; function will then return one of three flower species.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@app.route(&#39;/iris&#39;, methods=[&#39;POST&#39;])
@cross_origin()
def predict_species():
    model = joblib.load(&#39;iris.model&#39;)  #needs to be the correct path
    req = request.values[&#39;param&#39;]
    inputs = np.array(req.split(&#39;,&#39;), dtype=np.float32).reshape(1,-1)
    predict_target = model.predict(inputs)
    if predict_target == 0:
        return &#39;Setosa&#39;
    elif predict_target == 1:
        return &#39;Versicolor&#39;
    else:
        return &#39;Virginica&#39;

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;testing-the-api-endpoint-on-postman&#34;&gt;Testing the API endpoint on Postman&lt;/h2&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll use 
&lt;a href=&#34;https://www.postman.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Postman&lt;/a&gt;, a platform for API development. We will &lt;strong&gt;post&lt;/strong&gt; four parameters (i.e., sepal length, sepal width, petal length and petal width) to the API endpoint and expect to receive a name back, either Setosa, Versicolor or Virginica.
In Postman, we&amp;rsquo;ll create a new collection and a new request:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./postman.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The next step from here is to go beyond localhost and deploy the model. We&amp;rsquo;ll explore that in another post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Essential Readings in Data Science</title>
      <link>/post/data_science_canon/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/data_science_canon/</guid>
      <description>&lt;h2 id=&#34;data-science-literature-review&#34;&gt;Data Science Literature Review&lt;/h2&gt;
&lt;p&gt;I saw an 
&lt;a href=&#34;https://twitter.com/KellyBodwin/status/1303083136046170112?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intriguing question&lt;/a&gt; posed on Twitter and some of the responses were illuminating.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./reading_list.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s 
&lt;a href=&#34;https://twitter.com/beeonaposy/status/1191798851289649154?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;another variant&lt;/a&gt; of the question:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./ds_canon.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Although Data Science has a long history, it&amp;rsquo;s considered a relatively young field.&lt;/p&gt;
&lt;p&gt;This space will be used to document recommended reading for new entrants:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Downey, Allen (2016) There is only one test. 
&lt;a href=&#34;http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wickham, Hadley (2014) Tidy Data. The Journal of Statistical Software, vol 59. 
&lt;a href=&#34;https://vita.had.co.nz/papers/tidy-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original&lt;/a&gt;, 
&lt;a href=&#34;https://tidyr.tidyverse.org/articles/tidy-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;update&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;James, G., Witten, D., Hastie, T. &amp;amp; Tibshirani, R. (2014) An Introduction to Statistical Learning with Applications in R. 
&lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shmueli, G. (2010) To explain or to predict? Statistical Science, 25(3), 289-310. 
&lt;a href=&#34;https://projecteuclid.org/euclid.ss/1294167961&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hernan, M.A., Hsu, J. &amp;amp; Healy, B. (2019) A second chance to get causal inference right: A classification of Data Science tasks. Chance, vol 32(1). 
&lt;a href=&#34;https://amstat.tandfonline.com/doi/full/10.1080/09332480.2019.1579578&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gelman, A., Pasarica, C. &amp;amp; Dodhia, R. (2002) Let&amp;rsquo;s practice what we preach: Turning tables into graphs. The American Statistician, vol 56(2). 
&lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/000313002317572790&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scott Formann-Roe (June, 2012) Understanding the Bias-Variance Tradeoff. 
&lt;a href=&#34;http://scott.fortmann-roe.com/docs/BiasVariance.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Donoho, D (2017) 50 Years of Data Science. Journal of Computational and Graphical Statistics, vol 26(4). 
&lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1384734&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wilson, G., Bryan, J., Cranston, K., Kitzes, J., Nederbragt, L. &amp;amp; Teal, T.K. (2017) Good enough practices in scientific computing. Plos Computational Biology. 
&lt;a href=&#34;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kevin Markham (2019) 100 pandas tricks to save you time and energy. 
&lt;a href=&#34;https://www.dataschool.io/python-pandas-tips-and-tricks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chris Albon&amp;rsquo;s code snippets. 
&lt;a href=&#34;https://chrisalbon.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Howard, J. &amp;amp; Gugger, S. (Aug 4, 2020) Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD 1st Ed. 
&lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Brandon Rohrer (Jan, 2020) End-to-End Machine Learning: Complete Course Catalog. 
&lt;a href=&#34;https://end-to-end-machine-learning.teachable.com/p/complete-course-library-full-end-to-end-machine-learning-catalog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;; 
&lt;a href=&#34;https://e2eml.school/blog.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;second source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;John Rauser (Dec, 2016) How Humans See Data 
&lt;a href=&#34;https://www.youtube.com/watch?v=fSgEeI2Xpdc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Broman, K.W. &amp;amp; Woo, K.H. (2018) Data Organization in Spreadsheets. The American Statistician, vol 72(1). 
&lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaudhary, V., &amp;amp; Young, M. (2014) Machine Learning: The High Interest Credit Card of Technical Debt. 
&lt;a href=&#34;https://research.google/pubs/pub43146/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3Blue1Brown for Linear Algebra 
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jenny Bryan. Stat 545: Data Wrangling, Exploration and Analysis with R. 
&lt;a href=&#34;https://stat545.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>A collection of weird pretty plots</title>
      <link>/post/ggplot_art/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/ggplot_art/</guid>
      <description>&lt;h2 id=&#34;tidytuesday-2020-08-18-week-34&#34;&gt;TidyTuesday 2020-08-18 (week 34)&lt;/h2&gt;
&lt;p&gt;In the process of exploring dendrograms, I create jheri curls :)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./plant_dendogram_mess.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another plot with less hair:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./dendrogram_plant.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;I call this Disco Fire:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./disco_fire.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tidytuesday-2020-10-13-week-42&#34;&gt;TidyTuesday 2020-10-13 (week 42)&lt;/h2&gt;
&lt;p&gt;Dino-turn-Rorschach test&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./density_dino.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tidytuesday-2020-12-08-week-50&#34;&gt;TidyTuesday 2020-12-08 (week 50)&lt;/h2&gt;
&lt;p&gt;Sunburst (BBC&amp;rsquo;s 100 Influential Women 2020)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./women_bbc_sunburst.png&#34; alt=&#34;women_bbc_sunburst&#34;&gt;&lt;/p&gt;
&lt;p&gt;Circular Packing (experimental)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./circular.png&#34; alt=&#34;circular&#34;&gt;&lt;/p&gt;
&lt;p&gt;Oops, I did it again:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./oops.png&#34; alt=&#34;oops&#34;&gt;&lt;/p&gt;
&lt;p&gt;This one is inspired by 
&lt;a href=&#34;https://twitter.com/geokaramanis/status/1345811125678596096?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georgios Karamanis&#39;&lt;/a&gt; #genuary submission using the {ggridges} package.&lt;/p&gt;
&lt;p&gt;This is my daughter Milin, still lovely to me :)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./milin_fill_y.png&#34; alt=&#34;milin_fill_y&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Visualization Competition 2020 - Wharton People Analytics Conf</title>
      <link>/slides/wpa/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/slides/wpa/</guid>
      <description>
&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_1.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_3.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_4.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_5.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_6.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_8.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_13.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_15.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_17.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_19.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_20.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_21.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_22.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_23.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_25.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_28.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_30.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_34.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_36.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_37.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_38.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_39.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_40.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_41.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_43.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_46.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_48.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_50.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_51.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_53.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_54.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_55.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_56.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_58.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_59.jpeg&#34;
  &gt;

&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;./wpa_slides/wpa_final_slides_64.jpeg&#34;
  &gt;

&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/jupyter/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install-python-and-jupyterlab&#34;&gt;Install Python and JupyterLab&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and JupyterLab.&lt;/p&gt;
&lt;p&gt;Alternatively, install JupyterLab with &lt;code&gt;pip3 install jupyterlab&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;
&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code&gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
cd &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
jupyter lab index.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;jupyter&lt;/code&gt; command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p&gt;
&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;
&lt;p&gt;The first cell of your Jupter notebook will contain your post metadata (
&lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;front matter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In Jupter, choose &lt;em&gt;Markdown&lt;/em&gt; as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: My post&#39;s title
date: 2019-09-01

# Put any other Academic metadata here...
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Edit the metadata of your post, using the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; as a guide to the available options.&lt;/p&gt;
&lt;p&gt;To set a 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;
&lt;p&gt;For other tips, such as using math, see the guide on 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;This post was created with Jupyter. The orginal files can be found at &lt;a href=&#34;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&#34;&gt;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic: the website builder for Hugo</title>
      <link>/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/post/getting-started/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 &lt;em&gt;widgets&lt;/em&gt;, &lt;em&gt;themes&lt;/em&gt;, and &lt;em&gt;language packs&lt;/em&gt; included!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or 
&lt;a href=&#34;https://sourcethemes.com/academic/#expo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ð 
&lt;a href=&#34;#install&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View the &lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¬ 
&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Ask a question&lt;/strong&gt; on the forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¥ 
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¦ Twitter: 
&lt;a href=&#34;https://twitter.com/source_themes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@source_themes&lt;/a&gt; 
&lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; 
&lt;a href=&#34;https://twitter.com/search?q=%23MadeWithAcademic&amp;amp;src=typd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithAcademic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¡ 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;â¬ï¸ &lt;strong&gt;Updating?&lt;/strong&gt; View the 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Guide&lt;/a&gt; and 
&lt;a href=&#34;https://sourcethemes.com/academic/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;â¤ï¸ &lt;strong&gt;Support development&lt;/strong&gt; of Academic:
&lt;ul&gt;
&lt;li&gt;âï¸ 
&lt;a href=&#34;https://paypal.me/cushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Donate a coffee&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðµ 
&lt;a href=&#34;https://www.patreon.com/cushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Become a backer on &lt;strong&gt;Patreon&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð¼ï¸ 
&lt;a href=&#34;https://www.redbubble.com/people/neutreno/works/34387919-academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Decorate your laptop or journal with an Academic &lt;strong&gt;sticker&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð 
&lt;a href=&#34;https://academic.threadless.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wear the &lt;strong&gt;T-shirt&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ð©âð» 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/contribute/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Contribute&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;















&lt;figure id=&#34;figure-academic-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; data-caption=&#34;Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34;&gt;


  &lt;img src=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable 
&lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and 
&lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - 
&lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, 
&lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 15+ language packs including English, ä¸­æ, and PortuguÃªs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Academic comes with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can  choose their preferred mode - click the sun/moon icon in the top right of the 
&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/customization/#custom-theme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;customizable&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;
&lt;a href=&#34;https://github.com/sourcethemes/academic-admin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Admin&lt;/a&gt;:&lt;/strong&gt; An admin tool to import publications from BibTeX or import assets for an offline site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;
&lt;a href=&#34;https://github.com/sourcethemes/academic-scripts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Scripts&lt;/a&gt;:&lt;/strong&gt; Scripts to help migrate content to new versions of Academic&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;p&gt;You can choose from one of the following four methods to install:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-web-browser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;one-click install using your web browser (recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-git&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer using &lt;strong&gt;Git&lt;/strong&gt; with the Command Prompt/Terminal app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer by downloading the &lt;strong&gt;ZIP files&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;install on your computer with &lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;personalize and deploy your new site&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;updating&#34;&gt;Updating&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View the Update Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feel free to &lt;em&gt;star&lt;/em&gt; the project on 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; to help keep track of 
&lt;a href=&#34;https://sourcethemes.com/academic/updates&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;updates&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present 
&lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
