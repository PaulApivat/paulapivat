<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Paul Apivat Hanvongse">

  
  
  
    
  
  <meta name="description" content="An overview of statistical Hypothesis Testing, Estimation and Bayesian Inference">

  
  <link rel="alternate" hreflang="en-us" href="/post/dsfs_7/">

  


  
  
  
  <meta name="theme-color" content="#E32626">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Domine:wght@400;700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/dsfs_7/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@paulapivat">
  <meta property="twitter:creator" content="@paulapivat">
  
  <meta property="og:site_name" content="Paul Apivat">
  <meta property="og:url" content="/post/dsfs_7/">
  <meta property="og:title" content="Data Science from Scratch (ch7) - Hypothesis and Inference | Paul Apivat">
  <meta property="og:description" content="An overview of statistical Hypothesis Testing, Estimation and Bayesian Inference"><meta property="og:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-12-15T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-12-15T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/dsfs_7/"
  },
  "headline": "Data Science from Scratch (ch7) - Hypothesis and Inference",
  
  "datePublished": "2020-12-15T00:00:00Z",
  "dateModified": "2020-12-15T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Paul Apivat Hanvongse"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Paul Apivat",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "An overview of statistical Hypothesis Testing, Estimation and Bayesian Inference"
}
</script>

  

  


  


  





  <title>Data Science from Scratch (ch7) - Hypothesis and Inference | Paul Apivat</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Paul Apivat</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Paul Apivat</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/#"><span>Start Here</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#accomplishments"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#gallery"><span>Viz</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/technical_notes/"><span>Technical Notes</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Data Science from Scratch (ch7) - Hypothesis and Inference</h1>

  
  <p class="page-subtitle">Connecting probability and statistics to hypothesis testing and inference</p>
  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span><a href="/author/paul-apivat-hanvongse/">Paul Apivat Hanvongse</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Dec 15, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    15 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h3 id="table-of-contents">Table of contents</h3>
<ul>
<li>
<a href="#central_limit_theorem">Central Limit Theorem</a></li>
<li>
<a href="#hypothesis_testing">Hypothesis Testing</a></li>
<li>
<a href="#p_values">p-Values</a></li>
<li>
<a href="#confidence_intervals">Confidence Intervals</a></li>
<li>
<a href="#connecting_dots">Connecting dots with Python</a></li>
</ul>
<h2 id="overview">Overview</h2>
<p>This is a continuation of my progress through Data Science from Scratch by Joel Grus. We&rsquo;ll use a classic coin-flipping example in this post because it is simple to illustrate with both <strong>concept</strong> and <strong>code</strong>. The goal of this post is to connect the dots between several concepts including the Central Limit Theorem, Hypothesis Testing, p-Values and confidence intervals, using python to build our intuition.</p>
<h2 id="central_limit_theorem">Central_Limit_Theorem</h2>
<p>Terms like &ldquo;null&rdquo; and &ldquo;alternative&rdquo; hypothesis are used quite frequently, so let&rsquo;s set some context. The &ldquo;null&rdquo; is the <strong>default</strong> position. The &ldquo;alternative&rdquo;, alt for short, is something we&rsquo;re <em>comparing to</em> the default (null).</p>
<p>The classic coin-flipping exercise is to test the <em>fairness</em> off a coin. If a coin is fair, it&rsquo;ll land on heads 50% of the time (and tails 50% of the time). Let&rsquo;s translate into hypothesis testing language:</p>
<p><strong>Null Hypothesis</strong>: Probability of landing on Heads = 0.5.</p>
<p><strong>Alt Hypothesis</strong>: Probability of landing on Heads != 0.5.</p>
<p>Each coin flip is a <strong>Bernoulli trial</strong>, which is an experiment with two outcomes - outcome 1, &ldquo;success&rdquo;, (probability <em>p</em>) and outcome 0, &ldquo;fail&rdquo; (probability <em>p - 1</em>). The reason it&rsquo;s a Bernoulli trial is because there are only two outcome with a coin flip (heads or tails). Read more about 
<a href="https://en.wikipedia.org/wiki/Bernoulli_trial" target="_blank" rel="noopener">Bernoulli here</a>.</p>
<p>Here&rsquo;s the code for a single Bernoulli Trial:</p>
<pre><code class="language-python">def bernoulli_trial(p: float) -&gt; int:
    &quot;&quot;&quot;Returns 1 with probability p and 0 with probability 1-p&quot;&quot;&quot;
    return 1 if random.random() &lt; p else 0
</code></pre>
<p>When you <strong>sum the independent Bernoulli trials</strong>, you get a <strong>Binomial(n,p)</strong> random variable, a variable whose <em>possible</em> values have a probability distribution. The <strong>central limit theorem</strong> says as <strong>n</strong> or the <em>number</em> of independent Bernoulli trials get large, the Binomial distribution approaches a normal distribution.</p>
<p>Here&rsquo;s the code for when you sum all the Bernoulli Trials to get a Binomial random variable:</p>
<pre><code class="language-python">def binomial(n: int, p: float) -&gt; int:
    &quot;&quot;&quot;Returns the sum of n bernoulli(p) trials&quot;&quot;&quot;
    return sum(bernoulli_trial(p) for _ in range(n))
</code></pre>
<p><strong>Note</strong>: A single &lsquo;success&rsquo; in a Bernoulli trial is &lsquo;x&rsquo;. Summing up all those x&rsquo;s into X, is a Binomial random variable. Success doesn&rsquo;t imply desirability, nor does &ldquo;failure&rdquo; imply undesirability. They&rsquo;re just terms to count the cases we&rsquo;re looking for (i.e., number of heads in multiple coin flips to assess a coin&rsquo;s fairness).</p>
<p>Given that our <strong>null</strong> is (p = 0.5) and <strong>alt</strong> is (p != 0.5), we can run some independent bernoulli trials, then sum them up to get a binomial random variable.</p>
<p><img src="./independent_coin_flips.png" alt="independent_coin_flips"></p>
<p>Each <code>bernoulli_trial</code> is an experiment with either 0 or 1 as outcomes. The <code>binomial</code> function sums up <strong>n</strong> bernoulli(0.5) trails. We ran both twice and got different results. Each bernoulli experiment can be a success(1) or faill(0); summing up into a binomial random variable means we&rsquo;re taking the probability p(0.5) <em>that a coin flips head</em> and we ran the experiment 1,000 times to get a random binomial variable.</p>
<p>The first 1,000 flips we got 510. The second 1,000 flips we got 495. We can repeat this process many times to get a <em>distribution</em>. We can plot this distribution to reinforce our understanding. To this we&rsquo;ll use <code>binomial_histogram</code> function. This function picks points from a Binomial(n,p) random variable and plots their histogram.</p>
<pre><code class="language-python">from collections import Counter
import matplotlib.pyplot as plt

def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float:
    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2
    

def binomial_histogram(p: float, n: int, num_points: int) -&gt; None:
    &quot;&quot;&quot;Picks points from a Binomial(n, p) and plots their histogram&quot;&quot;&quot;
    data = [binomial(n, p) for _ in range(num_points)]
    # use a bar chart to show the actual binomial samples
    histogram = Counter(data)
    plt.bar([x - 0.4 for x in histogram.keys()],
            [v / num_points for v in histogram.values()],
            0.8,
            color='0.75')
    mu = p * n
    sigma = math.sqrt(n * p * (1 - p))
    # use a line chart to show the normal approximation
    xs = range(min(data), max(data) + 1)
    ys = [normal_cdf(i + 0.5, mu, sigma) -
          normal_cdf(i - 0.5, mu, sigma) for i in xs]
    plt.plot(xs, ys)
    plt.title(&quot;Binomial Distribution vs. Normal Approximation&quot;)
    plt.show()

# call function   
binomial_histogram(0.5, 1000, 10000)
</code></pre>
<p>This plot is then rendered:</p>
<p><img src="./binomial_coin_fairness.png" alt="binomial_coin_fairness"></p>
<p>What we did was sum up independent <code>bernoulli_trial</code>(s) of 1,000 coin flips, where the probability of head is p = 0.5, to create a <code>binomial</code> random variable. We then repeated this a large number of times (N = 10,000), then plotted a histogram of the distribution of all binomial random variables. And because we did it so many times, it approximates the standard normal distribution (smooth bell shape curve).</p>
<p>Just to demonstrate how this works, we can generate several <code>binomial</code> random variables:</p>
<p><img src="./several_binomial.png" alt="several_binomial"></p>
<p>If we do this 10,000 times, we&rsquo;ll generate the above histogram. You&rsquo;ll notice that because we are testing whether the coin is fair, the probability of heads (success) <em>should</em> be at 0.5 and, from 1,000 coin flips, the <strong>mean</strong>(<code>mu</code>) should be a 500.</p>
<p>We have another function that can help us calculate <code>normal_approximation_to_binomial</code>:</p>
<pre><code class="language-python">import random
from typing import Tuple
import math


def normal_approximation_to_binomial(n: int, p: float) -&gt; Tuple[float, float]:
    &quot;&quot;&quot;Return mu and sigma corresponding to a Binomial(n, p)&quot;&quot;&quot;
    mu = p * n
    sigma = math.sqrt(p * (1 - p) * n)
    return mu, sigma
    
# call function
# (500.0, 15.811388300841896)
normal_approximation_to_binomial(1000, 0.5)
</code></pre>
<p>When calling the function with our parameters, we get a mean <code>mu</code> of 500 (from 1,000 coin flips) and a standard deviation <code>sigma</code> of 15.8114. Which means that 68% of the time, the binomial random variable will be 500 +/- 15.8114 and 95% of the time it&rsquo;ll be 500 +/- 31.6228 (see 
<a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule" target="_blank" rel="noopener">68-95-99.7 rule</a>)</p>
<h2 id="hypothesis_testing">Hypothesis_Testing</h2>
<p>Now that we have seen the results of our &ldquo;coin fairness&rdquo; experiment plotted on a binomial distribution (approximately normal), we will be, for the purpose of testing our hypothesis, be interested in the probability of its realized value (binomial random variable) lies <strong>within or outside a particular interval</strong>.</p>
<p>This means we&rsquo;ll be interested in questions like:</p>
<ul>
<li>What&rsquo;s the probability that the binomial(n,p) is below a threshold?</li>
<li>Above a threshold?</li>
<li>Between an interval?</li>
<li>Outside an interval?</li>
</ul>
<p>First, the <code>normal_cdf</code> (normal cummulative distribution function), which we learned in a 
<a href="https://paulapivat.com/post/dsfs_6/#distributions" target="_blank" rel="noopener">previous post</a>, <em>is</em> the probability of a variable being <em>below</em> a certain threshold.</p>
<p>Here, the probability of X (success or heads for a &lsquo;fair coin&rsquo;) is at 0.5 (<code>mu</code> = 500, <code>sigma</code> = 15.8113), and we want to find the probability that X falls below 490, which comes out to roughly 26%</p>
<pre><code class="language-python">normal_probability_below = normal_cdf

# probability that binomal random variable, mu = 500, sigma = 15.8113, is below 490

# 0.26354347477247553
normal_probability_below(490, 500, 15.8113)
</code></pre>
<p>On the other hand, the <code>normal_probability_above</code>, probability that X falls <em>above</em> 490 would be
1 - 0.2635 = 0.7365 or roughly 74%.</p>
<pre><code class="language-python">def normal_probability_above(lo: float,
                             mu: float = 0,
                             sigma: float = 1) -&gt; float:
    &quot;&quot;&quot;The probability that an N(mu, sigma) is greater than lo.&quot;&quot;&quot;
    return 1 - normal_cdf(lo, mu, sigma)
    
# 0.7364565252275245
normal_probability_above(490, 500, 15.8113)
</code></pre>
<p>To make sense of this we need to recall the binomal distribution, that approximates the normal distribution, but we&rsquo;ll draw a vertical line at 490.</p>
<p><img src="./binomial_vline.png" alt="binomial_vline"></p>
<p>We&rsquo;re asking, given the binomal distribution with <code>mu</code> 500 and <code>sigma</code> at 15.8113, what is the probability that a binomal random variable falls below the threshold (left of the line); the answer is approximately 26% and correspondingly falling above the threshold (right of the line), is approximately 74%.</p>
<h3 id="between-interval">Between interval</h3>
<p>We may also wonder what the probability of a binomial random variable <strong>falling between 490 and 520</strong>:</p>
<p><img src="./binomial_2_vline.png" alt="binomial_2_vline"></p>
<p>Here is the function to calculate this probability and it comes out to approximately 63%. <em>note</em>: Bear in mind the full area under the curve is 1.0 or 100%.</p>
<pre><code class="language-python">def normal_probability_between(lo: float,
                               hi: float,
                               mu: float = 0,
                               sigma: float = 1) -&gt; float:
    &quot;&quot;&quot;The probability that an N(mu, sigma) is between lo and hi.&quot;&quot;&quot;
    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)

# 0.6335061861416337
normal_probability_between(490, 520, 500, 15.8113)
</code></pre>
<p>Finally, the area outside of the interval should be 1 - 0.6335 = 0.3665:</p>
<pre><code class="language-python">def normal_probability_outside(lo: float,
                               hi: float,
                               mu: float = 0,
                               sigma: float = 1) -&gt; float:
    &quot;&quot;&quot;The probability that an N(mu, sigma) is not between lo and hi.&quot;&quot;&quot;
    return 1 - normal_probability_between(lo, hi, mu, sigma)
    
# 0.3664938138583663
normal_probability_outside(490, 520, 500, 15.8113)
</code></pre>
<p>In addition to the above, we may also be interested in finding (symmetric) intervals around the mean that account for a <em>certain level of likelihood</em>, for example, 60% probability centered around the mean.</p>
<p>For this operation we would use the <code>inverse_normal_cdf</code>:</p>
<pre><code class="language-python">def inverse_normal_cdf(p: float,
                       mu: float = 0,
                       sigma: float = 1,
                       tolerance: float = 0.00001) -&gt; float:
    &quot;&quot;&quot;Find approximate inverse using binary search&quot;&quot;&quot;
    # if not standard, compute standard and rescale
    if mu != 0 or sigma != 1:
        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)
    low_z = -10.0     # normal_cdf(-10) is (very close to) 0
    hi_z = 10.0       # normal_cdf(10) is (very close to) 1
    while hi_z - low_z &gt; tolerance:
        mid_z = (low_z + hi_z) / 2      # Consider the midpoint
        mid_p = normal_cdf(mid_z)       # and the CDF's value there
        if mid_p &lt; p:
            low_z = mid_z               # Midpoint too low, search above it
        else:
            hi_z = mid_z                # Midpoint too high, search below it
    return mid_z
</code></pre>
<p>First we&rsquo;d have to find the cutoffs where the upper and lower tails each contain 20% of the probability. We calculate <code>normal_upper_bound</code> and <code>normal_lower_bound</code> and use those to calculate the <code>normal_two_sided_bounds</code>.</p>
<pre><code class="language-python">def normal_upper_bound(probability: float,
                       mu: float = 0,
                       sigma: float = 1) -&gt; float:
    &quot;&quot;&quot;Returns the z for which P(Z &lt;= z) = probability&quot;&quot;&quot;
    return inverse_normal_cdf(probability, mu, sigma)


def normal_lower_bound(probability: float,
                       mu: float = 0,
                       sigma: float = 1) -&gt; float:
    &quot;&quot;&quot;Returns the z for which P(Z &gt;= z) = probability&quot;&quot;&quot;
    return inverse_normal_cdf(1 - probability, mu, sigma)


def normal_two_sided_bounds(probability: float,
                            mu: float = 0,
                            sigma: float = 1) -&gt; Tuple[float, float]:
    &quot;&quot;&quot;
    Returns the symmetric (about the mean) bounds
    that contain the specified probability
    &quot;&quot;&quot;
    tail_probability = (1 - probability) / 2
    # upper bound should have tail_probability above it
    upper_bound = normal_lower_bound(tail_probability, mu, sigma)
    # lower bound should have tail_probability below it
    lower_bound = normal_upper_bound(tail_probability, mu, sigma)
    return lower_bound, upper_bound
</code></pre>
<p>So if we wanted to know what the cutoff points were for a 60% probability around the mean and standard deviation (<code>mu</code> = 500, <code>sigma</code> = 15.8113), it would be between <strong>486.69 and 513.31</strong>.</p>
<p>Said differently, this means roughly 60% of the time, we can expect the binomial random variable to fall between 486 and 513.</p>
<pre><code class="language-python"># (486.6927811021805, 513.3072188978196)
normal_two_sided_bounds(0.60, 500, 15.8113)
</code></pre>
<h3 id="significance-and-power">Significance and Power</h3>
<p>Now that we have a handle on the binomial normal distribution, thresholds (left and right of the mean), and cut-off points, we want to make a <strong>decision about significance</strong>. Probably the most important part of <em>statistical significance</em> is that it is a decision to be made, not a standard that is externally set.</p>
<p>Significance is a decision about how willing we are to make a <em>type 1</em> error (false positive), which we explored in a 
<a href="https://paulapivat.com/post/dsfs_6/#applying_bayes_theorem" target="_blank" rel="noopener">previous post</a>. The convention is to set it to a 5% or 1% willingness to make a type 1 error. Suppose we say 5%.</p>
<p>We would say that out of 1,000 coin flips, 95% of the time, we&rsquo;d get between 469 and 531 heads on a &ldquo;fair coin&rdquo; and 5% of the time, outside of this 469-531 range.</p>
<pre><code class="language-python"># (469.0104394712448, 530.9895605287552)
normal_two_sided_bounds(0.95, 500, 15.8113)
</code></pre>
<p>If we recall our hypotheses:</p>
<p><strong>Null Hypothesis</strong>: Probability of landing on Heads = 0.5 (fair coin)</p>
<p><strong>Alt Hypothesis</strong>: Probability of landing on Heads != 0.5 (biased coin)</p>
<p>Each binomial distribution (test) that consist of 1,000 bernoulli trials, each <em>test</em> where the number of heads falls outside the range of 469-531, we&rsquo;ll <strong>reject the null</strong> that the coin is fair. And we&rsquo;ll be wrong (false positive), 5% of the time. It&rsquo;s a false positive when we <strong>incorrectly reject</strong> the null hypothesis, when it&rsquo;s actually true.</p>
<p>We also want to avoid making a type-2 error (false negative), where we <strong>fail to reject</strong> the null hypothesis, when it&rsquo;s actually false.</p>
<p><strong>Note</strong>: Its important to keep in mind that terms like <em>significance</em> and <em>power</em> are used to describe <strong>tests</strong>, in our case, the test of whether a coin is fair or not. Each test is the sum of 1,000 independent bernoulli trials.</p>
<p>For a &ldquo;test&rdquo; that has a 95% significance, we&rsquo;ll assume that out of a 1,000 coin flips, it&rsquo;ll land on heads between 469-531 times and we&rsquo;ll determine the coin is fair. For the 5% of the time it lands outside of this range, we&rsquo;ll determine the coin to be &ldquo;unfair&rdquo;, but we&rsquo;ll be wrong because it actually is fair.</p>
<p>To calculate the power of the test, we&rsquo;ll take the assumed <code>mu</code> and <code>sigma</code> with a 95% bounds (based on the assumption that the probability of the coin landing on heads is 0.5 or 50% - a fair coin). We&rsquo;ll determine the lower and upper bounds:</p>
<pre><code class="language-python">lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)
lo # 469.01026640487555
hi # 530.9897335951244
</code></pre>
<p>And if the coin was <em>actually biased</em>, we should reject the null, but we fail to. Let&rsquo;s suppose the actual probability that the coin lands on heads is 55% ( <strong>biased</strong> towards head):</p>
<pre><code class="language-python">mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)
mu_1    # 550.0
sigma_1 # 15.732132722552274
</code></pre>
<p>Using the same range 469 - 531, where the coin is assumed &lsquo;fair&rsquo; with <code>mu</code> at 500 and <code>sigma</code> at 15.8113:</p>
<p><img src="./95sig_binomial.png" alt="95sig_binomial"></p>
<p>If the coin, in fact, had a bias towards head (p = 0.55), the distribution would shift right, but if our 95% significance test remains the same, we get:</p>
<p><img src="./type2_error.png" alt="type2_error"></p>
<p>The probability of making a type-2 error is 11.345%. This is the probability that we&rsquo;re see that the coin&rsquo;s distribution is within the previous interval 469-531, thinking we should accept the null hypothesis (that the coin is fair), but in actuality, failing to see that the distribution has shifted to the coin having a <em>bias</em> towards heads.</p>
<pre><code class="language-python"># 0.11345199870463285
type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)
</code></pre>
<p>The other way to arrive at this is to find the probability, under the <em>new</em> <code>mu</code> and <code>sigma</code> (new distribution), that X (number of successes) will fall <em>below</em> 531.</p>
<pre><code class="language-python"># 0.11357762975476304
normal_probability_below(531, mu_1, sigma_1)
</code></pre>
<p>So the probability of making a type-2 error or the probability that the <em>new</em> distribution falls below 531 is approximately 11.3%.</p>
<p>The <strong>power to detect</strong> a type-2 error is 1.00 minus the probability of a type-2 error (1 - 0.113 = 0.887), or 88.7%.</p>
<pre><code class="language-python">power = 1 - type_2_probability # 0.8865480012953671
</code></pre>
<p>Finally, we may be interested in <strong>increasing power</strong> to detect a type-2 error. Instead of using a <code>normal_two_sided_bounds</code> function to find the cut-off points (i.e., 469 and 531), we could use a <em>one-sided test</em> that rejects the null hypothesis (&lsquo;fair coin&rsquo;) when X (number of heads on a coin-flip) is much larger than 500.</p>
<p>Here&rsquo;s the code, using <code>normal_upper_bound</code>:</p>
<pre><code class="language-python"># 526.0073585242053
hi = normal_upper_bound(0.95, mu_0, sigma_0)
</code></pre>
<p>This means shifting the upper bounds from 531 to 526, providing more probability in the upper tail. This means the probability of a type-2 error goes down from 11.3 to 6.3.</p>
<p><img src="./increase_power.png" alt="increase_power"></p>
<pre><code class="language-python"># previous probability of type-2 error
# 0.11357762975476304
normal_probability_below(531, mu_1, sigma_1)


# new probability of type-2 error
# 0.06356221447122662
normal_probability_below(526, mu_1, sigma_1)
</code></pre>
<p>And the new (stronger) <strong>power to detect</strong> type-2 error is 1.0 - 0.064 = 0.936 or 93.6% (up from 88.7% above).</p>
<h2 id="p_values">p_values</h2>
<p>p-Values represent <em>another way</em> of deciding whether to accept or reject the Null Hypothesis. Instead of choosing bounds, thresholds or cut-off points, we could compute the probability, assuming the Null Hypothesis is true, that we would see a value <em>as extreme as</em> the one we just observed.</p>
<p>Here is the code:</p>
<pre><code class="language-python">def two_sided_p_values(x: float, mu: float = 0, sigma: float = 1) -&gt; float:
    &quot;&quot;&quot;
    How likely are we to see a value at least as extreme as x (in either
    direction) if our values are from an N(mu, sigma)?
    &quot;&quot;&quot;
    if x &gt;= mu:
        # x is greater than the mean, so the tail is everything greater than x
        return 2 * normal_probability_above(x, mu, sigma)
    else:
        # x is less than the mean, so the tail is everything less than x
        return 2 * normal_probability_below(x, mu, sigma)
</code></pre>
<p>If we wanted to compute, assuming we have a &ldquo;fair coin&rdquo; (<code>mu</code> = 500, <code>sigma</code> = 15.8113), what is the probability of seeing a value like 530? (<strong>note</strong>: We use 529.5 instead of 530 below due to 
<a href="https://en.wikipedia.org/wiki/Continuity_correction" target="_blank" rel="noopener">continuity correction</a>)</p>
<p>Answer: approximately 6.2%</p>
<pre><code class="language-python"># 0.06207721579598835
two_sided_p_values(529.5, mu_0, sigma_0)
</code></pre>
<p>The p-value, 6.2% is higher than our (hypothetical) 5% significance, so we don&rsquo;t reject the null. On the other hand, if X was slightly more extreme, 532, the probability of seeing that value would be approximately 4.3%, which is less than 5% significance, so we would reject the null.</p>
<pre><code class="language-python"># 0.04298479507085862
two_sided_p_values(532, mu_0, sigma_0)
</code></pre>
<p>For one-sided tests, we would use the <code>normal_probability_above</code> and <code>normal_probability_below</code> functions created above:</p>
<pre><code class="language-python">upper_p_value = normal_probability_above
lower_p_value = normal_probability_below
</code></pre>
<p>Under the <code>two_sided_p_values</code> test, the extreme value of 529.5 had a probability of 6.2% of showing up, but not low enough to reject the null hypothesis.</p>
<p>However, with a one-sided test, <code>upper_p_value</code> for the same threshold is now 3.1% and we would reject the null hypothesis.</p>
<pre><code class="language-python"># 0.031038607897994175
upper_p_value(529.5, mu_0, sigma_0)
</code></pre>
<h2 id="confidence_intervals">Confidence_Intervals</h2>
<p>A <em>third</em> approach to deciding whether to accept or reject the null is to use confidence intervals. We&rsquo;ll use the 530 as we did in the p-Values example.</p>
<pre><code class="language-python">p_hat = 530/1000
mu = p_hat
sigma = math.sqrt(p_hat * (1 - p_hat) / 1000) # 0.015782902141241326

# (0.4990660982192851, 0.560933901780715)
normal_two_sided_bounds(0.95, mu, sigma)
</code></pre>
<p>The confidence interval for a coin flipping heads 530 (out 1,000) times is (0.4991, 0.5609). Since this interval <strong>contains</strong> the p = 0.5 (probability of heads 50% of the time, assuming a fair coin), we do not reject the null.</p>
<p>If the extreme value were <em>more</em> extreme at 540, we would arrive at a different conclusion:</p>
<pre><code class="language-python">p_hat = 540/1000
mu = p_hat
sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)

(0.5091095927295919, 0.5708904072704082)
normal_two_sided_bounds(0.95, mu, sigma)
</code></pre>
<p>Here we would be 95% confident that the mean of this distribution is contained between 0.5091 and 0.5709 and this <strong>does not</strong> contain 0.500 (albiet by a slim margin), so we reject the null hypothesis that this is a fair coin.</p>
<p><strong>note</strong>: Confidence intervals are about the <em>interval</em> not probability p. We interpret the confidence interval as, if you were to repeat the experiment many times, 95% of the time, the &ldquo;true&rdquo; parameter, in our example p = 0.5, would lie within the observed confidence interval.</p>
<h2 id="connecting_dots">Connecting_Dots</h2>
<p>We used several python functions to build intuition around statistical hypothesis testing. To higlight this &ldquo;from scratch&rdquo; aspect of the book here is a diagram tying together the various python function used in this post:</p>
<p><img src="./connecting_dots.png" alt="connecting_dots"></p>
<p>In the next post, we&rsquo;ll cover Gradient Descent!</p>
<p><img src="./book_disclaimer.png" alt="book_disclaimer"></p>
<p>For more content on data science, machine learning, R, Python, SQL and more, 
<a href="https://twitter.com/paulapivat" target="_blank" rel="noopener">find me on Twitter</a>.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/data-science/">Data Science</a>
  
  <a class="badge badge-light" href="/tag/probability/">Probability</a>
  
  <a class="badge badge-light" href="/tag/python/">Python</a>
  
  <a class="badge badge-light" href="/tag/statistics/">Statistics</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/dsfs_7/&amp;text=Data%20Science%20from%20Scratch%20%28ch7%29%20-%20Hypothesis%20and%20Inference" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/dsfs_7/&amp;t=Data%20Science%20from%20Scratch%20%28ch7%29%20-%20Hypothesis%20and%20Inference" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Data%20Science%20from%20Scratch%20%28ch7%29%20-%20Hypothesis%20and%20Inference&amp;body=/post/dsfs_7/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/dsfs_7/&amp;title=Data%20Science%20from%20Scratch%20%28ch7%29%20-%20Hypothesis%20and%20Inference" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Data%20Science%20from%20Scratch%20%28ch7%29%20-%20Hypothesis%20and%20Inference%20/post/dsfs_7/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/dsfs_7/&amp;title=Data%20Science%20from%20Scratch%20%28ch7%29%20-%20Hypothesis%20and%20Inference" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
    
    





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/paul-apivat-hanvongse/avatar_hu63a07477d78c9be41fad7b90b509fc17_91571_270x270_fill_q90_lanczos_center.jpg" alt="Paul Apivat Hanvongse">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/">Paul Apivat Hanvongse</a></h5>
        <h6 class="card-subtitle">Self-Employed | Getwyze</h6>
        <p class="card-text">My interests include data science, machine learning and R/Python programming.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/paulapivat" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/paulapivat/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/paulapivat" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  










  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/statistics_probability/">Statistics &amp; Probability in Code</a></li>
      
      <li><a href="/post/dsfs_6/">Data Science from Scratch (ch6) - Probability</a></li>
      
      <li><a href="/post/dsfs_5/">Data Science from Scratch (ch5) - Statistics</a></li>
      
      <li><a href="/post/dsfs_4/">Data Science from Scratch (ch4) - Linear Algebra</a></li>
      
      <li><a href="/post/dsfs_3/">Making sense of matplotlib</a></li>
      
    </ul>
  </div>
  




  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  <p class="powered-by">
    Â© 2020 Paul Apivat Hanvongse. All Rights Reserved.
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
