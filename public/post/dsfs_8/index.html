<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Paul Apivat Hanvongse">

  
  
  
    
  
  <meta name="description" content="An overview of Gradient Descent">

  
  <link rel="alternate" hreflang="en-us" href="/post/dsfs_8/">

  


  
  
  
  <meta name="theme-color" content="#E32626">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Domine:wght@400;700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/dsfs_8/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@paulapivat">
  <meta property="twitter:creator" content="@paulapivat">
  
  <meta property="og:site_name" content="Paul Apivat">
  <meta property="og:url" content="/post/dsfs_8/">
  <meta property="og:title" content="Gradient Descent -- Data Science from Scratch (ch8) | Paul Apivat">
  <meta property="og:description" content="An overview of Gradient Descent"><meta property="og:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-12-22T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-12-22T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/dsfs_8/"
  },
  "headline": "Gradient Descent -- Data Science from Scratch (ch8)",
  
  "datePublished": "2020-12-22T00:00:00Z",
  "dateModified": "2020-12-22T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Paul Apivat Hanvongse"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Paul Apivat",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "An overview of Gradient Descent"
}
</script>

  

  


  


  





  <title>Gradient Descent -- Data Science from Scratch (ch8) | Paul Apivat</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Paul Apivat</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Paul Apivat</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/#"><span>Start Here</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#accomplishments"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#gallery"><span>Viz</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/technical_notes/"><span>Technical Notes</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Gradient Descent -- Data Science from Scratch (ch8)</h1>

  
  <p class="page-subtitle">Building gradient descent from the ground up</p>
  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span><a href="/author/paul-apivat-hanvongse/">Paul Apivat Hanvongse</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Dec 22, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    9 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h3 id="table-of-contents">Table of contents</h3>
<ul>
<li>
<a href="#overview">Overview</a></li>
<li>
<a href="#setup">Setup</a></li>
<li>
<a href="#gradient_descent">Gradient Descent</a></li>
<li>
<a href="#from_scratch">From Scratch</a></li>
<li>
<a href="#take_away">Take Away</a></li>
</ul>
<h2 id="overview">Overview</h2>
<p>In this post, we&rsquo;ll explore Gradient Descent from the ground up starting conceptually, then using code to build up our intuition brick by brick.</p>
<p>While this post is part of an ongoing series where I document my progress through 
<a href="https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/" target="_blank" rel="noopener">Data Science from Scratch by Joel Grus</a>, for this post I am drawing on external sources including Aurélien Geron&rsquo;s Hands-On Machine Learning to provide a context for why and when gradient descent is used.</p>
<p>We&rsquo;ll also be using external libraries such as <code>numpy</code>, that are generally avoided in Data Science from Scratch, to help highlight concepts.</p>
<p>While the book introduces gradient descent as a standalone topic, I find it more intuitive to reason about it within the context of a regression problem.</p>
<h2 id="setup">Setup</h2>
<p>In any modeling task, there is error, and our objective is minimize the errors so that when we develop models from our training data, we&rsquo;ll have some confidence that the predictions will work in testing and completely new data.</p>
<p>We&rsquo;ll train a <em>linear regression model</em>. Our dataset will only have three data points. To create the model, we&rsquo;ll setting up parameters (slope and intercept) that best &ldquo;fits&rdquo; the data (i.e., best-fitting line), for example:</p>
<p><img src="./best_fit_line2.png" alt="best fit line2"></p>
<p>We know the values for both <code>x</code> and <code>y</code>, so we can calculate the slope and intercept directly through the <strong>normal equation</strong>, which is the 
<a href="http://mlwiki.org/index.php/Normal_Equation" target="_blank" rel="noopener">analytical approach</a> to finding regression coefficients (slope and intercept):</p>
<pre><code class="language-python"># Normal Equation

import numpy as np
import matplotlib.pyplot as plt

x = np.array([2, 4, 5])
y = np.array([45, 85, 105])

# computing Normal Equation
x_b = np.c_[np.ones((3, 1)), x]       # add x0 = 1 to each of three instances
theta = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)

# array([ 5., 20.])
theta
</code></pre>
<p>The key line is <code>np.linalg.inv()</code> which computes the multiplicative inverse of a matrix.</p>
<p>Our slope is 20 and intercept is 5 (i.e., <code>theta</code>).</p>
<p>We could also have used the more familiar &ldquo;rise over run&rdquo; ((85 - 45) / (4 - 2)) or (40/2) or 20, but we want to illustrate the <strong>normal equation</strong> which should come in handy when we go beyond the simplistic three data point example.</p>
<p>We could also use the <code>LinearRegression</code> class from <code>sklearn</code> to call the least squares (<code>np.linalg.lstsq()</code>) function directly:</p>
<pre><code class="language-python"># Linear Regression

from sklearn.linear_model import LinearRegression
import numpy as np

x = np.array([2, 4, 5])
y = np.array([45, 85, 105])

x = x.reshape(-1, 1)              # reshape because sklearn expect 2D array

x_b = np.c_[np.ones((3, 1)), x]   # add x0 = 1 to each of three instances

theta, residuals, rank, s = np.linalg.lstsq(x_b, y, rcond=1e-6)

# array([ 5., 20.])
print(&quot;theta:&quot;, theta)
</code></pre>
<p>This appraoch also yields the slope (20) and intercept (5) directly.</p>
<p>We know the parameters of <code>x</code> and <code>y</code> in our example, but we want to see how <strong>learning from data</strong> would work. Here&rsquo;s the equation we&rsquo;re working with:</p>
<pre><code>y = 20 * x + 5
</code></pre>
<p>And here&rsquo;s what it looks like (intercept = 5, slope = 20)</p>
<p><img src="./known_x_y.png" alt="known x y"></p>
<h2 id="gradient_descent">Gradient_Descent</h2>
<h4 id="why">Why?</h4>
<p>The <strong>normal equation</strong> and the <strong>least squares</strong> approach can handle large training sets efficiently, but when your model has a large number of features or too many training instances to fit into memory, <strong>gradient descent</strong> is an often used alternative.</p>
<p>Moreover, linear least squares assume the errors have a normal distribution and the relationship in the data is linear (this is where closed-form solutions like the normal equation excel). When the data is non-linear, an iterative solution (gradient descent) can be used.</p>
<p>With linear regression we seek to minimize the sum-of-squares differences between the observed data and the predicted values (aka the error), in a <strong>non-iterative</strong> fashion.</p>
<p>Alternatively, we use gradient descent to find the slope and intercept that minimizes the average squared error, however, in an <strong>iterative fashion</strong>.</p>
<h4 id="using-gradient-descent-to-fit-a-model">Using Gradient Descent to Fit a Model</h4>
<p>The process for gradient descent is to start with a <strong>random</strong> slope and intercept, then compute the gradient of the mean squared error, while adjusting the slope/intercept (<code>theta</code>) in the direction that continues to minimize the error. This is repeated iteratively until we find a point where errors are <em>most</em> minimized.</p>
<p><strong>NOTE</strong>: This section builds heavily on a previous post on linear algebra. You&rsquo;ll want to 
<a href="https://paulapivat.com/post/dsfs_4/" target="_blank" rel="noopener">read this post</a> to get a feel for the functions used to construct the functions we see in this post.</p>
<pre><code class="language-python">from typing import TypeVar, List, Iterator
import math
import random
import matplotlib.pyplot as plt
from typing import Callable
from typing import List
import numpy as np

x = np.array([2, 4, 5])

# instead of putting y directly, we'll use the equation: 20 * x + 5, which is a direct representation of its relationship to x

# y = np.array([45, 85, 105])   

# both x and y are represented in inputs
inputs = [(x, 20 * x + 5) for x in range(2, 6)]
</code></pre>
<p>First, we&rsquo;ll start with random values for the slope and intercept; we&rsquo;ll also establish a learning rate, which controls how much a change in the model is warranted in response to the estimated error each time the model parameters (slope and intercept) are updated.</p>
<pre><code class="language-python"># 1. start with a random value for slope and intercept
theta = [random.uniform(-1, 1), random.uniform(-1, 1)]

learning_rate = 0.001
</code></pre>
<p>Next, we&rsquo;ll compute the mean of the gradients, then adjust the slope/intercept in the direction of minimizing the gradient, which is based on the error.</p>
<p>You&rsquo;ll note that this for-loop has 100 iterations. The more iterations we go through, the more that errors are minimized and the more we approach a slope/intercept where the model &ldquo;fits&rdquo; the data better.</p>
<p>You can see in this list, <code>[linear_gradient(x, y, theta) for x, y in inputs]</code>, that our <code>linear_gradient</code> function is applied to the known <code>x</code> and <code>y</code> values in the list of tuples, <code>inputs</code>, along with random values for slope/intercept (<code>theta</code>).</p>
<p>We multiply each <code>x</code> value with a random value for slope, then add a random value for intercept. This yields the initial prediction. Error is the gap between the initial prediction and <em>actual</em> <code>y</code> values. We minimize the squared error by using its gradient.</p>
<pre><code class="language-python"># start with a function that determines the gradient based on the error from a single data point
def linear_gradient(x: float, y: float, theta: Vector) -&gt; Vector:
    slope, intercept = theta
    predicted = slope * x + intercept   # model prediction
    error = (predicted - y)             # error is (predicted - actual)
    squared_error = error ** 2          # minimize squared error
    grad = [2 * error * x, 2 * error]   # using its gradient
    return grad
</code></pre>
<p>The <code>linear_gradient</code> function along with initial parameters are then passed to <code>vector_mean</code>, which utilize <code>scalar_multiply</code> and <code>vector_sum</code>:</p>
<pre><code class="language-python">
def vector_mean(vectors: List[Vector]) -&gt; Vector:
    &quot;&quot;&quot;Computes the element-wise average&quot;&quot;&quot;
    n = len(vectors)
    return scalar_multiply(1/n, vector_sum(vectors))

def scalar_multiply(c: float, v: Vector) -&gt; Vector:
    &quot;&quot;&quot;Multiplies every element by c&quot;&quot;&quot;
    return [c * v_i for v_i in v]
    
def vector_sum(vectors: List[Vector]) -&gt; Vector:
    &quot;&quot;&quot;Sum all corresponding elements (componentwise sum)&quot;&quot;&quot;
    # Check that vectors is not empty
    assert vectors, &quot;no vectors provided!&quot;
    # Check the vectorss are all the same size
    num_elements = len(vectors[0])
    assert all(len(v) == num_elements for v in vectors), &quot;different sizes!&quot;
    # the i-th element of the result is the sum of every vector[i]
    return [sum(vector[i] for vector in vectors)
            for i in range(num_elements)]
</code></pre>
<p>This yields the gradient. Then, each <code>gradient_step</code> is deteremined as our function adjusts the initial random <code>theta</code> values (slope/intercept) in the direction that minimizes the error.</p>
<pre><code class="language-python">def gradient_step(v: Vector, gradient: Vector, step_size: float) -&gt; Vector:
    &quot;&quot;&quot;Moves `step_size` in the `gradient` direction from `v`&quot;&quot;&quot;
    assert len(v) == len(gradient)
    step = scalar_multiply(step_size, gradient)
    return add(v, step)
    
def add(v: Vector, w: Vector) -&gt; Vector:
    &quot;&quot;&quot;Adds corresponding elements&quot;&quot;&quot;
    assert len(v) == len(w), &quot;vectors must be the same length&quot;
    return [v_i + w_i for v_i, w_i in zip(v, w)]
</code></pre>
<p>All this comes together in this <strong>for-loop</strong> to print out how the slope and intercept change with each iteration (we start with 100):</p>
<pre><code class="language-python">for epoch in range(100):     # start with 100 &lt;--- change this figure to try different iterations
    # compute the mean of the gradients
    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])
    # take a step in that direction
    theta = gradient_step(theta, grad, -learning_rate)
    print(epoch, grad, theta)

slope, intercept = theta

#assert 19.9 &lt; slope &lt; 20.1,  &quot;slope should be about 20&quot;
#assert 4.9 &lt; intercept &lt; 5.1, &quot;intercept should be about 5&quot;
print(&quot;slope&quot;, slope)
print(&quot;intercept&quot;, intercept)
</code></pre>
<h4 id="iterative-descent">Iterative Descent</h4>
<p>At 100 iterations, the slope is 18.87 and intercept is 4.87 and the gradient is -32.48 (error for the slope) and -8.45 (error for the intercept). These numbers suggest that we need to decrease the slope and intercept from our random starting point, but our emphasis needs to be on decreasing the slope.</p>
<p><img src="./100_iterations.png" alt="100 iterations"></p>
<p>At 200 iterations, the slope is 19.97 and intercept is 4.86 and the gradient is -1.76 (error for the slope) and -0.48 (error for the intercept). Our errors have been reduced significantly.</p>
<p><img src="./200_iterations.png" alt="200 iterations"></p>
<p>At 1000 iterations, the slope is 19.97 (not much difference from 200 iterations) and intercept is 5.09 and the gradients are markedly lower at -0.004 (error for the slope) and 0.02 (error for the intercept). Here the errors may not be much different from zero and we are near our optimal point.</p>
<p><img src="./1000_iterations.png" alt="1000 iterations"></p>
<p>In summary, the <strong>normal equation</strong> and <strong>regression</strong> approaches gave us a slope of 20 and intercept of 5. With gradient descent, we approached these values with each successive iterations, 1000 iterations yielding <strong>less error</strong> than 100 or 200 iterations.</p>
<h2 id="from_scratch">From_Scratch</h2>
<p>As mentioned above, the functions used to compute the gradients and adjust the slope/intercept build on functions we explored in 
<a href="https://paulapivat.com/post/dsfs_4/" target="_blank" rel="noopener">this post</a>. Here&rsquo;s a visual showing how the functions we used to iteratively arrive at the slope and intercept through gradient descent was built:</p>
<p><img src="./ch8_funct.png" alt="ch8_funct"></p>
<h2 id="take_away">Take_Away</h2>
<p>Gradient descent is an optimization technique often used in machine learning and in this post, we built some intuition around how it works by applying it to a simple linear regression problem, favoring code over math (which we&rsquo;ll return to in a later post). Gradient Descent is useful if you are expecting computational complexity due to the number of features or training instances.</p>
<p>We placed gradient descent in context, in comparison to a more analytical approach, normal equation and the least squares method, both of which are non-iterative.</p>
<p>Furthermore, we saw how the functions used in this post can be traced back to a previous post on 
<a href="https://paulapivat.com/post/dsfs_4/" target="_blank" rel="noopener">linear algebra</a>, thus giving us a big picture view of how the building blocks of data science and an intuition for areas we&rsquo;ll need to explore at a deeper, perhaps at a more mathematical, level.</p>
<p>This post is part of an ongoing series where I document my progress through 
<a href="https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/" target="_blank" rel="noopener">Data Science from Scratch by Joel Grus</a>.</p>
<p><img src="./book_disclaim_ch8.png" alt="book disclaim ch8"></p>
<p>For more content on data science, machine learning, R, Python, SQL and more, 
<a href="https://twitter.com/paulapivat" target="_blank" rel="noopener">find me on Twitter</a>.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/python/">Python</a>
  
  <a class="badge badge-light" href="/tag/data-science/">Data Science</a>
  
  <a class="badge badge-light" href="/tag/gradient-descent/">Gradient Descent</a>
  
  <a class="badge badge-light" href="/tag/machine-learning/">Machine Learning</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/dsfs_8/&amp;text=Gradient%20Descent%20--%20Data%20Science%20from%20Scratch%20%28ch8%29" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/dsfs_8/&amp;t=Gradient%20Descent%20--%20Data%20Science%20from%20Scratch%20%28ch8%29" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Gradient%20Descent%20--%20Data%20Science%20from%20Scratch%20%28ch8%29&amp;body=/post/dsfs_8/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/dsfs_8/&amp;title=Gradient%20Descent%20--%20Data%20Science%20from%20Scratch%20%28ch8%29" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Gradient%20Descent%20--%20Data%20Science%20from%20Scratch%20%28ch8%29%20/post/dsfs_8/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/dsfs_8/&amp;title=Gradient%20Descent%20--%20Data%20Science%20from%20Scratch%20%28ch8%29" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
    
    





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/paul-apivat-hanvongse/avatar_hu63a07477d78c9be41fad7b90b509fc17_91571_270x270_fill_q90_lanczos_center.jpg" alt="Paul Apivat Hanvongse">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/">Paul Apivat Hanvongse</a></h5>
        <h6 class="card-subtitle">Self-Employed | Getwyze</h6>
        <p class="card-text">My interests include data science, machine learning and R/Python programming.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/paulapivat" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/paulapivat/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/paulapivat" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  










  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/sentiment_analysis/">How Positive are Your Facebook Posts?</a></li>
      
      <li><a href="/post/dsfs_7/">Data Science from Scratch (ch7) - Hypothesis and Inference</a></li>
      
      <li><a href="/post/statistics_probability/">Statistics &amp; Probability in Code</a></li>
      
      <li><a href="/post/dsfs_6/">Data Science from Scratch (ch6) - Probability</a></li>
      
      <li><a href="/post/end_to_end/">End-to-End Projects</a></li>
      
    </ul>
  </div>
  




  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  <p class="powered-by">
    © 2020 Paul Apivat Hanvongse. All Rights Reserved.
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
