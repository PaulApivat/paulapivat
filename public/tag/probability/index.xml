<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probability | Paul Apivat</title>
    <link>/tag/probability/</link>
      <atom:link href="/tag/probability/index.xml" rel="self" type="application/rss+xml" />
    <description>Probability</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Paul Apivat Hanvongse. All Rights Reserved.</copyright><lastBuildDate>Tue, 15 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Probability</title>
      <link>/tag/probability/</link>
    </image>
    
    <item>
      <title>Data Science from Scratch (ch7) - Hypothesis and Inference</title>
      <link>/post/dsfs_7/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_7/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#central_limit_theorem&#34;&gt;Central Limit Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#hypothesis_testing&#34;&gt;Hypothesis Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#p_values&#34;&gt;p-Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#confidence_intervals&#34;&gt;Confidence Intervals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#connecting_dots&#34;&gt;Connecting dots with Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll use a classic coin-flipping example in this post because it is simple to illustrate with both &lt;strong&gt;concept&lt;/strong&gt; and &lt;strong&gt;code&lt;/strong&gt;. My goal is to tie several concepts together including (traditional) Hypothesis and Inference, Estimation Theory, and Bayesian Inference. All using the same coin-flipping example.&lt;/p&gt;
&lt;h2 id=&#34;central_limit_theorem&#34;&gt;Central_Limit_Theorem&lt;/h2&gt;
&lt;p&gt;Terms like &amp;ldquo;null&amp;rdquo; and &amp;ldquo;alternative&amp;rdquo; hypothesis are used quite frequently, so let&amp;rsquo;s set some context. The &amp;ldquo;null&amp;rdquo; is the &lt;strong&gt;default&lt;/strong&gt; position. The &amp;ldquo;alternative&amp;rdquo;, alt for short, is something we&amp;rsquo;re &lt;em&gt;comparing to&lt;/em&gt; the default (null).&lt;/p&gt;
&lt;p&gt;The classic coin-flipping exercise is to test the &lt;em&gt;fairness&lt;/em&gt; off a coin. If a coin is fair, it&amp;rsquo;ll land on heads 50% of the time (and tails 50% of the time). Let&amp;rsquo;s translate into hypothesis testing language:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis&lt;/strong&gt;: Probability of landing on Heads = 0.5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alt Hypothesis&lt;/strong&gt;: Probability of landing on Heads != 0.5.&lt;/p&gt;
&lt;p&gt;Each coin flip is a &lt;strong&gt;Bernoulli trial&lt;/strong&gt;, which is an experiment with two outcomes - outcome 1, &amp;ldquo;success&amp;rdquo;, (probability &lt;em&gt;p&lt;/em&gt;) and outcome 0, &amp;ldquo;fail&amp;rdquo; (probability &lt;em&gt;p - 1&lt;/em&gt;). The reason it&amp;rsquo;s a Bernoulli trial is because there are only two outcome with a coin flip (heads or tails). Read more about 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_trial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bernoulli here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code for a single Bernoulli Trial:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bernoulli_trial(p: float) -&amp;gt; int:
    &amp;quot;&amp;quot;&amp;quot;Returns 1 with probability p and 0 with probability 1-p&amp;quot;&amp;quot;&amp;quot;
    return 1 if random.random() &amp;lt; p else 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you &lt;strong&gt;sum the independent Bernoulli trials&lt;/strong&gt;, you get a &lt;strong&gt;Binomial(n,p)&lt;/strong&gt; random variable, a variable whose &lt;em&gt;possible&lt;/em&gt; values have a probability distribution. The &lt;strong&gt;central limit theorem&lt;/strong&gt; says as &lt;strong&gt;n&lt;/strong&gt; or the &lt;em&gt;number&lt;/em&gt; of independent Bernoulli trials get large, the Binomial distribution approaches a normal distribution.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code for when you sum all the Bernoulli Trials to get a Binomial random variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def binomial(n: int, p: float) -&amp;gt; int:
    &amp;quot;&amp;quot;&amp;quot;Returns the sum of n bernoulli(p) trials&amp;quot;&amp;quot;&amp;quot;
    return sum(bernoulli_trial(p) for _ in range(n))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: A single &amp;lsquo;success&amp;rsquo; in a Bernoulli trial is &amp;lsquo;x&amp;rsquo;. Summing up all those x&amp;rsquo;s into X, is a Binomial random variable. Success doesn&amp;rsquo;t imply desirability, nor does &amp;ldquo;failure&amp;rdquo; imply undesirability. They&amp;rsquo;re just terms to count the cases we&amp;rsquo;re looking for (i.e., number of heads in multiple coin flips to assess a coin&amp;rsquo;s fairness).&lt;/p&gt;
&lt;p&gt;Given that our &lt;strong&gt;null&lt;/strong&gt; is (p = 0.5) and &lt;strong&gt;alt&lt;/strong&gt; is (p != 0.5), we can run some independent bernoulli trials, then sum them up to get a binomial random variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./independent_coin_flips.png&#34; alt=&#34;independent_coin_flips&#34;&gt;&lt;/p&gt;
&lt;p&gt;Each &lt;code&gt;bernoulli_trial&lt;/code&gt; is an experiment with either 0 or 1 as outcomes. The &lt;code&gt;binomial&lt;/code&gt; function sums up &lt;strong&gt;n&lt;/strong&gt; bernoulli(0.5) trails. We ran both twice and got different results. Each bernoulli experiment can be a success(1) or faill(0); summing up into a binomial random variable means we&amp;rsquo;re taking the probability p(0.5) &lt;em&gt;that a coin flips head&lt;/em&gt; and we ran the experiment 1,000 times to get a random binomial variable.&lt;/p&gt;
&lt;p&gt;The first 1,000 flips we got 510. The second 1,000 flips we got 495. We can repeat this process many times to get a &lt;em&gt;distribution&lt;/em&gt;. We can plot this distribution to reinforce our understanding. To this we&amp;rsquo;ll use &lt;code&gt;binomial_histogram&lt;/code&gt; function. This function picks points from a Binomial(n,p) random variable and plots their histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter
import matplotlib.pyplot as plt

def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2
    

def binomial_histogram(p: float, n: int, num_points: int) -&amp;gt; None:
    &amp;quot;&amp;quot;&amp;quot;Picks points from a Binomial(n, p) and plots their histogram&amp;quot;&amp;quot;&amp;quot;
    data = [binomial(n, p) for _ in range(num_points)]
    # use a bar chart to show the actual binomial samples
    histogram = Counter(data)
    plt.bar([x - 0.4 for x in histogram.keys()],
            [v / num_points for v in histogram.values()],
            0.8,
            color=&#39;0.75&#39;)
    mu = p * n
    sigma = math.sqrt(n * p * (1 - p))
    # use a line chart to show the normal approximation
    xs = range(min(data), max(data) + 1)
    ys = [normal_cdf(i + 0.5, mu, sigma) -
          normal_cdf(i - 0.5, mu, sigma) for i in xs]
    plt.plot(xs, ys)
    plt.title(&amp;quot;Binomial Distribution vs. Normal Approximation&amp;quot;)
    plt.show()

# call function   
binomial_histogram(0.5, 1000, 10000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This plot is then rendered:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./binomial_coin_fairness.png&#34; alt=&#34;binomial_coin_fairness&#34;&gt;&lt;/p&gt;
&lt;p&gt;What we did was sum up independent &lt;code&gt;bernoulli_trial&lt;/code&gt;(s) of 1,000 coin flips, where the probability of head is p = 0.5, to create a &lt;code&gt;binomial&lt;/code&gt; random variable. We then repeated this a large number of times (N = 10,000), then plotted a histogram of the distribution of all binomial random variables. And because we did it so many times, it approximates the standard normal distribution (smooth bell shape curve).&lt;/p&gt;
&lt;p&gt;Just to demonstrate how this works, we can generate several &lt;code&gt;binomial&lt;/code&gt; random variables:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./several_binomial.png&#34; alt=&#34;several_binomial&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we do this 10,000 times, we&amp;rsquo;ll generate the above histogram. You&amp;rsquo;ll notice that because we are testing whether the coin is fair, the probability of heads (success) &lt;em&gt;should&lt;/em&gt; be at 0.5 and, from 1,000 coin flips, the &lt;strong&gt;mean&lt;/strong&gt;(&lt;code&gt;mu&lt;/code&gt;) should be a 500.&lt;/p&gt;
&lt;p&gt;We have another function that can help us calculate &lt;code&gt;normal_approximation_to_binomial&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random
from typing import Tuple
import math


def normal_approximation_to_binomial(n: int, p: float) -&amp;gt; Tuple[float, float]:
    &amp;quot;&amp;quot;&amp;quot;Return mu and sigma corresponding to a Binomial(n, p)&amp;quot;&amp;quot;&amp;quot;
    mu = p * n
    sigma = math.sqrt(p * (1 - p) * n)
    return mu, sigma
    
# call function
# (500.0, 15.811388300841896)
normal_approximation_to_binomial(1000, 0.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When calling the function with our parameters, we get a mean &lt;code&gt;mu&lt;/code&gt; of 500 (from 1,000 coin flips) and a standard deviation &lt;code&gt;sigma&lt;/code&gt; of 15.8114. Which means that 68% of the time, the binomial random variable will be 500 +/- 15.8114 and 95% of the time it&amp;rsquo;ll be 500 +/- 31.6228 (see 
&lt;a href=&#34;https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;68-95-99.7 rule&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;hypothesis_testing&#34;&gt;Hypothesis_Testing&lt;/h2&gt;
&lt;p&gt;Now that we have seen the results of our &amp;ldquo;coin fairness&amp;rdquo; experiment plotted on a binomial distribution (approximately normal), we will be, for the purpose of testing our hypothesis, be interested in the probability of its realized value (binomial random variable) lies &lt;strong&gt;within or outside a particular interval&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This means we&amp;rsquo;ll be interested in questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What&amp;rsquo;s the probability that the binomial(n,p) is below a threshold?&lt;/li&gt;
&lt;li&gt;Above a threshold?&lt;/li&gt;
&lt;li&gt;Between an interval?&lt;/li&gt;
&lt;li&gt;Outside an interval?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, the &lt;code&gt;normal_cdf&lt;/code&gt; (normal cummulative distribution function), which we learned in a 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_6/#distributions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;, &lt;em&gt;is&lt;/em&gt; the probability of a variable being &lt;em&gt;below&lt;/em&gt; a certain threshold.&lt;/p&gt;
&lt;p&gt;Here, the probability of X (success or heads for a &amp;lsquo;fair coin&amp;rsquo;) is at 0.5 (&lt;code&gt;mu&lt;/code&gt; = 500, &lt;code&gt;sigma&lt;/code&gt; = 15.8113), and we want to find the probability that X falls below 490, which comes out to roughly 26%&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;normal_probability_below = normal_cdf

# probability that binomal random variable, mu = 500, sigma = 15.8113, is below 490

# 0.26354347477247553
normal_probability_below(490, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the other hand, the &lt;code&gt;normal_probability_above&lt;/code&gt;, probability that X falls &lt;em&gt;above&lt;/em&gt; 490 would be
1 - 0.2635 = 0.7365 or roughly 74%.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normal_probability_above(lo: float,
                             mu: float = 0,
                             sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;The probability that an N(mu, sigma) is greater than lo.&amp;quot;&amp;quot;&amp;quot;
    return 1 - normal_cdf(lo, mu, sigma)
    
# 0.7364565252275245
normal_probability_above(490, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make sense of this we need to recall the binomal distribution, that approximates the normal distribution, but we&amp;rsquo;ll draw a vertical line at 490.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./binomial_vline.png&#34; alt=&#34;binomial_vline&#34;&gt;&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re asking, given the binomal distribution with &lt;code&gt;mu&lt;/code&gt; 500 and &lt;code&gt;sigma&lt;/code&gt; at 15.8113, what is the probability that a binomal random variable falls below the threshold (left of the line); the answer is approximately 26% and correspondingly falling above the threshold (right of the line), is approximately 74%.&lt;/p&gt;
&lt;h3 id=&#34;between-interval&#34;&gt;Between interval&lt;/h3&gt;
&lt;p&gt;We may also wonder what the probability of a binomial random variable &lt;strong&gt;falling between 490 and 520&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./binomial_2_vline.png&#34; alt=&#34;binomial_2_vline&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here is the function to calculate this probability and it comes out to approximately 63%. &lt;em&gt;note&lt;/em&gt;: Bear in mind the full area under the curve is 1.0 or 100%.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normal_probability_between(lo: float,
                               hi: float,
                               mu: float = 0,
                               sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;The probability that an N(mu, sigma) is between lo and hi.&amp;quot;&amp;quot;&amp;quot;
    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)

# 0.6335061861416337
normal_probability_between(490, 520, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the area outside of the interval should be 1 - 0.6335 = 0.3665:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normal_probability_outside(lo: float,
                               hi: float,
                               mu: float = 0,
                               sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;The probability that an N(mu, sigma) is not between lo and hi.&amp;quot;&amp;quot;&amp;quot;
    return 1 - normal_probability_between(lo, hi, mu, sigma)
    
# 0.3664938138583663
normal_probability_outside(490, 520, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to the above, we may also be interested in finding (symmetric) intervals around the mean that account for a &lt;em&gt;certain level of likelihood&lt;/em&gt;, for example, 60% probability centered around the mean.&lt;/p&gt;
&lt;p&gt;For this operation we would use the &lt;code&gt;inverse_normal_cdf&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def inverse_normal_cdf(p: float,
                       mu: float = 0,
                       sigma: float = 1,
                       tolerance: float = 0.00001) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Find approximate inverse using binary search&amp;quot;&amp;quot;&amp;quot;
    # if not standard, compute standard and rescale
    if mu != 0 or sigma != 1:
        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)
    low_z = -10.0     # normal_cdf(-10) is (very close to) 0
    hi_z = 10.0       # normal_cdf(10) is (very close to) 1
    while hi_z - low_z &amp;gt; tolerance:
        mid_z = (low_z + hi_z) / 2      # Consider the midpoint
        mid_p = normal_cdf(mid_z)       # and the CDF&#39;s value there
        if mid_p &amp;lt; p:
            low_z = mid_z               # Midpoint too low, search above it
        else:
            hi_z = mid_z                # Midpoint too high, search below it
    return mid_z
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we&amp;rsquo;d have to find the cutoffs where the upper and lower tails each contain 20% of the probability. We calculate &lt;code&gt;normal_upper_bound&lt;/code&gt; and &lt;code&gt;normal_lower_bound&lt;/code&gt; and use those to calculate the &lt;code&gt;normal_two_sided_bounds&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normal_upper_bound(probability: float,
                       mu: float = 0,
                       sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Returns the z for which P(Z &amp;lt;= z) = probability&amp;quot;&amp;quot;&amp;quot;
    return inverse_normal_cdf(probability, mu, sigma)


def normal_lower_bound(probability: float,
                       mu: float = 0,
                       sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Returns the z for which P(Z &amp;gt;= z) = probability&amp;quot;&amp;quot;&amp;quot;
    return inverse_normal_cdf(1 - probability, mu, sigma)


def normal_two_sided_bounds(probability: float,
                            mu: float = 0,
                            sigma: float = 1) -&amp;gt; Tuple[float, float]:
    &amp;quot;&amp;quot;&amp;quot;
    Returns the symmetric (about the mean) bounds
    that contain the specified probability
    &amp;quot;&amp;quot;&amp;quot;
    tail_probability = (1 - probability) / 2
    # upper bound should have tail_probability above it
    upper_bound = normal_lower_bound(tail_probability, mu, sigma)
    # lower bound should have tail_probability below it
    lower_bound = normal_upper_bound(tail_probability, mu, sigma)
    return lower_bound, upper_bound
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So if we wanted to know what the cutoff points were for a 60% probability around the mean and standard deviation (&lt;code&gt;mu&lt;/code&gt; = 500, &lt;code&gt;sigma&lt;/code&gt; = 15.8113), it would be between &lt;strong&gt;486.69 and 513.31&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Said differently, this means roughly 60% of the time, we can expect the binomial random variable to fall between 486 and 513.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# (486.6927811021805, 513.3072188978196)
normal_two_sided_bounds(0.60, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;significance-and-power&#34;&gt;Significance and Power&lt;/h3&gt;
&lt;p&gt;Now that we have a handle on the binomial normal distribution, thresholds (left and right of the mean), and cut-off points, we want to make a &lt;strong&gt;decision about significance&lt;/strong&gt;. Probably the most important part of &lt;em&gt;statistical significance&lt;/em&gt; is that it is a decision to be made, not a standard that is externally set.&lt;/p&gt;
&lt;p&gt;Significance is a decision about how willing we are to make a &lt;em&gt;type 1&lt;/em&gt; error (false positive), which we explored in a 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_6/#applying_bayes_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;. The convention is to set it to a 5% or 1% willingness to make a type 1 error. Suppose we say 5%.&lt;/p&gt;
&lt;p&gt;We would say that out of 1,000 coin flips, 95% of the time, we&amp;rsquo;d get between 469 and 531 heads on a &amp;ldquo;fair coin&amp;rdquo; and 5% of the time, outside of this 469-531 range.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# (469.0104394712448, 530.9895605287552)
normal_two_sided_bounds(0.95, 500, 15.8113)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we recall our hypotheses:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis&lt;/strong&gt;: Probability of landing on Heads = 0.5 (fair coin)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alt Hypothesis&lt;/strong&gt;: Probability of landing on Heads != 0.5 (biased coin)&lt;/p&gt;
&lt;p&gt;Each binomial distribution (test) that consist of 1,000 bernoulli trials, each &lt;em&gt;test&lt;/em&gt; where the number of heads falls outside the range of 469-531, we&amp;rsquo;ll &lt;strong&gt;reject the null&lt;/strong&gt; that the coin is fair. And we&amp;rsquo;ll be wrong (false positive), 5% of the time. It&amp;rsquo;s a false positive when we &lt;strong&gt;incorrectly reject&lt;/strong&gt; the null hypothesis, when it&amp;rsquo;s actually true.&lt;/p&gt;
&lt;p&gt;We also want to avoid making a type-2 error (false negative), where we &lt;strong&gt;fail to reject&lt;/strong&gt; the null hypothesis, when it&amp;rsquo;s actually false.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Its important to keep in mind that terms like &lt;em&gt;significance&lt;/em&gt; and &lt;em&gt;power&lt;/em&gt; are used to describe &lt;strong&gt;tests&lt;/strong&gt;, in our case, the test of whether a coin is fair or not. Each test is the sum of 1,000 independent bernoulli trials.&lt;/p&gt;
&lt;p&gt;For a &amp;ldquo;test&amp;rdquo; that has a 95% significance, we&amp;rsquo;ll assume that out of a 1,000 coin flips, it&amp;rsquo;ll land on heads between 469-531 times and we&amp;rsquo;ll determine the coin is fair. For the 5% of the time it lands outside of this range, we&amp;rsquo;ll determine the coin to be &amp;ldquo;unfair&amp;rdquo;, but we&amp;rsquo;ll be wrong because it actually is fair.&lt;/p&gt;
&lt;p&gt;To calculate the power of the test, we&amp;rsquo;ll take the assumed &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt; with a 95% bounds (based on the assumption that the probability of the coin landing on heads is 0.5 or 50% - a fair coin). We&amp;rsquo;ll determine the lower and upper bounds:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)
lo # 469.01026640487555
hi # 530.9897335951244
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if the coin was &lt;em&gt;actually biased&lt;/em&gt;, we should reject the null, but we fail to. Let&amp;rsquo;s suppose the actual probability that the coin lands on heads is 55% ( &lt;strong&gt;biased&lt;/strong&gt; towards head):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)
mu_1    # 550.0
sigma_1 # 15.732132722552274
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the same range 469 - 531, where the coin is assumed &amp;lsquo;fair&amp;rsquo; with &lt;code&gt;mu&lt;/code&gt; at 500 and &lt;code&gt;sigma&lt;/code&gt; at 15.8113:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./95sig_binomial.png&#34; alt=&#34;95sig_binomial&#34;&gt;&lt;/p&gt;
&lt;p&gt;If the coin, in fact, had a bias towards head (p = 0.55), the distribution would shift right, but if our 95% significance test remains the same, we get:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./type2_error.png&#34; alt=&#34;type2_error&#34;&gt;&lt;/p&gt;
&lt;p&gt;The probability of making a type-2 error is 11.345%. This is the probability that we&amp;rsquo;re see that the coin&amp;rsquo;s distribution is within the previous interval 469-531, thinking we should accept the null hypothesis (that the coin is fair), but in actuality, failing to see that the distribution has shifted to the coin having a &lt;em&gt;bias&lt;/em&gt; towards heads.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.11345199870463285
type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other way to arrive at this is to find the probability, under the &lt;em&gt;new&lt;/em&gt; &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt; (new distribution), that X (number of successes) will fall &lt;em&gt;below&lt;/em&gt; 531.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.11357762975476304
normal_probability_below(531, mu_1, sigma_1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the probability of making a type-2 error or the probability that the &lt;em&gt;new&lt;/em&gt; distribution falls below 531 is approximately 11.3%.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;power to detect&lt;/strong&gt; a type-2 error is 1.00 minus the probability of a type-2 error (1 - 0.113 = 0.887), or 88.7%.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;power = 1 - type_2_probability # 0.8865480012953671
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we may be interested in &lt;strong&gt;increasing power&lt;/strong&gt; to detect a type-2 error. Instead of using a &lt;code&gt;normal_two_sided_bounds&lt;/code&gt; function to find the cut-off points (i.e., 469 and 531), we could use a &lt;em&gt;one-sided test&lt;/em&gt; that rejects the null hypothesis (&amp;lsquo;fair coin&amp;rsquo;) when X (number of heads on a coin-flip) is much larger than 500.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code, using &lt;code&gt;normal_upper_bound&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 526.0073585242053
hi = normal_upper_bound(0.95, mu_0, sigma_0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means shifting the upper bounds from 531 to 526, providing more probability in the upper tail. This means the probability of a type-2 error goes down from 11.3 to 6.3.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./increase_power.png&#34; alt=&#34;increase_power&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# previous probability of type-2 error
# 0.11357762975476304
normal_probability_below(531, mu_1, sigma_1)


# new probability of type-2 error
# 0.06356221447122662
normal_probability_below(526, mu_1, sigma_1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the new (stronger) &lt;strong&gt;power to detect&lt;/strong&gt; type-2 error is 1.0 - 0.064 = 0.936 or 93.6% (up from 88.7% above).&lt;/p&gt;
&lt;h2 id=&#34;p_values&#34;&gt;p_values&lt;/h2&gt;
&lt;p&gt;p-Values represent &lt;em&gt;another way&lt;/em&gt; of deciding whether to accept or reject the Null Hypothesis. Instead of choosing bounds, thresholds or cut-off points, we could compute the probability, assuming the Null Hypothesis is true, that we would see a value &lt;em&gt;as extreme as&lt;/em&gt; the one we just observed.&lt;/p&gt;
&lt;p&gt;Here is the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def two_sided_p_values(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;
    How likely are we to see a value at least as extreme as x (in either
    direction) if our values are from an N(mu, sigma)?
    &amp;quot;&amp;quot;&amp;quot;
    if x &amp;gt;= mu:
        # x is greater than the mean, so the tail is everything greater than x
        return 2 * normal_probability_above(x, mu, sigma)
    else:
        # x is less than the mean, so the tail is everything less than x
        return 2 * normal_probability_below(x, mu, sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to compute, assuming we have a &amp;ldquo;fair coin&amp;rdquo; (&lt;code&gt;mu&lt;/code&gt; = 500, &lt;code&gt;sigma&lt;/code&gt; = 15.8113), what is the probability of seeing a value like 530? (&lt;strong&gt;note&lt;/strong&gt;: We use 529.5 instead of 530 below due to 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Continuity_correction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continuity correction&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Answer: approximately 6.2%&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.06207721579598835
two_sided_p_values(529.5, mu_0, sigma_0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value, 6.2% is higher than our (hypothetical) 5% significance, so we don&amp;rsquo;t reject the null. On the other hand, if X was slightly more extreme, 532, the probability of seeing that value would be approximately 4.3%, which is less than 5% significance, so we would reject the null.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.04298479507085862
two_sided_p_values(532, mu_0, sigma_0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For one-sided tests, we would use the &lt;code&gt;normal_probability_above&lt;/code&gt; and &lt;code&gt;normal_probability_below&lt;/code&gt; functions created above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;upper_p_value = normal_probability_above
lower_p_value = normal_probability_below
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under the &lt;code&gt;two_sided_p_values&lt;/code&gt; test, the extreme value of 529.5 had a probability of 6.2% of showing up, but not low enough to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;However, with a one-sided test, &lt;code&gt;upper_p_value&lt;/code&gt; for the same threshold is now 3.1% and we would reject the null hypothesis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 0.031038607897994175
upper_p_value(529.5, mu_0, sigma_0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;confidence_intervals&#34;&gt;Confidence_Intervals&lt;/h2&gt;
&lt;p&gt;A &lt;em&gt;third&lt;/em&gt; approach to deciding whether to accept or reject the null is to use confidence intervals. We&amp;rsquo;ll use the 530 as we did in the p-Values example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p_hat = 530/1000
mu = p_hat
sigma = math.sqrt(p_hat * (1 - p_hat) / 1000) # 0.015782902141241326

# (0.4990660982192851, 0.560933901780715)
normal_two_sided_bounds(0.95, mu, sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The confidence interval for a coin flipping heads 530 (out 1,000) times is (0.4991, 0.5609). Since this interval &lt;strong&gt;contains&lt;/strong&gt; the p = 0.5 (probability of heads 50% of the time, assuming a fair coin), we do not reject the null.&lt;/p&gt;
&lt;p&gt;If the extreme value were &lt;em&gt;more&lt;/em&gt; extreme at 540, we would arrive at a different conclusion:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p_hat = 540/1000
mu = p_hat
sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)

(0.5091095927295919, 0.5708904072704082)
normal_two_sided_bounds(0.95, mu, sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we would be 95% confident that the mean of this distribution is contained between 0.5091 and 0.5709 and this &lt;strong&gt;does not&lt;/strong&gt; contain 0.500 (albiet by a slim margin), so we reject the null hypothesis that this is a fair coin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: Confidence intervals are about the &lt;em&gt;interval&lt;/em&gt; not probability p. We interpret the confidence interval as, if you were to repeat the experiment many times, 95% of the time, the &amp;ldquo;true&amp;rdquo; parameter, in our example p = 0.5, would lie within the observed confidence interval.&lt;/p&gt;
&lt;h2 id=&#34;connecting_dots&#34;&gt;Connecting_Dots&lt;/h2&gt;
&lt;p&gt;To recap, we used python to build intuition around statistical hypothesis testing.&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics &amp; Probability in Code</title>
      <link>/post/statistics_probability/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/statistics_probability/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#permutations&#34;&gt;Permutations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Itertools&lt;/code&gt; are a core set of fast, memory efficient tools for creating iterators for efficient looping (read the 
&lt;a href=&#34;https://docs.python.org/3/library/itertools.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; here).&lt;/p&gt;
&lt;h2 id=&#34;itertools-permutations&#34;&gt;Itertools Permutations&lt;/h2&gt;
&lt;p&gt;One (of many) uses for &lt;code&gt;itertools&lt;/code&gt; is to create a &lt;code&gt;permutations()&lt;/code&gt; function that will return all possible combinations of items in a list.&lt;/p&gt;
&lt;p&gt;I was working on a project that involved user funnels with different stages and we were wondering how many different &amp;ldquo;paths&amp;rdquo; a user &lt;em&gt;could&lt;/em&gt; take, so this was naturally a good fit for using &lt;strong&gt;permutations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./sample_funnel.png&#34; alt=&#34;sample_funnel&#34;&gt;
&lt;em&gt;Sample Funnel&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our hypothetical example, we&amp;rsquo;re looking at a funnel with three stages for a total of 6 permutations. Here&amp;rsquo;s the formula:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./permutation_formula.png&#34; alt=&#34;permutation_formula&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re using a sales/marketing funnel, you&amp;rsquo;ll have in mind what your funnel would look like so you may &lt;strong&gt;not&lt;/strong&gt; want all possible paths, but if you&amp;rsquo;re interested in exploring potentially &lt;em&gt;overlooked&lt;/em&gt; paths, read on.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the python 
&lt;a href=&#34;https://docs.python.org/3.6/library/itertools.html#itertools.permutations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; for &lt;code&gt;itertools&lt;/code&gt;, and &lt;code&gt;permutations&lt;/code&gt; specifically. We&amp;rsquo;ll break down the code to better understand what&amp;rsquo;s going on in this function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;note:&lt;/strong&gt; I found a clearer alternative after the fact. Feel free to skip to the final section below, although there is value in comparing the two versions.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll start off with the &lt;code&gt;iterable&lt;/code&gt; which is a &lt;code&gt;list&lt;/code&gt; with three strings. The &lt;code&gt;permutations&lt;/code&gt; function takes in two parameters, the &lt;code&gt;iterable&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt; which is the number of items from the list that we&amp;rsquo;re interested in finding the combination of. If we have three items in the list, we generally want to find &lt;em&gt;all possible&lt;/em&gt; combinations of those three items.&lt;/p&gt;
&lt;p&gt;Here is the code, and subsequent breakdown:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# list of length 3
list1 = [&#39;stage 1&#39;, &#39;stage 2&#39;, &#39;stage 3&#39;]

# iterable is the list
# r = number of items from the list to find combinations of


def permutations(iterable, r=None):
    &amp;quot;&amp;quot;&amp;quot;Find all possible order of a list of elements&amp;quot;&amp;quot;&amp;quot;
    # permutations(&#39;ABCD&#39;,2)--&amp;gt; AB AC AD BA BC BD CA CB CD DA DB DC
    # permutations(range(3))--&amp;gt; 012 021 102 120 201 210
    # permutations(list1, 6)--&amp;gt; ...720 permutations
    pool = tuple(iterable)
    n = len(pool)
    r = n if r is None else r
    if r &amp;gt; n:
        return
    indices = list(range(n))                     # [0, 1, 2]
    cycles = list(range(n, n-r, -1))             # [3, 2, 1]
    yield tuple(pool[i] for i in indices[:r])
    print(&amp;quot;Now entering while-loop \n&amp;quot;)
    while n:
        for i in reversed(range(r)):
            cycles[i] -= 1
            if cycles[i] == 0:
                indices[i:] = indices[i+1:] + indices[i:i+1]
                cycles[i] = n - i
            else:
                j = cycles[i]
                indices[i], indices[-j] = indices[-j], indices[i]
                yield tuple(pool[i] for i in indices[:r])
                print(&amp;quot;indices[:r]&amp;quot;, indices[:r])
                print(&amp;quot;pool[i]:&amp;quot;, tuple(pool[i] for i in indices[:r]))
                print(&amp;quot;n:&amp;quot;, n)
                break
        else:
            print(&amp;quot;return:&amp;quot;)
            return


#permutations(list1, 6)

perm = permutations(list1, 3)
count = 0

for p in perm:
    count += 1
    print(p)
print(&amp;quot;there are:&amp;quot;, count, &amp;quot;permutations.&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing we do is take the &lt;code&gt;iterable&lt;/code&gt; input parameter is turn it from a &lt;code&gt;list&lt;/code&gt; into a &lt;code&gt;tuple&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pool = tuple(iterable)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are several reasons to do this. First, &lt;code&gt;tuples&lt;/code&gt; are &lt;em&gt;faster&lt;/em&gt; than &lt;code&gt;lists&lt;/code&gt;; the &lt;code&gt;permutations()&lt;/code&gt; function will do several operations to the input so changing it to a &lt;code&gt;tuple&lt;/code&gt; allows faster operations and because &lt;code&gt;tuples&lt;/code&gt; are &lt;em&gt;immutable&lt;/em&gt;, we can do a bunch of different operations without fear that we might &lt;em&gt;inadvertently&lt;/em&gt; change the list.&lt;/p&gt;
&lt;p&gt;We then create &lt;code&gt;n&lt;/code&gt; from the length of &lt;code&gt;pool&lt;/code&gt; (in our case it&amp;rsquo;s 3) and the additional &lt;code&gt;r&lt;/code&gt; parameter, which defaults to &lt;code&gt;None&lt;/code&gt; is also 3 as we&amp;rsquo;re interested in seeing &lt;strong&gt;all combinations&lt;/strong&gt; of a list of three elements.&lt;/p&gt;
&lt;p&gt;We also have a line that ensures that &lt;code&gt;r&lt;/code&gt; can never be greater than the number of elements in the &lt;code&gt;iterable&lt;/code&gt; (list).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if r &amp;gt; n:
    return
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create &lt;code&gt;indices&lt;/code&gt; and &lt;code&gt;cycles&lt;/code&gt;. Indices are basically the index of each item, starting with 0 to 2, for three items. Cycles uses &lt;code&gt;range(n, n-r, -1)&lt;/code&gt;, which in our case is &lt;code&gt;range(3, 3-3, -1)&lt;/code&gt;; this means &lt;strong&gt;start&lt;/strong&gt; at three and &lt;strong&gt;end&lt;/strong&gt; at zero, in -1 &lt;strong&gt;steps&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The next chunk of code is a &lt;code&gt;while-loop&lt;/code&gt; that will continue for the length of the list, &lt;code&gt;n&lt;/code&gt; (note the &lt;code&gt;break&lt;/code&gt; at the bottom to exit out of this loop).&lt;/p&gt;
&lt;p&gt;After each &lt;code&gt;if-else&lt;/code&gt; cycle, a new set of &lt;code&gt;indices&lt;/code&gt; are created, which then gets looped through with &lt;code&gt;pool&lt;/code&gt;, the interable parameter input, which changes the order of the elements in the list.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll note in the commented code above, &lt;code&gt;cycles&lt;/code&gt; start off at [3,2,1] and &lt;code&gt;indices&lt;/code&gt; start off at [0,1,2]. Each loop through the code changes the &lt;code&gt;indices&lt;/code&gt; where &lt;code&gt;indices[i:]&lt;/code&gt; successively gets longer [2], then [1,2], then [1,2,3]. While &lt;code&gt;cycles&lt;/code&gt; changes as it trends toward [1,1,1], which point the code breaks out of the loop.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;while n:
        for i in reversed(range(r)):
            cycles[i] -= 1
            if cycles[i] == 0:
                indices[i:] = indices[i+1:] + indices[i:i+1]
                cycles[i] = n - i
            else:
                j = cycles[i]
                indices[i], indices[-j] = indices[-j], indices[i]
                yield tuple(pool[i] for i in indices[:r])
                print(&amp;quot;indices[:r]&amp;quot;, indices[:r])
                print(&amp;quot;pool[i]:&amp;quot;, tuple(pool[i] for i in indices[:r]))
                print(&amp;quot;n:&amp;quot;, n)
                break
        else:
            print(&amp;quot;return:&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;permutations(iterable, r)&lt;/code&gt; function actually creates a &lt;code&gt;generator&lt;/code&gt; so we need to loop through it again to print out all the permutations of the list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;lt;generator object permutations at 0x7fe19400fdd0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We add another for-loop at the bottom to print out all the permutations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;perm = permutations(list1, 3)
count = 0

for p in perm:
    count += 1
    print(p)
print(&amp;quot;there are:&amp;quot;, count, &amp;quot;permutations.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is our result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./permutations.png&#34; alt=&#34;permutations&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;a-clearer-alternative-permutation-using-recursion&#34;&gt;A Clearer Alternative: Permutation Using Recursion&lt;/h3&gt;
&lt;p&gt;As is often the case, there is a better way I found in retrospect from 
&lt;a href=&#34;https://stackoverflow.com/questions/104420/how-to-generate-all-permutations-of-a-list&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this stack overflow&lt;/a&gt; (h/t to 
&lt;a href=&#34;https://twitter.com/lebigot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eric O Lebigot&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def all_perms(elements):
    if len(elements) &amp;lt;= 1:
        yield elements  # Only permutation possible = no permutation
    else:
        # Iteration over the first element in the result permutation:
        for (index, first_elmt) in enumerate(elements):
            other_elmts = elements[:index] + elements[index+1:]
            for permutation in all_perms(other_elmts):
                yield [first_elmt] + permutation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;enumerate&lt;/code&gt; built-in function obviates the need to separately create &lt;code&gt;cycles&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt;. The local variable &lt;code&gt;other_elmts&lt;/code&gt; separates the other elements in the list from the &lt;code&gt;first_elmt&lt;/code&gt;, then the second for-loop recursively finds the permutation of the other elements before adding with the &lt;code&gt;first_elmt&lt;/code&gt; on the final line, yielding all possible permutations of a list. As with the previous case, the result of this function is a &lt;code&gt;generator&lt;/code&gt; which requires looping through and printing the permutations.&lt;/p&gt;
&lt;p&gt;I found this much easier to digest than the documentation version.&lt;/p&gt;
&lt;p&gt;Permutations can be useful when you have varied user journeys through your product and you want to figure out all the possible paths. With this short python script, you can easily print out all options for consideration.&lt;/p&gt;
&lt;h3 id=&#34;take-aways&#34;&gt;Take Aways&lt;/h3&gt;
&lt;p&gt;From the perspective of a user funnel, &lt;strong&gt;permutations&lt;/strong&gt; allow us to explore all possible &lt;em&gt;paths&lt;/em&gt; a user might take. For our hypothetical example, a three-step funnel yields six possible paths a user could navigate from start to finish.&lt;/p&gt;
&lt;p&gt;Knowing permutations should also &lt;strong&gt;give us pause&lt;/strong&gt; when deciding whether to add another &amp;ldquo;step&amp;rdquo; to a funnel. Going from a three-step funnel to a four-step funnel increases the number of possible paths from six to 24 - a quadruple increase.&lt;/p&gt;
&lt;p&gt;Not only does this increase &lt;strong&gt;friction&lt;/strong&gt; between your user and the &amp;lsquo;end goal&amp;rsquo; (conversion), whatever that may be for your product, but it also increases complexity (and potentially confusion) in the user experience.&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science from Scratch (ch6) - Probability</title>
      <link>/post/dsfs_6/</link>
      <pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_6/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#challenge&#34;&gt;Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#marginal_and_joint_probabilities&#34;&gt;Marginal and Joint Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#conditional_probability&#34;&gt;Conditional Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#bayes_theorem&#34;&gt;Bayes&amp;rsquo; Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#applying_bayes_theorem&#34;&gt;Applying Bayes&amp;rsquo; Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#distributions&#34;&gt;Distributions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h2 id=&#34;challenge&#34;&gt;Challenge&lt;/h2&gt;
&lt;p&gt;The first challenge in this section is distinguishing between &lt;strong&gt;two&lt;/strong&gt; conditional probability statements.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the setup. We have a family with two (unknown) children with two assumptions. First, each child is equally likely to be a boy or a girl. Second, the gender of the second child is &lt;em&gt;independent&lt;/em&gt; of the gender of the first child.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for statement one is roughly 50% or (1/2).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for statement two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;But at first glance, they look similar.&lt;/p&gt;
&lt;h2 id=&#34;marginal_and_joint_probabilities&#34;&gt;Marginal_and_Joint_Probabilities&lt;/h2&gt;
&lt;p&gt;The book jumps straight to conditional probabilities, but first, we&amp;rsquo;ll have to look at &lt;strong&gt;marginal&lt;/strong&gt; and &lt;strong&gt;joint&lt;/strong&gt; probabilities. Then we&amp;rsquo;ll create a &lt;strong&gt;joint probabilities table&lt;/strong&gt; and &lt;strong&gt;sum&lt;/strong&gt; probabilities to help us figure out the differences. We&amp;rsquo;ll then &lt;em&gt;resume&lt;/em&gt; with &lt;strong&gt;conditional probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Before anything, we need to realize the situation we have is one of &lt;strong&gt;independence&lt;/strong&gt;. The gender of one child is &lt;strong&gt;independent&lt;/strong&gt; of a second child.&lt;/p&gt;
&lt;p&gt;The intuition for this scenario will be different from a &lt;strong&gt;dependent&lt;/strong&gt; situation. For example, if we draw two cards from a deck (without replacement), the probabilities are different. The probability of drawing one King ♠️ is (4/52) and the probability of drawing a second King ♣️ is now (3/51); the probability of the second event (a second King) is &lt;em&gt;dependent&lt;/em&gt; on the result of the first draw.&lt;/p&gt;
&lt;p&gt;Ok back to the two unknown children.&lt;/p&gt;
&lt;p&gt;We can say the probability of the first child being either a boy or a girl is 50/50. Moreover, the probability of the second child, which is &lt;strong&gt;independent&lt;/strong&gt; of the first, is &lt;em&gt;also&lt;/em&gt; 50/50. Remember, our first assumption is that &lt;em&gt;each child is equally likely to be a boy or a girl&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s put these numbers in a table. The (1/2) probabilities shown here are called &lt;strong&gt;marginal&lt;/strong&gt; probabilities (note how they&amp;rsquo;re at the margins of the table).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./marginal.png&#34; alt=&#34;marginal&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since we have two gender (much like two sides of a flipped coin), we can intuitively figure out &lt;em&gt;all&lt;/em&gt; possible outcomes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;first child (Boy), second child (Boy)&lt;/li&gt;
&lt;li&gt;first child (Boy), second child (Girl)&lt;/li&gt;
&lt;li&gt;first child (Girl), second child (Boy)&lt;/li&gt;
&lt;li&gt;first child (Girl), second child (Girl)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are &lt;em&gt;4 possible outcomes&lt;/em&gt; so the probability of getting any one of the four outcomes is (1/4). We can actually write these probabilities in the middle of the table, the &lt;strong&gt;joint probabilities&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./joint.png&#34; alt=&#34;joint&#34;&gt;&lt;/p&gt;
&lt;p&gt;To recap, the probability of the first child being either boy or girl is 50/50, simple enough. The probability of the second child being either boy or girl is also 50/50. When put in a table, this yielded the &lt;strong&gt;marginal probability&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now we want to know the probability of say, &amp;lsquo;first child being a boy and second child being a girl&amp;rsquo;. This is a &lt;strong&gt;joint probability&lt;/strong&gt; because is is the probability that the first child take a specific gender (boy) &lt;strong&gt;AND&lt;/strong&gt; the second child take a specific gender (girl).&lt;/p&gt;
&lt;p&gt;If two event are &lt;strong&gt;independent&lt;/strong&gt;, and in this case they are, their &lt;strong&gt;joint probabilities&lt;/strong&gt; are the &lt;em&gt;product&lt;/em&gt; of the probabilities of &lt;strong&gt;each one happening&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The probability of the first child being a Boy (1/2) &lt;strong&gt;and&lt;/strong&gt; second child being a Girl (1/2); The product of each marginal probability is the joint probability (1/2 * 1/2 = 1/4).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./product_marginal.png&#34; alt=&#34;product_marginal&#34;&gt;&lt;/p&gt;
&lt;p&gt;This can be repeated for the other three joint probabilities.&lt;/p&gt;
&lt;h2 id=&#34;conditional_probability&#34;&gt;Conditional_Probability&lt;/h2&gt;
&lt;p&gt;Now we get into &lt;strong&gt;conditional probability&lt;/strong&gt; which is the probability of one event happening (i.e., second child being a Boy or Girl) &lt;strong&gt;given that&lt;/strong&gt; or &lt;strong&gt;on conditional that&lt;/strong&gt; another event happened (i.e., first child being a Boy).&lt;/p&gt;
&lt;p&gt;At this point, it might be a good idea to get familiar with notation.&lt;/p&gt;
&lt;p&gt;A joint probability is the product of each individual event happening (assuming they are independent events). For example we might have two individual events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy): 1/2&lt;/li&gt;
&lt;li&gt;P(2nd Child = Boy): 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is their &lt;strong&gt;joint probability&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy, 2nd Child = Boy) =&amp;gt;&lt;/li&gt;
&lt;li&gt;P(1st Child = Boy) * P(2nd Child = Boy) =&amp;gt;&lt;/li&gt;
&lt;li&gt;(1/2 * 1/2 = 1/4)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is a relationship between &lt;strong&gt;conditional&lt;/strong&gt; probabilities and &lt;strong&gt;joint&lt;/strong&gt; probabilities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy | 2nd Child = Boy) = P(1st Child = Boy, 2nd Child = Boy) / P(2nd Child = Boy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Namely, the &lt;strong&gt;conditional&lt;/strong&gt; probability is equal to the &lt;strong&gt;joint&lt;/strong&gt; probability divided by the conditional.&lt;/p&gt;
&lt;p&gt;Thie works out to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy | 2nd Child = Boy) = (1/4) / (1/2)
or&lt;/li&gt;
&lt;li&gt;(1/4) * (2/1)
= 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, the probability that the second child is a boy, given that the first child is a boy is &lt;em&gt;still&lt;/em&gt; 50% (this implies that with respect to &lt;strong&gt;conditional&lt;/strong&gt; probability, if the events are &lt;strong&gt;independent&lt;/strong&gt; it is not different from a single event).&lt;/p&gt;
&lt;p&gt;Now we&amp;rsquo;re ready to tackle the two challenges posed at the beginning of this post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Challenge 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s break it down. First we want the probability of the event that &amp;ldquo;both children are girls&amp;rdquo;. We&amp;rsquo;ll take the product of two events; the probability that the first child is a girl (1/2) and the probability that the second child is a girl (1/2). So the  &lt;strong&gt;joint probability of both&lt;/strong&gt; child being girls is 1/2 * 1/2 = 1/4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl, 2nd Child = Girl) = 1/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Second, we want that to be &lt;strong&gt;given that&lt;/strong&gt; the &amp;ldquo;older child is a girl&amp;rdquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl) = 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conditional probability&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(Both Child = Girls | 1st Child = Girl) = P(1st Child = Girl, 2nd Child = Girl) / P(1st Child = Girl)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(Both Child = Girls | 1st Child = Girl) = (1/4) / (1/2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(1/4) * (2/1) = &lt;strong&gt;1/2&lt;/strong&gt; or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let&amp;rsquo;s break down the second challenge:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Challenge 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, we start with &amp;ldquo;both children are girls&amp;rdquo;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl, 2nd Child = Girl) = 1/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, we have &amp;ldquo;on condition that at least one of the children is a girl&amp;rdquo;. We&amp;rsquo;ll reference a &lt;strong&gt;joint probability table&lt;/strong&gt;. We see that when trying to figure out the probability that &amp;ldquo;at least one of the children is a girl&amp;rdquo;, we rule out the scenario where &lt;strong&gt;both&lt;/strong&gt; children are boys. The remaining 3 out of 4 probabilities, fit the condition.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./at_least.png&#34; alt=&#34;at least&#34;&gt;&lt;/p&gt;
&lt;p&gt;The probability of at least one children being a girl is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1/4) + (1/4) + (1/4) = 3/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So (introducing notation):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(B) = &amp;ldquo;probability of both child being girls&amp;rdquo; (i.e., 1st Child = Girl, 2nd Child = Girl)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(L) = &amp;ldquo;probability of at least one child being a girl&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|L) = P(B,L) / P(L)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|L) = (1/4) / (3/4) = (1/4) * (4/3) = &lt;strong&gt;1/3&lt;/strong&gt; or roughly &lt;strong&gt;33%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-take-away&#34;&gt;Key Take-away&lt;/h4&gt;
&lt;p&gt;When two events are &lt;strong&gt;independent&lt;/strong&gt;, their &lt;strong&gt;joint probability&lt;/strong&gt; is the product of each event:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(E,F) = P(E) * P(F)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Their &lt;strong&gt;conditional&lt;/strong&gt; probability is the &lt;strong&gt;joint probability&lt;/strong&gt; divided by the conditional (i.e., P(F)).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(E|F) = P(E,F) / P(F)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And so for our two challenge scenarios, we have:&lt;/p&gt;
&lt;p&gt;Challenge 1:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;B = probability that both children are girls&lt;/li&gt;
&lt;li&gt;G = probability that the &lt;em&gt;older&lt;/em&gt; children is a girl&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This can be stated as: P(B|G) = P(B,G) / P(G)&lt;/p&gt;
&lt;p&gt;Challenge 2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;B = probability that both children are girls&lt;/li&gt;
&lt;li&gt;L = probability that &lt;em&gt;at least one&lt;/em&gt; children is a girl&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This can be stated as: P(B|L) = P(B,L) / P(L)&lt;/p&gt;
&lt;h4 id=&#34;python-code&#34;&gt;Python Code&lt;/h4&gt;
&lt;p&gt;Now that we have an intuition and have worked out the problem on paper, we can use code to express conditional probability:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import enum, random
class Kid(enum.Enum):
    BOY = 0
    GIRL = 1
    
def random_kid() -&amp;gt; Kid:
    return random.choice([Kid.BOY, Kid.GIRL])
    
both_girls = 0
older_girl = 0
either_girl = 0

random.seed(0)
for _ in range(10000):
    younger = random_kid()
    older = random_kid()
    if older == Kid.GIRL:
        older_girl += 1
    if older == Kid.GIRL and younger == Kid.GIRL:
        both_girls += 1
    if older == Kid.GIRL or younger == Kid.GIRL:
        either_girl += 1
        
print(&amp;quot;P(both | older):&amp;quot;, both_girls / older_girl)   # 0.5007089325501317
print(&amp;quot;P(both | either):&amp;quot;, both_girls / either_girl) # 0.3311897106109325
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that code confirms our intuition.&lt;/p&gt;
&lt;p&gt;We use a &lt;code&gt;for-loop&lt;/code&gt; and &lt;code&gt;range(10000)&lt;/code&gt; to randomly simulate 10,000 scenarios. The &lt;code&gt;random_kid&lt;/code&gt; function randomly picks either a boy or girl (assumption #1). We set the following variables to start a 0, &lt;code&gt;both_girls&lt;/code&gt; (both children are girls); &lt;code&gt;older_girl&lt;/code&gt; (first child is a girl); and &lt;code&gt;either_girl&lt;/code&gt; (at least one child is a girl).&lt;/p&gt;
&lt;p&gt;Then, each of these variables are incremented by 1 through each of the 10,000 loops if it meets certain conditions. After we finish looping, we can call on each of the three variables to see if they match our calculations above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;either_girl #7,464 / 10,000 ~ roughly 75% or 3/4 probability that there is at least one girl
both_girls  #2,472 / 10,000 ~ roughly 25% or 1/4 probability that both children are girls
older_girl  #4,937 / 10,000 ~ roughly 50% or 1/2 probability that the first child is a girl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will look at Bayes Theorem next.&lt;/p&gt;
&lt;h2 id=&#34;bayes_theorem&#34;&gt;Bayes_Theorem&lt;/h2&gt;
&lt;p&gt;Previously, we established an understanding of &lt;strong&gt;conditional&lt;/strong&gt; probability, but building up with &lt;strong&gt;marginal&lt;/strong&gt; and &lt;strong&gt;joint&lt;/strong&gt; probabilities. We explored the conditional probabilities of two outcomes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome one is roughly 50% or (1/2).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayes&amp;rsquo; Theorem&lt;/strong&gt; is simply &lt;em&gt;an alternate&lt;/em&gt; way of calculating conditional probability.&lt;/p&gt;
&lt;p&gt;Previously, we used the &lt;strong&gt;joint&lt;/strong&gt; probability to calculate the &lt;strong&gt;conditional&lt;/strong&gt; probability.&lt;/p&gt;
&lt;h3 id=&#34;outcome-1&#34;&gt;Outcome 1&lt;/h3&gt;
&lt;p&gt;Here&amp;rsquo;s the conditional probability for outcome 1, using a joint probability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(G) = &amp;lsquo;Probability that first child is a girl&amp;rsquo; (1/2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B) = &amp;lsquo;Probability that both children are girls&amp;rsquo; (1/4)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|G) = P(B,G) / P(G)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|G) =  (1/4) / (1/2) = &lt;strong&gt;1/2&lt;/strong&gt; or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Technically, we &lt;em&gt;can&amp;rsquo;t&lt;/em&gt; use joint probability because the two events are &lt;em&gt;not independent&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To clarify, the probability of the older child being a certain gender and the probability of the younger child being a certain gender &lt;em&gt;is&lt;/em&gt; independent, but &lt;code&gt;P(B|G)&lt;/code&gt; the &amp;lsquo;probability of &lt;em&gt;both&lt;/em&gt; child being a girl&amp;rsquo; and &amp;lsquo;the probability of the older child being a girl&amp;rsquo; are &lt;em&gt;not independent&lt;/em&gt;; and hence we express it as a &lt;em&gt;conditional&lt;/em&gt; probability.&lt;/p&gt;
&lt;p&gt;So, the joint probability of &lt;code&gt;P(B,G)&lt;/code&gt; is just event B,&lt;code&gt;P(B)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an alternate way to calculate the conditional probability (&lt;strong&gt;without&lt;/strong&gt; joint probability):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(B|G) = P(G|B) * P(B) / P(G)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;P(B|G) = 1 * (1/4) / (1/2)&lt;/li&gt;
&lt;li&gt;P(B|G) = (1/4) * (2/1)&lt;/li&gt;
&lt;li&gt;P(B|G) = 1/2 = &lt;strong&gt;50%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: P(G|B) is &amp;lsquo;the probability that the first child is a girl, given that &lt;strong&gt;both&lt;/strong&gt; children are girls is a certainty (1.0)&amp;rsquo;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reverse&lt;/strong&gt; conditional probability, can also be calculated, without joint probability:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability of the older child being a girl, given that both children are girls?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(G|B) = P(B|G) * P(G) / P(B)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem (reverse case)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;P(G|B) = (1/2) * (1/2) / (1/4)&lt;/li&gt;
&lt;li&gt;P(G|B) = (1/4) / (1/4)&lt;/li&gt;
&lt;li&gt;P(G|B) = 1 = &lt;strong&gt;100%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is consistent with what we already derived above, namely that P(G|B) is a &lt;strong&gt;certainty&lt;/strong&gt; (probability = 1.0), that the older child is a girl, &lt;strong&gt;given that&lt;/strong&gt; both children are girls.&lt;/p&gt;
&lt;p&gt;We can point out two additional observations / rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;While, joint probabilities are &lt;strong&gt;symmetrical&lt;/strong&gt;: P(B,G) == P(G,B),&lt;/li&gt;
&lt;li&gt;Conditional probabilities are &lt;strong&gt;not symmetrical&lt;/strong&gt;: P(B|G) != P(G|B)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bayes-theorem-alternative-expression&#34;&gt;Bayes&amp;rsquo; Theorem: Alternative Expression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt; is a way of calculating conditional probability &lt;em&gt;without&lt;/em&gt; the joint probability, summarized here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(B|G) = P(G|B) * P(B) / P(G)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(G|B) = P(B|G) * P(G) / P(B)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem (reverse case)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You&amp;rsquo;ll note that &lt;code&gt;P(G)&lt;/code&gt; is the denominator in the former, and &lt;code&gt;P(B)&lt;/code&gt; is the denominator in the latter.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What if, for some reasons, we don&amp;rsquo;t have access to the denominator?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We could derive both &lt;code&gt;P(G)&lt;/code&gt; and &lt;code&gt;P(B)&lt;/code&gt; in another way using the &lt;code&gt;NOT&lt;/code&gt; operator:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(G) = P(G,B) + P(G,not B) = P(G|B) * P(B) + P(G|not B) * P(not B)&lt;/li&gt;
&lt;li&gt;P(B) = P(B,G) + P(B,not G) = P(B|G) * P(G) + P(B|not G) * P(not G)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the alternative expression of Bayes Theorem for the probability of &lt;em&gt;both&lt;/em&gt; children being girls, given that the first child is a girl ( P(B|G) ) is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|G) = P(G|B) * P(B) / ( P(G|B) * P(B) + P(G|not B) * P(not B) )&lt;/li&gt;
&lt;li&gt;P(B|G) =     1 * 1/4 / (1 * 1/4 + 1/3 * 3/4)&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/4  /  (1/4 + 3/12)&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/4  /  2/4  =  1/4 * 4/2&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/2 or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can check the result in code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bayes_theorem(p_b, p_g_given_b, p_g_given_not_b):
   # calculate P(not B)
   not_b = 1 - p_b
   # calculate P(G)
   p_g = p_g_given_b * p_b + p_g_given_not_b * not_b
   # calculate P(B|G)
   p_b_given_g = (p_g_given_b * p_b) / p_g
   return p_b_given_g
   
#P(B)
p_b = 1/4

# P(G|B)
p_g_given_b = 1

# P(G|notB)
p_g_given_not_b = 1/3

# calculate P(B|G)
result = bayes_theorem(p_b, p_g_given_b, p_g_given_not_b)

# print result
print(&#39;P(B|G) = %.2f%%&#39; % (result * 100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the probability that the first child is a girl, given that &lt;em&gt;both&lt;/em&gt; children are girls ( P(G|B) ) is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(G|B) = P(B|G) * P(G) / ( P(G|B) * P(G) + P(B|not G) * P(not G) )&lt;/li&gt;
&lt;li&gt;P(G|B) =   1/2 * 1/2  / ((1/2 * 1/2) + (0 * 1/2))&lt;/li&gt;
&lt;li&gt;P(G|B) =  1/4  /  1/4&lt;/li&gt;
&lt;li&gt;P(G|B) = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s unpack Outcome 2.&lt;/p&gt;
&lt;h3 id=&#34;outcome-2&#34;&gt;Outcome 2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll go through the same process as above.&lt;/p&gt;
&lt;p&gt;We could use &lt;strong&gt;joint&lt;/strong&gt; probability to calculate the &lt;strong&gt;conditional&lt;/strong&gt; probability. As with the previous outcome, the joint probability of &lt;code&gt;P(B,G)&lt;/code&gt; is just event B,&lt;code&gt;P(B)&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(B,L) / P(L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Or, we could use Bayes&amp;rsquo; Theorem to figure out the &lt;strong&gt;conditional&lt;/strong&gt; probability &lt;strong&gt;without joint&lt;/strong&gt; probability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(L|B) * P(B) / P(L)&lt;/li&gt;
&lt;li&gt;P(B|L) =  (1 * 1/4) / (3/4)&lt;/li&gt;
&lt;li&gt;P(B|L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And, if there&amp;rsquo;s no &lt;code&gt;P(L)&lt;/code&gt;, we can calculate that indirectly, also using Bayes&amp;rsquo; Theorem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(L) = P(L|B) * P(B) + P(L|not B) * P(not B)&lt;/li&gt;
&lt;li&gt;P(L) =  1 * (1/4) + (2/3) * (3/4)&lt;/li&gt;
&lt;li&gt;P(L) =  (1/4) + (2/4)&lt;/li&gt;
&lt;li&gt;P(L) = 3/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, we can use &lt;code&gt;P(L)&lt;/code&gt; in the way Bayes&amp;rsquo; Theorem is commonly expressed, when we don&amp;rsquo;t have the denominator:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(L|B) * P(B) / ( P(L|B) * P(B) + P(L|not B) * P(not B) )&lt;/li&gt;
&lt;li&gt;P(B|L) =  1 * (1/4) / (3/4)&lt;/li&gt;
&lt;li&gt;P(B|L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we&amp;rsquo;ve gone through the calculation for two conditional probabilities, &lt;code&gt;P(B|G)&lt;/code&gt; and &lt;code&gt;P(B|L)&lt;/code&gt;, using Bayes Theorem, and implemented code for one of the scenarios, let&amp;rsquo;s take a step back and assess what this &lt;em&gt;means&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bayesian-terminology&#34;&gt;Bayesian Terminology&lt;/h3&gt;
&lt;p&gt;I think its useful to understand that probability in general shines when we want to describe uncertainty and that Bayes&amp;rsquo; Theorem allows us to quantify how much the data we observe, should change our beliefs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes_table.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have two &lt;strong&gt;posteriors&lt;/strong&gt;, &lt;code&gt;P(B|G)&lt;/code&gt; and &lt;code&gt;P(B|L)&lt;/code&gt;, both with equal &lt;strong&gt;priors&lt;/strong&gt; and &lt;strong&gt;likelihood&lt;/strong&gt;, but with &lt;em&gt;different&lt;/em&gt; &lt;strong&gt;evidence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Said differently, we want to know the &amp;lsquo;probability that both children are girls`, given &lt;em&gt;different&lt;/em&gt; conditions.&lt;/p&gt;
&lt;p&gt;In the first case, our condition is &amp;lsquo;the first child is a girl&amp;rsquo; and in the second case, our condition is &amp;lsquo;&lt;em&gt;at least one&lt;/em&gt; of the child is a girl&amp;rsquo;. The question is which condition will increase the probability that &lt;strong&gt;both&lt;/strong&gt; children are girls?&lt;/p&gt;
&lt;p&gt;Bayes&amp;rsquo; Theorem allows us to update our belief about the probability in these two cases, as we incorporate varied data into our framework.&lt;/p&gt;
&lt;p&gt;What the calculations tell us is that the &lt;strong&gt;evidence&lt;/strong&gt; that &amp;lsquo;one child is a girl&amp;rsquo; increases the probability that &lt;strong&gt;both&lt;/strong&gt; children are girls &lt;em&gt;more than&lt;/em&gt; the other piece of &lt;strong&gt;evidence&lt;/strong&gt; that &amp;lsquo;at least one child is a girl&amp;rsquo; increases that probability.&lt;/p&gt;
&lt;p&gt;And our beliefs should be updated accordingly.&lt;/p&gt;
&lt;p&gt;At the end of the day, understanding conditional probability (and Bayes Theorem) comes down to &lt;strong&gt;counting&lt;/strong&gt;. For our hypothetical scenarios, we only need one hand:&lt;/p&gt;
&lt;p&gt;When we look at the probability table for outcome one, &lt;code&gt;P(B|G)&lt;/code&gt;, we can see how the posterior probability comes out to 1/2:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./outcome_one.png&#34; alt=&#34;outcome_one&#34;&gt;&lt;/p&gt;
&lt;p&gt;When we look at the probability table for outcome two, &lt;code&gt;P(B|L)&lt;/code&gt;, we can see how the posterior probability comes out to 1/3:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./outcome_two.png&#34; alt=&#34;outcome_two&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is part of an ongoing series documenting my progress through Data Science from Scratch by Joel Grus:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./conditional_prob_ch6.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;applying_bayes_theorem&#34;&gt;Applying_Bayes_Theorem&lt;/h2&gt;
&lt;p&gt;Now that we have a basic understanding of Bayes Theorem, let&amp;rsquo;s extend the application to a slightly more complex example. This section was inspired by this 
&lt;a href=&#34;https://twitter.com/3blue1brown/status/1333121058824613889?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweet&lt;/a&gt; from Grant Sanderson (of 
&lt;a href=&#34;https://www.youtube.com/watch?v=HZGCoVF3YvM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown fame&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./grant_tweet.png&#34; alt=&#34;grant_tweet&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is a classic application of Bayes Theorem - the &lt;strong&gt;medical diagnostic scenario&lt;/strong&gt;. The above tweet can be re-stated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability of you &lt;em&gt;actually having the disease&lt;/em&gt;, given that you tested positive?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This happens to be even more relevant as we&amp;rsquo;re living through a generational pandemic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start off with a conceptual understanding, using the tools we learned previously. First, we have to keep in mind &lt;strong&gt;testing&lt;/strong&gt; and &lt;strong&gt;actually having the disease&lt;/strong&gt; are &lt;strong&gt;not independent&lt;/strong&gt; events. Therefore, we will use &lt;strong&gt;conditional probability&lt;/strong&gt; to express their joint outcomes.&lt;/p&gt;
&lt;p&gt;The intuitive visual to illustrate this is the &lt;strong&gt;tree diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./initial_tree.png&#34; alt=&#34;initial_tree&#34;&gt;&lt;/p&gt;
&lt;p&gt;The initial given information contains the information in the tree.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(D): Probability of having the disease (covid-19)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(P): Probability of testing positive&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;*P(D|P): Our objective is to find the probability of having the disease, given a positive test&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1 in 1,000 actively have covid-19, P(D), this implies&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;999 in 1,000 do &lt;strong&gt;not&lt;/strong&gt; actively have covid-19, P(not D)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1% or 0.01 false positive (given)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;10% or 0.1 false negative (given)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;false positive&lt;/strong&gt; is when you &lt;em&gt;don&amp;rsquo;t&lt;/em&gt; have the disease, but your test (in error) shows up positive. &lt;strong&gt;False negative&lt;/strong&gt; is when you &lt;em&gt;have&lt;/em&gt; the disease, but your test (in error) shows up negative. We are provided this information and have to calculate other values to fill in the tree.&lt;/p&gt;
&lt;p&gt;We know that all possible events have to add up to 1, so if 1 in 1,000 actively have the disease, we know that 999 in 1,000 do not have it. If the false negative is 10%, then the &lt;strong&gt;true positive&lt;/strong&gt; is 90%. If the false positive is 1%, then the &lt;strong&gt;true negative&lt;/strong&gt; is 99%. From our calculations, the tree can be updated:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./second_tree.png&#34; alt=&#34;second_tree&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we&amp;rsquo;ve filled out the tree, we can use &lt;strong&gt;Bayes&amp;rsquo; Theorem&lt;/strong&gt; to find &lt;code&gt;P(D|P)&lt;/code&gt;. Here&amp;rsquo;s Bayes&amp;rsquo; Theorem that we discussed in the previous section. We have Bayes&amp;rsquo; Theorem, the denominator, probability of testing positive &lt;code&gt;P(P)&lt;/code&gt; and the &lt;em&gt;second&lt;/em&gt; version of Bayes Theorem in cases were we &lt;em&gt;do not know&lt;/em&gt; the probability of testing positive (as in the present case):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes1.png&#34; alt=&#34;bayes1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then we can plug-in the denominator to get the alternative version of Bayes&amp;rsquo; Theorem:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes2.png&#34; alt=&#34;bayes2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how the numbers add up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(D|P) = P(P|D) * P(D) / P(P|D) * P(D) + P(P|not D) * P(not D)&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.9 * 0.001 / 0.9 * 0.001 + 0.01 * 0.999&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.0009 / 0.0009 + 0.00999&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.0009 / 0.01089&lt;/li&gt;
&lt;li&gt;P(D|P) ~ 0.08264 or 8.26%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interestingly, 
&lt;a href=&#34;https://twitter.com/karpathy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy&lt;/a&gt; actually 
&lt;a href=&#34;https://twitter.com/karpathy/status/1333217287155847169?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;responded in the thread&lt;/a&gt; and provided an intuitive way to arrive at the same result using Python.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s his code (with added comments):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from random import random, seed
seed(0)

pop = 10000000 # 10M people
counts = {}

for i in range(pop):
    has_covid = i % 1000 == 0 # one in 1,000 people have covid (priors or prevalence of disease)
    # The major assumption is that every person gets tested regardless of any symptoms
    if has_covid:                  # Has disease
        tests_positive = True      # True positive
        if random() &amp;lt; 0.1:     
            tests_positive = False # False negative
    else:                          # Does not have disease
        tests_positive = False     # True negative
        if random() &amp;lt; 0.01:    
            tests_positive = True  # False positive
    outcome = (has_covid, tests_positive)
    counts[outcome] = counts.get(outcome, 0) + 1
    
for (has_covid, tested_positive), n in counts.items():
    print(&#39;has covid: %6s, tests positive: %6s, count: %d&#39; % (has_covid, tested_positive, n))
    
n_positive = counts[(True, True)] + counts[(False, True)]

print(&#39;number of people who tested positive:&#39;, n_positive)
print(&#39;probability that a test-positive person actually has covid: %.2f&#39; % (100.0 * counts[(True, True)] / n_positive), )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first build a hypothetical population of 10 million. If the &lt;strong&gt;prior&lt;/strong&gt; or &lt;strong&gt;prevalence&lt;/strong&gt; of disease is 1 in 1,000, a population of 10 million should find 10000 people with covid. You can see how this works with this short snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pop = 10000000
counts = 0

for i in range(pop):
    has_covid = i % 1000 == 0
    if has_covid:
        counts = counts + 1
print(counts, &amp;quot;people have the disease in a population of 10 million&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nested in the &lt;code&gt;for-loop&lt;/code&gt; are &lt;code&gt;if-statements&lt;/code&gt; that segment the population (10M) into one of four categories True Positive, False Negative, True Negative, False Positive. Each category is counted and stored in a &lt;code&gt;dict&lt;/code&gt; called &lt;code&gt;counts&lt;/code&gt;. Then another &lt;code&gt;for-loop&lt;/code&gt; is used to loop through this dictionary to print out all the categories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;has covid:   True, tests positive:   True, count: 9033
has covid:  False, tests positive:  False, count: 9890133
has covid:  False, tests positive:   True, count: 99867
has covid:   True, tests positive:  False, count: 967

number of people who tested positive: 108900
probability that a test-positive person actually has covid: 8.29
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we want the number of people who &lt;em&gt;have&lt;/em&gt; the disease &lt;em&gt;and&lt;/em&gt; tested positive (True Positive, 9033) divided by the number of people who tested positive, regardless of whether they actually have the disease (True Positive (9033) + False Positive (99867) = 108,900) and this comes out to approximately 8.29.&lt;/p&gt;
&lt;p&gt;Although the 
&lt;a href=&#34;https://twitter.com/karpathy/status/1333217287155847169?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; was billed as &amp;ldquo;simple code to build intuition&amp;rdquo;, I found that Bayes&amp;rsquo; Theorem &lt;em&gt;is&lt;/em&gt; the intuition.&lt;/p&gt;
&lt;h3 id=&#34;what-about-symptoms&#34;&gt;What about symptoms?&lt;/h3&gt;
&lt;p&gt;The key to Bayes&amp;rsquo; Theorem is that it encourages us to update our beliefs when presented with new evidence. But what if there&amp;rsquo;s evidence we missed in the first place?&lt;/p&gt;
&lt;p&gt;If you look back at the 
&lt;a href=&#34;https://twitter.com/3blue1brown/status/1333121058824613889?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original tweet&lt;/a&gt;, there are important details about symptoms that, if we wanted to be more realistic, should be accounted for.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You feel fatigued and have a slight sore throat.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, instead of assuming that prevalence of the disease (1 in 1,000 people have covid-19) is the prior, we might ask what probability that someone who is symptomatic has the disease?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s suppose we change from 1 in 1,000 to 1 in 100. We could change just one line of code (while everything else remains the same):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(pop):
    has_covid = i % 100 == 0 # update info: 1/1000 have covid, but 1/100 with symptoms have covid
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability that someone with a positive test actually has the disease jumps from 8.29% to 47.61%&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;has covid:   True, tests positive:   True, count: 180224
has covid:  False, tests positive:  False, count: 19601715
has covid:  False, tests positive:   True, count: 198285
has covid:   True, tests positive:  False, count: 19776
number of people who tested positive: 378509
probability that a test-positive person with symptoms actually has covid: 47.61
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, being symptomatic means our &lt;strong&gt;priors&lt;/strong&gt; should be adjusted and our &lt;strong&gt;beliefs&lt;/strong&gt; about the likelihood that a positive test means we have the disease (&lt;code&gt;P(D|P)&lt;/code&gt;) should be updated accordingly (in this case, it goes way up).&lt;/p&gt;
&lt;h3 id=&#34;take-aways&#34;&gt;Take Aways&lt;/h3&gt;
&lt;p&gt;Hypothetically, if we have family or friends living in an area where 1 in 1,000 people have covid-19 and they (god forbid) got tested and got a positive result, you could tell them that their probability of actually having the disease, given a positive test was around 8.26–8.29%.&lt;/p&gt;
&lt;p&gt;However, what’s useful about the Bayesian approach is that it encourages us to incorporate new information and update our beliefs accordingly. So if we find out our family or friend is also &lt;em&gt;symptomatic&lt;/em&gt;, we could advise them of the higher probability (~47.61%).&lt;/p&gt;
&lt;p&gt;Finally, we may also advise our family/friends to get tested &lt;strong&gt;again&lt;/strong&gt;, because as much as test-positive person would hope they got a ‘false positive’, chances are low. And even lower, is getting a false positive &lt;em&gt;twice&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./second_test.png&#34; alt=&#34;second_test&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll cover various distributions. This is a broad topic so we&amp;rsquo;ll sample a few concepts to get a feel for it. Borrowing from the previous post, we&amp;rsquo;ll chart our medical diagnostic outcomes.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll recall that each outcome is the combination of whether someone has a disease, &lt;code&gt;P(D)&lt;/code&gt;, or not, &lt;code&gt;P(not D)&lt;/code&gt;. Then, they&amp;rsquo;re given a diagnostic test that returns positive, &lt;code&gt;P(P)&lt;/code&gt; or negative, &lt;code&gt;P(not P)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;These are discrete outcomes so they can be represented with the &lt;strong&gt;probability mass function&lt;/strong&gt;, as opposed to a &lt;strong&gt;probability density function&lt;/strong&gt;, which represent a continuous distribution.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take another &lt;em&gt;hypothetical&lt;/em&gt; scenario of a city where 1 in 10 people have a disease and a diagnostic test has a True Positive of 95% and True Negative of 90%. The probability that a test-positive person &lt;em&gt;actually&lt;/em&gt; having the disease is 46.50%.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from random import random, seed

seed(0)
pop = 1000  # 1000 people
counts = {}
for i in range(pop):
    has_disease = i % 10 == 0  # one in 10 people have disease
    # assuming that every person gets tested regardless of any symptoms
    if has_disease:
        tests_positive = True       # True Positive  95%
        if random() &amp;lt; 0.05:
            tests_positive = False  # False Negative 5%
    else:
        tests_positive = False      # True Negative  90%
        if random() &amp;lt; 0.1:
            tests_positive = True   # False Positive 10%
    outcome = (has_disease, tests_positive)
    counts[outcome] = counts.get(outcome, 0) + 1

for (has_disease, tested_positive), n in counts.items():
    print(&#39;Has Disease: %6s, Test Positive: %6s, count: %d&#39; %
          (has_disease, tested_positive, n))

n_positive = counts[(True, True)] + counts[(False, True)]
print(&#39;Number of people who tested positive:&#39;, n_positive)
print(&#39;Probability that a test-positive person actually has disease: %.2f&#39; %
      (100.0 * counts[(True, True)] / n_positive),)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given the probability that someone has the disease (1 in 10), also called the &amp;lsquo;prior&amp;rsquo; in Bayesian terms. We modeled four scenarios where people were given a diagnostic test. Again, the big assumption here is that people get randomly tested. With the true positive and true negative rates stated above, here are the outcomes:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./hypothetical_outcome.png&#34; alt=&#34;hypothetical_outcome&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;probability-mass-function&#34;&gt;Probability Mass Function&lt;/h3&gt;
&lt;p&gt;Given these discrete events, we can chart a &lt;strong&gt;probability mass function&lt;/strong&gt;, also known as 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_mass_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discrete density function&lt;/a&gt;. We&amp;rsquo;ll import &lt;code&gt;pandas&lt;/code&gt; to help us create &lt;code&gt;DataFrames&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; to chart the &lt;strong&gt;probability mass function&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We first need to turn the counts of events into a &lt;code&gt;DataFrame&lt;/code&gt; and change the column to &lt;code&gt;item_counts&lt;/code&gt;. Then, we&amp;rsquo;ll calculate the probability of each event by dividing the count by the total number of people in our hypothetical city (i.e., population: 1000).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optional&lt;/strong&gt;: Create another column with abbreviations for test outcome (i.e., &amp;ldquo;True True&amp;rdquo; becomes &amp;ldquo;TT&amp;rdquo;). We&amp;rsquo;ll call this column &lt;code&gt;item2&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame.from_dict(counts, orient=&#39;index&#39;)
df = df.rename(columns={0: &#39;item_counts&#39;})
df[&#39;probability&#39;] = df[&#39;item_counts&#39;]/1000
df[&#39;item2&#39;] = [&#39;TT&#39;, &#39;FF&#39;, &#39;FT&#39;, &#39;TF&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the &lt;code&gt;DataFrame&lt;/code&gt; we have so far:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./pmf_df.png&#34; alt=&#34;pmf_df&#34;&gt;&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll note that the numbers in the &lt;code&gt;probability&lt;/code&gt; column adds up to 1.0 and that the &lt;code&gt;item_counts&lt;/code&gt; numbers are the same as the count above when we had calculated the probability of a test-positive person actually having the disease.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll use a simple bar chart to chart out the diagnostic probabilities and this is how we&amp;rsquo;d visually represent the probability mass function - probabilities of each discrete event; each &amp;lsquo;discrete event&amp;rsquo; is a conditional (e.g., probability that someone has a positive test, given that they &lt;em&gt;have&lt;/em&gt; the disease - TT or probability that someone has a negative test, given that they &lt;em&gt;don&amp;rsquo;t have&lt;/em&gt; the disease - FF, and so on).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./prob_mass_function.png&#34; alt=&#34;prob_mass_function.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.DataFrame.from_dict(counts, orient=&#39;index&#39;)
df = df.rename(columns={0: &#39;item_counts&#39;})
df[&#39;probability&#39;] = df[&#39;item_counts&#39;]/1000
df[&#39;item2&#39;] = [&#39;TT&#39;, &#39;FF&#39;, &#39;FT&#39;, &#39;TF&#39;]
plt.bar(df[&#39;item2&#39;], df[&#39;probability&#39;])
plt.title(&amp;quot;Probability Mass Function&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;cumulative-distribution-function&#34;&gt;Cumulative Distribution Function&lt;/h3&gt;
&lt;p&gt;While the probability mass function can tell us the probability of each discrete event (i.e., TT, FF, FT, and TF) we can also represent the same information as a &lt;strong&gt;cumulative distribution function&lt;/strong&gt; which allows us to see how the probability changes as we add events together.&lt;/p&gt;
&lt;p&gt;The cumulative distribution function simply adds the probability from the previous row in a &lt;code&gt;DataFrame&lt;/code&gt; in a cumulative fashion, like in the column &lt;code&gt;probability2&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cdf_df.png&#34; alt=&#34;cdf_df.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;cumsum()&lt;/code&gt; function to create the &lt;code&gt;cumsum&lt;/code&gt; column which is simply adding the &lt;code&gt;item_counts&lt;/code&gt;, with each successive row. When we create the corresponding probability column, &lt;code&gt;probability2&lt;/code&gt;, it gets larger until we reach 1.0.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cum_distri_function.png&#34; alt=&#34;cum_distri_function&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart tells us that the probability of getting both TT and FF (True, True = True Positive, and False, False = True Negative) is 88.6% which indicates that 11.4% (100 - 88.6) of the time, the diagnostic test will let us down.&lt;/p&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;p&gt;More often than not, you&amp;rsquo;ll be interested in &lt;em&gt;continuous&lt;/em&gt; distributions and you can see better see how the &lt;strong&gt;cumulative distribution function&lt;/strong&gt; works.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;re probably familiar with the bell shaped curve or the &lt;em&gt;normal distribution&lt;/em&gt;, defined solely by its mean (mu) and standard deviation (sigma). If you have a &lt;strong&gt;standard normal distribution&lt;/strong&gt; of probability values, the average would be 0 and the standard deviation would be 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./1_normal.png&#34; alt=&#34;1_normal&#34;&gt;&lt;/p&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math
SQRT_TWO_PI = math.sqrt(2 * math.pi)

def normal_pdf(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (SQRT_TWO_PI * sigma))
    
# plot
xs = [x / 10.0 for x in range(-50, 50)]
plt.plot(xs, [normal_pdf(x, sigma=1) for x in xs], &#39;-&#39;, label=&#39;mu=0, sigma=1&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;strong&gt;standard normal distribution&lt;/strong&gt; curve, you see the average probability is around 0.4. But if you add up the area under the curve (i.e., all probabilities of every possible outcome), you would get 1.0, just like with the medical diagnostic example.&lt;/p&gt;
&lt;p&gt;And if you split the bell in half, then flip over the left half, you&amp;rsquo;ll (visually) get the &lt;strong&gt;cumulative distribution function&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./1_cumu.png&#34; alt=&#34;1_cumu&#34;&gt;&lt;/p&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math

def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2
    
# plot
xs = [x / 10.0 for x in range(-50, 50)]
plt.plot(xs, [normal_cdf(x, sigma=1) for x in xs], &#39;-&#39;, label=&#39;mu=0,sigma=1&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In both cases, the area under the curve for the &lt;strong&gt;standard normal distribution&lt;/strong&gt; and the &lt;strong&gt;cumulative distribution function&lt;/strong&gt; is 1.0, thus summing the probabilities of all events is one.&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
