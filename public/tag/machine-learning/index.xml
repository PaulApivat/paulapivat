<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Paul Apivat</title>
    <link>/tag/machine-learning/</link>
      <atom:link href="/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Paul Apivat Hanvongse. All Rights Reserved.</copyright><lastBuildDate>Tue, 22 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Machine Learning</title>
      <link>/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Gradient Descent -- Data Science from Scratch (ch8)</title>
      <link>/post/dsfs_8/</link>
      <pubDate>Tue, 22 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_8/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#setup&#34;&gt;Setup&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#gradient_descent&#34;&gt;Gradient Descent&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#take_away&#34;&gt;Take Away&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll explore Gradient Descent from the ground up starting conceptually, then using code to build up our intuition brick by brick.&lt;/p&gt;
&lt;p&gt;While this post is part of an ongoing series where I document my progress through 
&lt;a href=&#34;https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science from Scratch by Joel Grus&lt;/a&gt;, for this post I am drawing on external sources including Aurélien Geron&amp;rsquo;s Hands-On Machine Learning to provide a context for why and when gradient descent is used.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll also be using external libraries such as &lt;code&gt;numpy&lt;/code&gt;, that are generally avoided in Data Science from Scratch, to help highlight concepts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; While the book introduces gradient descent as a standalone topic, I find it more intuitive to reason about it within the context of a regression problem.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In any modeling exercise, there is error (because no model is perfect), and our objective is minimize the errors so that when we develop models from our training data, we&amp;rsquo;ll have some confidence that the predictions will work in testing and completely new data.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll train a &lt;em&gt;linear regression model&lt;/em&gt;. Our &lt;em&gt;training data&lt;/em&gt; will only have three data points. We train the model by setting up parameters (slope &amp;amp; intercept) that best &amp;ldquo;fits&amp;rdquo; the data (i.e., best-fitting line), for example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./best_fit_line2.png&#34; alt=&#34;best fit line2&#34;&gt;&lt;/p&gt;
&lt;p&gt;We know the values for both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, so we can calculate the slope and intercept directly through the &lt;strong&gt;normal equation&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Normal Equation

import numpy as np
import matplotlib.pyplot as plt

x = np.array([2, 4, 5])
y = np.array([45, 85, 105])

# computing Normal Equation
x_b = np.c_[np.ones((3, 1)), x]       # add x0 = 1 to each of three instances
theta = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)

# array([ 5., 20.])
theta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key line is &lt;code&gt;np.linalg.inv()&lt;/code&gt; which computes the multiplicative inverse of a matrix.&lt;/p&gt;
&lt;p&gt;Our slope is 20 and intercept is 5 (&lt;code&gt;theta&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: We could also have used the more familiar &amp;ldquo;rise over run&amp;rdquo; ((85 - 45) / (4 - 2)) or (40/2) or 20, but we want to illustrate the &lt;strong&gt;normal equation&lt;/strong&gt; which should come in handy when we go beyond the simplistic three data point example.&lt;/p&gt;
&lt;p&gt;We could also use the &lt;code&gt;LinearRegression&lt;/code&gt; class from &lt;code&gt;sklearn&lt;/code&gt; to call the least squares (&lt;code&gt;np.linalg.lstsq()&lt;/code&gt;) function directly:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Linear Regression

from sklearn.linear_model import LinearRegression
import numpy as np

x = np.array([2, 4, 5])
y = np.array([45, 85, 105])

x = x.reshape(-1, 1)              # reshape because sklearn expect 2D array

x_b = np.c_[np.ones((3, 1)), x]   # add x0 = 1 to each of three instances

theta, residuals, rank, s = np.linalg.lstsq(x_b, y, rcond=1e-6)

# array([ 5., 20.])
print(&amp;quot;theta:&amp;quot;, theta)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This appraoch also yields the slope (20) and intercept (5) directly.&lt;/p&gt;
&lt;p&gt;Again we should note that we know the parameters of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; in our example, but we want to see how &lt;strong&gt;learning from data&lt;/strong&gt; would work. Here&amp;rsquo;s the equation we&amp;rsquo;re working with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;y = 20 * x + 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here&amp;rsquo;s what it looks like (intercept = 5, slope = 20)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./known_x_y.png&#34; alt=&#34;known x y&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;gradient_descent&#34;&gt;Gradient_Descent&lt;/h2&gt;
&lt;h4 id=&#34;why-gradient-descent&#34;&gt;Why Gradient Descent?&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;normal equation&lt;/strong&gt; and the &lt;strong&gt;least squares&lt;/strong&gt; (linear regresion) approach can handle large training sets efficiently, but when your model has a &lt;strong&gt;large number of features&lt;/strong&gt; or too &lt;strong&gt;many training instances&lt;/strong&gt; to fit into memory, &lt;strong&gt;gradient descent&lt;/strong&gt; is an often used alternative.&lt;/p&gt;
&lt;p&gt;Moreover, linear least squares assume the errors have a normal distribution and the relationship in the data is linear (this is where closed-form solutions like the &lt;strong&gt;normal equation&lt;/strong&gt; excel). When the data is non-linear, an iterative solution (gradient descent) can be used.&lt;/p&gt;
&lt;p&gt;With linear regression we seek to &lt;strong&gt;minimize the sum-of-squares&lt;/strong&gt; differences between the observed data and the predicted values (aka the error), in a &lt;strong&gt;non-iterative&lt;/strong&gt; fashion.&lt;/p&gt;
&lt;p&gt;Similar to the regression, we are seeking to use gradient descent to find the slope and intercept that minimizes the average squared error, however we do so in an &lt;strong&gt;interative fashion&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;using-gradient-descent-to-fit-a-model&#34;&gt;Using Gradient Descent to Fit a Model&lt;/h4&gt;
&lt;p&gt;The process for gradient descent is to start with a &lt;strong&gt;random slope and intercept&lt;/strong&gt;, then compute the &lt;strong&gt;gradient&lt;/strong&gt; of the &lt;strong&gt;mean squared error&lt;/strong&gt;, while adjusting the slope/intercept (&lt;code&gt;theta&lt;/code&gt;) in the direction that continues to minimize the error. This is repeated iteratively until we find a point where errors are minimized.&lt;/p&gt;
&lt;p&gt;As with the &lt;strong&gt;normal equation&lt;/strong&gt; and &lt;strong&gt;linear regression&lt;/strong&gt; approach above, we technically &lt;em&gt;know&lt;/em&gt; the parameters of the linear relationship between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, but we&amp;rsquo;ll pretend like we don&amp;rsquo;t and need to learn from the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This section builds heavily on a previous post on linear algebra. You&amp;rsquo;ll want to 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;read this post&lt;/a&gt; to get a feel for the functions used to construct the functions we see in this post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import TypeVar, List, Iterator
import math
import random
import matplotlib.pyplot as plt
from typing import Callable
from typing import List
import numpy as np

x = np.array([2, 4, 5])

# instead of putting y directly, we&#39;ll use the equation: 20 * x + 5, which is a direct representation of its relationship to x

# y = np.array([45, 85, 105])   

# both x and y are represented in inputs
inputs = [(x, 20 * x + 5) for x in range(2, 6)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we&amp;rsquo;ll start with random values for the slope and intercept; we&amp;rsquo;ll also establish a learning rate, which controls how much a change in the model is warranted in response to the estimated error each time the model parameters, slope and intercept, are updated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 1. start with a random value for slope and intercept
theta = [random.uniform(-1, 1), random.uniform(-1, 1)]

learning_rate = 0.001
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we&amp;rsquo;ll compute the mean of the gradients, then adjust the slope/intercept in the direction of minimizing the gradient, which is based on the error.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll note that this for-loop has 100 iterations. The more interations we go through, the more that errors are minimized and the more we approach a slope/intercept where the model &amp;ldquo;fits&amp;rdquo; the data better.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for epoch in range(100):     # start with 100
    # compute the mean of the gradients
    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])
    # take a step in that direction
    theta = gradient_step(theta, grad, -learning_rate)
    print(epoch, grad, theta)

slope, intercept = theta

#assert 19.9 &amp;lt; slope &amp;lt; 20.1,  &amp;quot;slope should be about 20&amp;quot;
#assert 4.9 &amp;lt; intercept &amp;lt; 5.1, &amp;quot;intercept should be about 5&amp;quot;
print(&amp;quot;slope&amp;quot;, slope)
print(&amp;quot;intercept&amp;quot;, intercept)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;iterative-descent&#34;&gt;Iterative Descent&lt;/h4&gt;
&lt;p&gt;At 100 iterations, the slope is 18.87 and intercept is 4.87 and the gradient is -32.48 (error for the slope) and -8.45 (error for the intercept). These numbers suggest that we need to decrease the slope and intercept from our random starting point, but our emphasis needs to be on decreasing the slope.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./100_iterations.png&#34; alt=&#34;100 iterations&#34;&gt;&lt;/p&gt;
&lt;p&gt;At 200 iterations, the slope is 19.97 and intercept is 4.86 and the gradient is -1.76 (error for the slope) and -0.48 (error for the intercept). Our errors have been reduced significantly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./200_iterations.png&#34; alt=&#34;200 iterations&#34;&gt;&lt;/p&gt;
&lt;p&gt;At 1000 iterations, the slope is 19.97 (not much difference from 200 iterations) and intercept is 5.09 and the gradients are markedly lower at -0.004 (error for the slope) and 0.02 (error for the intercept). Here the errors may not be much different from zero and we are near our optimal point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./1000_iterations.png&#34; alt=&#34;1000 iterations&#34;&gt;&lt;/p&gt;
&lt;p&gt;Through this process, we can note a couple things. The sign in front of the gradient moving from -32.48 to -0.004 for the slope tells us that with each iteration, the slope should go down from where it started, randomly.&lt;/p&gt;
&lt;p&gt;Similarly we see for the 100th and 200th iterations, the sign for the intercept was negative, suggesting that it should also go down from it&amp;rsquo;s random starting point. Howeer, at the 1000th iteration, the sign changed to positive (+ 0.02) suggesting an ever so slight increase in intercept, so it appears we found the point of minimal error for the intercept&amp;rsquo;s gradient.&lt;/p&gt;
&lt;p&gt;In summary, the &lt;strong&gt;normal equation&lt;/strong&gt; and &lt;strong&gt;regression&lt;/strong&gt; approaches gave us a slope of 20 and intercept of 5. With gradient descent, we approached these values with each successive iterations, 1000 iterations yielding &lt;strong&gt;less error&lt;/strong&gt; than 100 or 200 iterations.&lt;/p&gt;
&lt;h4 id=&#34;linear-algebra-foundations&#34;&gt;Linear Algebra foundations&lt;/h4&gt;
&lt;p&gt;As mentioned above, the functions used to compute the gradients and adjust the slope/intercept build on functions we explored in 
&lt;a href=&#34;https://paulapivat.com/post/dsfs_4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt;. Here&amp;rsquo;s a visual showing how the functions we used to iteratively arrive at the slope and intercept through gradient descent was built:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./ch8_funct.png&#34; alt=&#34;ch8_funct&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;take_away&#34;&gt;Take_Away&lt;/h2&gt;
&lt;p&gt;In any modeling process we are &amp;ldquo;fitting&amp;rdquo; a model to the data to see which model fits better. A better fit yields better predictions. Models are evaluated by their error (also known as cost function) - the distance between the predicted value and the actual data points.&lt;/p&gt;
&lt;p&gt;With linear regression, there are two &lt;em&gt;different&lt;/em&gt; ways of training the model. First, is to use an equation that directly computes model parameters to fit the model to the training data (while minimizing errors or &amp;lsquo;cost function&amp;rsquo;); this is the Normal Equation. Second is to use Gradient Descent.&lt;/p&gt;
&lt;p&gt;Gradient Descent is useful you are expecting computational complexity due to the number of features or training instances.&lt;/p&gt;
&lt;p&gt;This post is part of an ongoing series where I document my progress through 
&lt;a href=&#34;https://joelgrus.com/2019/05/13/data-science-from-scratch-second-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science from Scratch by Joel Grus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./book_disclaimer.png&#34; alt=&#34;book_disclaimer&#34;&gt;&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>End-to-End Projects</title>
      <link>/post/end_to_end/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/end_to_end/</guid>
      <description>&lt;h2 id=&#34;2021-goals&#34;&gt;2021 Goals&lt;/h2&gt;
&lt;p&gt;One of my goals for 2021 is to build up a portfolio of end-to-end machine learning projects. In this post, I&amp;rsquo;ll keep a running list of resources for inspiration:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.kdnuggets.com/2020/10/guide-authentic-data-science-portfolio-project.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science Portfolio Projects: A Step-by-Step Guide&lt;/a&gt; (by 
&lt;a href=&#34;https://www.linkedin.com/in/felix-vemmer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Felix Vemmer&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;This is a clear step-by-step guide. I like the emphasis on web scraping which is where I need to focus my skills on next.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;bit.ly/berkeleyfsdl&#34;&gt;Full Stack Deep Learning (at Berkeley)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This looks to be a promising course that covers: &amp;ldquo;a promising experiment to a shipped product: project structure, useful tooling, data management, best practices for deployment, social responsibility, and finding a job or starting a venture&amp;rdquo;. The course is &lt;strong&gt;entirely online&lt;/strong&gt;. See this 
&lt;a href=&#34;https://twitter.com/full_stack_dl/status/1329477077733609480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweet thread&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://madewithml.com/courses/applied-ml-in-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied ML in Production&lt;/a&gt; by 
&lt;a href=&#34;https://twitter.com/GokuMohandas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goku Mohandas&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This aims to be a &amp;ldquo;guide and code-driven case study on MLOps for software engineers, data scientists and product managers&amp;hellip;developing an end-to-end ML feature, from product &amp;ndash;&amp;gt; ML &amp;ndash;&amp;gt; production, with open source tools&amp;rdquo;. Sounds very promising.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://end-to-end-machine-learning.teachable.com/p/complete-course-library-full-end-to-end-machine-learning-catalog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;End-to-End Machine Learning Course Catalog&lt;/a&gt; by 
&lt;a href=&#34;https://twitter.com/_brohrer_&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brandon Rohrer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/PrasoonPratham/status/1330372876134912000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;First 30 days of Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tweet thread by 
&lt;a href=&#34;https://twitter.com/PrasoonPratham&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pratham Prasoon&lt;/a&gt;, as the title suggests, is for newcomers to ML, but I think by the end of the sequence (doesn&amp;rsquo;t have to be 30 days) there&amp;rsquo;s a Kaggle project to complete. &lt;em&gt;note&lt;/em&gt;: this is not ML-in-production like some of the other resources, but Kaggle projects are great for learning.&lt;/p&gt;
&lt;p&gt;He has another thread 
&lt;a href=&#34;https://twitter.com/PrasoonPratham/status/1325331515090219008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;worth checking out&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jangiacomelli/status/1331170945738760192&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Suggested Project from Jan Giacomelli&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a pretty 🔥 thread. He suggests:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Build an expense tracker CLI app:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each expensee should have the following: title (string), amount(float), created_at(date), tags(list of strings)&lt;/p&gt;
&lt;p&gt;2 Add Database&lt;/p&gt;
&lt;p&gt;Instead of storing/reading in/from TXT file, start using SQLite. Write script to copy all of the existing expenses from TXT file to database. Don&amp;rsquo;t use ORM at this point.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Start using Classes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Represent expense with class Expense having attributes: title(string), amount(float), created_at(date), tags(list of strings).&lt;/p&gt;
&lt;p&gt;Represent Database with class ExpenseRepository with methods: save, get_by_id, list, delete&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Re-write App to use Commands and Queries&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each command/query is a class with method execute.
At initialization you need to provide all required data for execution.&lt;/p&gt;
&lt;p&gt;Commands: AddExpense, EditExpense
Queries: GetById, ListAll&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/modern-tdd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modern Test-Driven Development in Python&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Add Tests&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Add tests for commands and queries&lt;/p&gt;
&lt;p&gt;Example:
GIVEN Valid data
WHEN execute method is called on AddExpense command
THEN record is created in database with same attributes as provided&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/modern-tdd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modern Test-Driven Development in Python&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Flask&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use Flask to build the web application for your expense tracker.
Reuse commands and queries inside views
Use Jinja2 for HTML templating
Add integration tests for endpoints&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;PostgreSQL&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Start using PostgreSQL instead of SQLite.
You should only edit ExpenseRepository.
Create script to copy all existing data from SQLite to Postgres&lt;/p&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Add sign up and login to your Flask application
Protect endpoints for expenses to allow only logged in users to use them
Allow user to only see own expenses.&lt;/p&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;Dockerize and Deploy&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dockerize your Flask application
Deploy to Heroku (don&amp;rsquo;t use DB in docker, use it on Heroku)&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/dockerizing-flask-with-postgres-gunicorn-and-nginx/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dockerizing Flask with Postgres, Gunicorn and Nginx&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;Start using your application for real&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Start tracking your expenses
Even the most little ones
Don&amp;rsquo;t forget to add them daily&lt;/p&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;Data Analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use Pandas and Matplotlib to analyze your expenses
Check frequency, check biggest amount, smallest amount, average amount, most frequent amount and most used tags&amp;hellip;&lt;/p&gt;
&lt;p&gt;Draw plots: Number of expenses per day, amount spent per day&lt;/p&gt;
&lt;ol start=&#34;12&#34;&gt;
&lt;li&gt;ML&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Build model which will predict tags based on the title of expense
Use your existing records
Although your data set is small, try to build model as precise as possible&lt;/p&gt;
&lt;ol start=&#34;13&#34;&gt;
&lt;li&gt;Congratulate yourself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Don&amp;rsquo;t forget to write a blog post for each of these steps.
Don&amp;rsquo;t forget to share your code in a public git repository (GitHub)
Don&amp;rsquo;t forget to tweet it out
Don&amp;rsquo;t forget to add all the skills to LinkedIn&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine learning as a service</title>
      <link>/post/mlaas/</link>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/mlaas/</guid>
      <description>&lt;h2 id=&#34;preparing-api-endpoints-in-python-with-flask&#34;&gt;Preparing API endpoints in Python with Flask&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll create a minimal API endpoint that allows users to make request to calculate the area of a rectangle. The following code sets up an API endpoint locally. We&amp;rsquo;ll import &lt;code&gt;Flask&lt;/code&gt;, a lightweight web application framework and &lt;code&gt;CORS&lt;/code&gt; (cross-origin resource sharing) which allows for various HTTP requests.&lt;/p&gt;
&lt;p&gt;We have two endpoints, one basic &amp;ldquo;hello world&amp;rdquo; and the other calculate the area (i.e., width x height).&lt;/p&gt;
&lt;p&gt;This is saved in &lt;code&gt;App.py&lt;/code&gt;. The command to run this file is &lt;code&gt;$ python3 App.py&lt;/code&gt;. The last line ensures the API is running locally on &lt;code&gt;localhost:5000&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from flask import Flask, request
from flask_cors import CORS, cross_origin
import joblib
import numpy as np 

app = Flask(__name__)
CORS(app)

@app.route(&#39;/&#39;)
def helloworld():
    return &#39;Helloworld&#39;

# Example request: http://localhost:5000/area?w=50&amp;amp;h=3
@app.route(&#39;/area&#39;, methods=[&#39;GET&#39;])
@cross_origin()
def area():
    w = float(request.values[&#39;w&#39;])
    h = float(request.values[&#39;h&#39;])
    return str(w * h)

if __name__ == &#39;__main__&#39;:
    app.run(host=&#39;0.0.0.0&#39;, port=5000, debug=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can just run &lt;code&gt;localhost:5000&lt;/code&gt; and get &lt;code&gt;Helloworld&lt;/code&gt; or make a request to get the &lt;strong&gt;area&lt;/strong&gt;, for example: &lt;code&gt;http://localhost:5000/area?w=20&amp;amp;h=33&lt;/code&gt; (this yeilds 660)&lt;/p&gt;
&lt;h2 id=&#34;training-a-logistic-regression-classification-model&#34;&gt;Training a Logistic Regression classification model&lt;/h2&gt;
&lt;p&gt;After setting up some API endpoints, it&amp;rsquo;s time to create a basic machine learning model. We&amp;rsquo;ll create a logistic regression model to classify flowers from the &lt;strong&gt;Iris&lt;/strong&gt; dataset. This will be created in &lt;em&gt;one&lt;/em&gt; &lt;code&gt;jupyter notebook&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll load all required libraries.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we&amp;rsquo;ll load the Iris dataset that comes with scikit learn, &lt;code&gt;sklearn&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;iris = load_iris()

# assign two variables at once
X, y = iris[&#39;data&#39;], iris[&#39;target&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll reshape the data using &lt;code&gt;numpy&lt;/code&gt;, then split the data into training and validation sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# reshape data for logistic regression
dataset = np.hstack((X, y.reshape(-1,1)))
np.random.shuffle(dataset)

# split data into training, validation sets
X_train, X_test, y_train, y_test = train_test_split(dataset[:, :4],
                                                    dataset[:, 4],
                                                    test_size=0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll then fit a logistic regression model by fitting the training set to the validation set.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model = LogisticRegression()
model.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we&amp;rsquo;ll use the model to predict on the validation data (&lt;em&gt;note&lt;/em&gt;: in a real project a distinction is made between &lt;code&gt;validation&lt;/code&gt; and &lt;code&gt;testing&lt;/code&gt; sets, but we&amp;rsquo;ll blur that distinction for this demo). You can also test the model to make a prediction on a single observation.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s also a good idea to get the &lt;code&gt;accuracy_score()&lt;/code&gt;, although it may not be ideal for classification models.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# make a prediction
y_pred = model.predict(X_test)

# get accuracy score
accuracy_score(y_test, y_pred)

# make prediction on single Iris obervation
model.predict([[5.1, 3.5, 1.4, 0.2]])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we need to use &lt;code&gt;joblib&lt;/code&gt; to save an &lt;code&gt;iris.model&lt;/code&gt; to our directory, this will be used to connect to the API.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;joblib.dump(model, &#39;iris.model&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;creating-an-api-endpoint-for-the-logistic-regression-model&#34;&gt;Creating an API endpoint for the Logistic Regression model&lt;/h2&gt;
&lt;p&gt;Back in the &lt;code&gt;App.py&lt;/code&gt; file, we&amp;rsquo;ll &lt;em&gt;add&lt;/em&gt; this section to create an endpoint, the &lt;code&gt;predict_species()&lt;/code&gt; function that loads the &lt;code&gt;iris.model&lt;/code&gt;, then sends a Post request of the four parameter values from &lt;code&gt;iris[&#39;data&#39;]&lt;/code&gt;. The &lt;code&gt;predict_species()&lt;/code&gt; function will then return one of three flower species.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@app.route(&#39;/iris&#39;, methods=[&#39;POST&#39;])
@cross_origin()
def predict_species():
    model = joblib.load(&#39;iris.model&#39;)  #needs to be the correct path
    req = request.values[&#39;param&#39;]
    inputs = np.array(req.split(&#39;,&#39;), dtype=np.float32).reshape(1,-1)
    predict_target = model.predict(inputs)
    if predict_target == 0:
        return &#39;Setosa&#39;
    elif predict_target == 1:
        return &#39;Versicolor&#39;
    else:
        return &#39;Virginica&#39;

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;testing-the-api-endpoint-on-postman&#34;&gt;Testing the API endpoint on Postman&lt;/h2&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll use 
&lt;a href=&#34;https://www.postman.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Postman&lt;/a&gt;, a platform for API development. We will &lt;strong&gt;post&lt;/strong&gt; four parameters (i.e., sepal length, sepal width, petal length and petal width) to the API endpoint and expect to receive a name back, either Setosa, Versicolor or Virginica.
In Postman, we&amp;rsquo;ll create a new collection and a new request:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./postman.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The next step from here is to go beyond localhost and deploy the model. We&amp;rsquo;ll explore that in another post.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
