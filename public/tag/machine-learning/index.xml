<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Paul Apivat</title>
    <link>/tag/machine-learning/</link>
      <atom:link href="/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Paul Apivat Hanvongse. All Rights Reserved.</copyright><lastBuildDate>Sat, 21 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Machine Learning</title>
      <link>/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>End-to-End Projects</title>
      <link>/post/end_to_end/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/end_to_end/</guid>
      <description>&lt;h2 id=&#34;2021-goals&#34;&gt;2021 Goals&lt;/h2&gt;
&lt;p&gt;One of my goals for 2021 is to build up a portfolio of end-to-end machine learning projects. In this post, I&amp;rsquo;ll keep a running list of resources for inspiration:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.kdnuggets.com/2020/10/guide-authentic-data-science-portfolio-project.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science Portfolio Projects: A Step-by-Step Guide&lt;/a&gt; (by 
&lt;a href=&#34;https://www.linkedin.com/in/felix-vemmer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Felix Vemmer&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;This is a clear step-by-step guide. I like the emphasis on web scraping which is where I need to focus my skills on next.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;bit.ly/berkeleyfsdl&#34;&gt;Full Stack Deep Learning (at Berkeley)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This looks to be a promising course that covers: &amp;ldquo;a promising experiment to a shipped product: project structure, useful tooling, data management, best practices for deployment, social responsibility, and finding a job or starting a venture&amp;rdquo;. The course is &lt;strong&gt;entirely online&lt;/strong&gt;. See this 
&lt;a href=&#34;https://twitter.com/full_stack_dl/status/1329477077733609480&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweet thread&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://madewithml.com/courses/applied-ml-in-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied ML in Production&lt;/a&gt; by 
&lt;a href=&#34;https://twitter.com/GokuMohandas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Goku Mohandas&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This aims to be a &amp;ldquo;guide and code-driven case study on MLOps for software engineers, data scientists and product managers&amp;hellip;developing an end-to-end ML feature, from product &amp;ndash;&amp;gt; ML &amp;ndash;&amp;gt; production, with open source tools&amp;rdquo;. Sounds very promising.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://end-to-end-machine-learning.teachable.com/p/complete-course-library-full-end-to-end-machine-learning-catalog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;End-to-End Machine Learning Course Catalog&lt;/a&gt; by 
&lt;a href=&#34;https://twitter.com/_brohrer_&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brandon Rohrer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/PrasoonPratham/status/1330372876134912000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;First 30 days of Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tweet thread by 
&lt;a href=&#34;https://twitter.com/PrasoonPratham&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pratham Prasoon&lt;/a&gt;, as the title suggests, is for newcomers to ML, but I think by the end of the sequence (doesn&amp;rsquo;t have to be 30 days) there&amp;rsquo;s a Kaggle project to complete. &lt;em&gt;note&lt;/em&gt;: this is not ML-in-production like some of the other resources, but Kaggle projects are great for learning.&lt;/p&gt;
&lt;p&gt;He has another thread 
&lt;a href=&#34;https://twitter.com/PrasoonPratham/status/1325331515090219008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;worth checking out&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jangiacomelli/status/1331170945738760192&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Suggested Project from Jan Giacomelli&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a pretty ðŸ”¥ thread. He suggests:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Build an expense tracker CLI app:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each expensee should have the following: title (string), amount(float), created_at(date), tags(list of strings)&lt;/p&gt;
&lt;p&gt;2 Add Database&lt;/p&gt;
&lt;p&gt;Instead of storing/reading in/from TXT file, start using SQLite. Write script to copy all of the existing expenses from TXT file to database. Don&amp;rsquo;t use ORM at this point.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Start using Classes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Represent expense with class Expense having attributes: title(string), amount(float), created_at(date), tags(list of strings).&lt;/p&gt;
&lt;p&gt;Represent Database with class ExpenseRepository with methods: save, get_by_id, list, delete&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Re-write App to use Commands and Queries&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each command/query is a class with method execute.
At initialization you need to provide all required data for execution.&lt;/p&gt;
&lt;p&gt;Commands: AddExpense, EditExpense
Queries: GetById, ListAll&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/modern-tdd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modern Test-Driven Development in Python&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Add Tests&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Add tests for commands and queries&lt;/p&gt;
&lt;p&gt;Example:
GIVEN Valid data
WHEN execute method is called on AddExpense command
THEN record is created in database with same attributes as provided&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/modern-tdd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modern Test-Driven Development in Python&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Flask&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use Flask to build the web application for your expense tracker.
Reuse commands and queries inside views
Use Jinja2 for HTML templating
Add integration tests for endpoints&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;PostgreSQL&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Start using PostgreSQL instead of SQLite.
You should only edit ExpenseRepository.
Create script to copy all existing data from SQLite to Postgres&lt;/p&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Authentication&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Add sign up and login to your Flask application
Protect endpoints for expenses to allow only logged in users to use them
Allow user to only see own expenses.&lt;/p&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;Dockerize and Deploy&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dockerize your Flask application
Deploy to Heroku (don&amp;rsquo;t use DB in docker, use it on Heroku)&lt;/p&gt;
&lt;p&gt;See this post on 
&lt;a href=&#34;https://testdriven.io/blog/dockerizing-flask-with-postgres-gunicorn-and-nginx/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dockerizing Flask with Postgres, Gunicorn and Nginx&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;Start using your application for real&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Start tracking your expenses
Even the most little ones
Don&amp;rsquo;t forget to add them daily&lt;/p&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;Data Analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use Pandas and Matplotlib to analyze your expenses
Check frequency, check biggest amount, smallest amount, average amount, most frequent amount and most used tags&amp;hellip;&lt;/p&gt;
&lt;p&gt;Draw plots: Number of expenses per day, amount spent per day&lt;/p&gt;
&lt;ol start=&#34;12&#34;&gt;
&lt;li&gt;ML&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Build model which will predict tags based on the title of expense
Use your existing records
Although your data set is small, try to build model as precise as possible&lt;/p&gt;
&lt;ol start=&#34;13&#34;&gt;
&lt;li&gt;Congratulate yourself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Don&amp;rsquo;t forget to write a blog post for each of these steps.
Don&amp;rsquo;t forget to share your code in a public git repository (GitHub)
Don&amp;rsquo;t forget to tweet it out
Don&amp;rsquo;t forget to add all the skills to LinkedIn&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine learning as a service</title>
      <link>/post/mlaas/</link>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/mlaas/</guid>
      <description>&lt;h2 id=&#34;preparing-api-endpoints-in-python-with-flask&#34;&gt;Preparing API endpoints in Python with Flask&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll create a minimal API endpoint that allows users to make request to calculate the area of a rectangle. The following code sets up an API endpoint locally. We&amp;rsquo;ll import &lt;code&gt;Flask&lt;/code&gt;, a lightweight web application framework and &lt;code&gt;CORS&lt;/code&gt; (cross-origin resource sharing) which allows for various HTTP requests.&lt;/p&gt;
&lt;p&gt;We have two endpoints, one basic &amp;ldquo;hello world&amp;rdquo; and the other calculate the area (i.e., width x height).&lt;/p&gt;
&lt;p&gt;This is saved in &lt;code&gt;App.py&lt;/code&gt;. The command to run this file is &lt;code&gt;$ python3 App.py&lt;/code&gt;. The last line ensures the API is running locally on &lt;code&gt;localhost:5000&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from flask import Flask, request
from flask_cors import CORS, cross_origin
import joblib
import numpy as np 

app = Flask(__name__)
CORS(app)

@app.route(&#39;/&#39;)
def helloworld():
    return &#39;Helloworld&#39;

# Example request: http://localhost:5000/area?w=50&amp;amp;h=3
@app.route(&#39;/area&#39;, methods=[&#39;GET&#39;])
@cross_origin()
def area():
    w = float(request.values[&#39;w&#39;])
    h = float(request.values[&#39;h&#39;])
    return str(w * h)

if __name__ == &#39;__main__&#39;:
    app.run(host=&#39;0.0.0.0&#39;, port=5000, debug=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can just run &lt;code&gt;localhost:5000&lt;/code&gt; and get &lt;code&gt;Helloworld&lt;/code&gt; or make a request to get the &lt;strong&gt;area&lt;/strong&gt;, for example: &lt;code&gt;http://localhost:5000/area?w=20&amp;amp;h=33&lt;/code&gt; (this yeilds 660)&lt;/p&gt;
&lt;h2 id=&#34;training-a-logistic-regression-classification-model&#34;&gt;Training a Logistic Regression classification model&lt;/h2&gt;
&lt;p&gt;After setting up some API endpoints, it&amp;rsquo;s time to create a basic machine learning model. We&amp;rsquo;ll create a logistic regression model to classify flowers from the &lt;strong&gt;Iris&lt;/strong&gt; dataset. This will be created in &lt;em&gt;one&lt;/em&gt; &lt;code&gt;jupyter notebook&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll load all required libraries.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we&amp;rsquo;ll load the Iris dataset that comes with scikit learn, &lt;code&gt;sklearn&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;iris = load_iris()

# assign two variables at once
X, y = iris[&#39;data&#39;], iris[&#39;target&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll reshape the data using &lt;code&gt;numpy&lt;/code&gt;, then split the data into training and validation sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# reshape data for logistic regression
dataset = np.hstack((X, y.reshape(-1,1)))
np.random.shuffle(dataset)

# split data into training, validation sets
X_train, X_test, y_train, y_test = train_test_split(dataset[:, :4],
                                                    dataset[:, 4],
                                                    test_size=0.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll then fit a logistic regression model by fitting the training set to the validation set.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model = LogisticRegression()
model.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we&amp;rsquo;ll use the model to predict on the validation data (&lt;em&gt;note&lt;/em&gt;: in a real project a distinction is made between &lt;code&gt;validation&lt;/code&gt; and &lt;code&gt;testing&lt;/code&gt; sets, but we&amp;rsquo;ll blur that distinction for this demo). You can also test the model to make a prediction on a single observation.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s also a good idea to get the &lt;code&gt;accuracy_score()&lt;/code&gt;, although it may not be ideal for classification models.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# make a prediction
y_pred = model.predict(X_test)

# get accuracy score
accuracy_score(y_test, y_pred)

# make prediction on single Iris obervation
model.predict([[5.1, 3.5, 1.4, 0.2]])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we need to use &lt;code&gt;joblib&lt;/code&gt; to save an &lt;code&gt;iris.model&lt;/code&gt; to our directory, this will be used to connect to the API.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;joblib.dump(model, &#39;iris.model&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;creating-an-api-endpoint-for-the-logistic-regression-model&#34;&gt;Creating an API endpoint for the Logistic Regression model&lt;/h2&gt;
&lt;p&gt;Back in the &lt;code&gt;App.py&lt;/code&gt; file, we&amp;rsquo;ll &lt;em&gt;add&lt;/em&gt; this section to create an endpoint, the &lt;code&gt;predict_species()&lt;/code&gt; function that loads the &lt;code&gt;iris.model&lt;/code&gt;, then sends a Post request of the four parameter values from &lt;code&gt;iris[&#39;data&#39;]&lt;/code&gt;. The &lt;code&gt;predict_species()&lt;/code&gt; function will then return one of three flower species.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@app.route(&#39;/iris&#39;, methods=[&#39;POST&#39;])
@cross_origin()
def predict_species():
    model = joblib.load(&#39;iris.model&#39;)  #needs to be the correct path
    req = request.values[&#39;param&#39;]
    inputs = np.array(req.split(&#39;,&#39;), dtype=np.float32).reshape(1,-1)
    predict_target = model.predict(inputs)
    if predict_target == 0:
        return &#39;Setosa&#39;
    elif predict_target == 1:
        return &#39;Versicolor&#39;
    else:
        return &#39;Virginica&#39;

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;testing-the-api-endpoint-on-postman&#34;&gt;Testing the API endpoint on Postman&lt;/h2&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll use 
&lt;a href=&#34;https://www.postman.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Postman&lt;/a&gt;, a platform for API development. We will &lt;strong&gt;post&lt;/strong&gt; four parameters (i.e., sepal length, sepal width, petal length and petal width) to the API endpoint and expect to receive a name back, either Setosa, Versicolor or Virginica.
In Postman, we&amp;rsquo;ll create a new collection and a new request:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./postman.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The next step from here is to go beyond localhost and deploy the model. We&amp;rsquo;ll explore that in another post.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
