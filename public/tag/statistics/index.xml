<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics | Paul Apivat</title>
    <link>/tag/statistics/</link>
      <atom:link href="/tag/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Paul Apivat Hanvongse. All Rights Reserved.</copyright><lastBuildDate>Sun, 22 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Statistics</title>
      <link>/tag/statistics/</link>
    </image>
    
    <item>
      <title>Data Science from Scratch (ch6) - Probability</title>
      <link>/post/dsfs_6/</link>
      <pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/dsfs_6/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#challenge&#34;&gt;Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#marginal_and_joint_probabilities&#34;&gt;Marginal and Joint Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#conditional_probability&#34;&gt;Conditional Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#bayes_theorem&#34;&gt;Bayes&amp;rsquo; Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#applying_bayes_theorem&#34;&gt;Applying Bayes&amp;rsquo; Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#distributions&#34;&gt;Distributions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h2 id=&#34;challenge&#34;&gt;Challenge&lt;/h2&gt;
&lt;p&gt;The first challenge in this section is distinguishing between &lt;strong&gt;two&lt;/strong&gt; conditional probability statements.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the setup. We have a family with two (unknown) children with two assumptions. First, each child is equally likely to be a boy or a girl. Second, the gender of the second child is &lt;em&gt;independent&lt;/em&gt; of the gender of the first child.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for statement one is roughly 50% or (1/2).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for statement two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;But at first glance, they look similar.&lt;/p&gt;
&lt;h2 id=&#34;marginal_and_joint_probabilities&#34;&gt;Marginal_and_Joint_Probabilities&lt;/h2&gt;
&lt;p&gt;The book jumps straight to conditional probabilities, but first, we&amp;rsquo;ll have to look at &lt;strong&gt;marginal&lt;/strong&gt; and &lt;strong&gt;joint&lt;/strong&gt; probabilities. Then we&amp;rsquo;ll create a &lt;strong&gt;joint probabilities table&lt;/strong&gt; and &lt;strong&gt;sum&lt;/strong&gt; probabilities to help us figure out the differences. We&amp;rsquo;ll then &lt;em&gt;resume&lt;/em&gt; with &lt;strong&gt;conditional probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Before anything, we need to realize the situation we have is one of &lt;strong&gt;independence&lt;/strong&gt;. The gender of one child is &lt;strong&gt;independent&lt;/strong&gt; of a second child.&lt;/p&gt;
&lt;p&gt;The intuition for this scenario will be different from a &lt;strong&gt;dependent&lt;/strong&gt; situation. For example, if we draw two cards from a deck (without replacement), the probabilities are different. The probability of drawing one King ♠️ is (4/52) and the probability of drawing a second King ♣️ is now (3/51); the probability of the second event (a second King) is &lt;em&gt;dependent&lt;/em&gt; on the result of the first draw.&lt;/p&gt;
&lt;p&gt;Ok back to the two unknown children.&lt;/p&gt;
&lt;p&gt;We can say the probability of the first child being either a boy or a girl is 50/50. Moreover, the probability of the second child, which is &lt;strong&gt;independent&lt;/strong&gt; of the first, is &lt;em&gt;also&lt;/em&gt; 50/50. Remember, our first assumption is that &lt;em&gt;each child is equally likely to be a boy or a girl&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s put these numbers in a table. The (1/2) probabilities shown here are called &lt;strong&gt;marginal&lt;/strong&gt; probabilities (note how they&amp;rsquo;re at the margins of the table).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./marginal.png&#34; alt=&#34;marginal&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since we have two gender (much like two sides of a flipped coin), we can intuitively figure out &lt;em&gt;all&lt;/em&gt; possible outcomes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;first child (Boy), second child (Boy)&lt;/li&gt;
&lt;li&gt;first child (Boy), second child (Girl)&lt;/li&gt;
&lt;li&gt;first child (Girl), second child (Boy)&lt;/li&gt;
&lt;li&gt;first child (Girl), second child (Girl)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are &lt;em&gt;4 possible outcomes&lt;/em&gt; so the probability of getting any one of the four outcomes is (1/4). We can actually write these probabilities in the middle of the table, the &lt;strong&gt;joint probabilities&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./joint.png&#34; alt=&#34;joint&#34;&gt;&lt;/p&gt;
&lt;p&gt;To recap, the probability of the first child being either boy or girl is 50/50, simple enough. The probability of the second child being either boy or girl is also 50/50. When put in a table, this yielded the &lt;strong&gt;marginal probability&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now we want to know the probability of say, &amp;lsquo;first child being a boy and second child being a girl&amp;rsquo;. This is a &lt;strong&gt;joint probability&lt;/strong&gt; because is is the probability that the first child take a specific gender (boy) &lt;strong&gt;AND&lt;/strong&gt; the second child take a specific gender (girl).&lt;/p&gt;
&lt;p&gt;If two event are &lt;strong&gt;independent&lt;/strong&gt;, and in this case they are, their &lt;strong&gt;joint probabilities&lt;/strong&gt; are the &lt;em&gt;product&lt;/em&gt; of the probabilities of &lt;strong&gt;each one happening&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The probability of the first child being a Boy (1/2) &lt;strong&gt;and&lt;/strong&gt; second child being a Girl (1/2); The product of each marginal probability is the joint probability (1/2 * 1/2 = 1/4).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./product_marginal.png&#34; alt=&#34;product_marginal&#34;&gt;&lt;/p&gt;
&lt;p&gt;This can be repeated for the other three joint probabilities.&lt;/p&gt;
&lt;h2 id=&#34;conditional_probability&#34;&gt;Conditional_Probability&lt;/h2&gt;
&lt;p&gt;Now we get into &lt;strong&gt;conditional probability&lt;/strong&gt; which is the probability of one event happening (i.e., second child being a Boy or Girl) &lt;strong&gt;given that&lt;/strong&gt; or &lt;strong&gt;on conditional that&lt;/strong&gt; another event happened (i.e., first child being a Boy).&lt;/p&gt;
&lt;p&gt;At this point, it might be a good idea to get familiar with notation.&lt;/p&gt;
&lt;p&gt;A joint probability is the product of each individual event happening (assuming they are independent events). For example we might have two individual events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy): 1/2&lt;/li&gt;
&lt;li&gt;P(2nd Child = Boy): 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is their &lt;strong&gt;joint probability&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy, 2nd Child = Boy) =&amp;gt;&lt;/li&gt;
&lt;li&gt;P(1st Child = Boy) * P(2nd Child = Boy) =&amp;gt;&lt;/li&gt;
&lt;li&gt;(1/2 * 1/2 = 1/4)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is a relationship between &lt;strong&gt;conditional&lt;/strong&gt; probabilities and &lt;strong&gt;joint&lt;/strong&gt; probabilities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy | 2nd Child = Boy) = P(1st Child = Boy, 2nd Child = Boy) / P(2nd Child = Boy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Namely, the &lt;strong&gt;conditional&lt;/strong&gt; probability is equal to the &lt;strong&gt;joint&lt;/strong&gt; probability divided by the conditional.&lt;/p&gt;
&lt;p&gt;Thie works out to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Boy | 2nd Child = Boy) = (1/4) / (1/2)
or&lt;/li&gt;
&lt;li&gt;(1/4) * (2/1)
= 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, the probability that the second child is a boy, given that the first child is a boy is &lt;em&gt;still&lt;/em&gt; 50% (this implies that with respect to &lt;strong&gt;conditional&lt;/strong&gt; probability, if the events are &lt;strong&gt;independent&lt;/strong&gt; it is not different from a single event).&lt;/p&gt;
&lt;p&gt;Now we&amp;rsquo;re ready to tackle the two challenges posed at the beginning of this post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Challenge 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s break it down. First we want the probability of the event that &amp;ldquo;both children are girls&amp;rdquo;. We&amp;rsquo;ll take the product of two events; the probability that the first child is a girl (1/2) and the probability that the second child is a girl (1/2). So the  &lt;strong&gt;joint probability of both&lt;/strong&gt; child being girls is 1/2 * 1/2 = 1/4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl, 2nd Child = Girl) = 1/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Second, we want that to be &lt;strong&gt;given that&lt;/strong&gt; the &amp;ldquo;older child is a girl&amp;rdquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl) = 1/2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conditional probability&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(Both Child = Girls | 1st Child = Girl) = P(1st Child = Girl, 2nd Child = Girl) / P(1st Child = Girl)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(Both Child = Girls | 1st Child = Girl) = (1/4) / (1/2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(1/4) * (2/1) = &lt;strong&gt;1/2&lt;/strong&gt; or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let&amp;rsquo;s break down the second challenge:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Challenge 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, we start with &amp;ldquo;both children are girls&amp;rdquo;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(1st Child = Girl, 2nd Child = Girl) = 1/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, we have &amp;ldquo;on condition that at least one of the children is a girl&amp;rdquo;. We&amp;rsquo;ll reference a &lt;strong&gt;joint probability table&lt;/strong&gt;. We see that when trying to figure out the probability that &amp;ldquo;at least one of the children is a girl&amp;rdquo;, we rule out the scenario where &lt;strong&gt;both&lt;/strong&gt; children are boys. The remaining 3 out of 4 probabilities, fit the condition.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./at_least.png&#34; alt=&#34;at least&#34;&gt;&lt;/p&gt;
&lt;p&gt;The probability of at least one children being a girl is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1/4) + (1/4) + (1/4) = 3/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So (introducing notation):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(B) = &amp;ldquo;probability of both child being girls&amp;rdquo; (i.e., 1st Child = Girl, 2nd Child = Girl)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(L) = &amp;ldquo;probability of at least one child being a girl&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|L) = P(B,L) / P(L)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|L) = (1/4) / (3/4) = (1/4) * (4/3) = &lt;strong&gt;1/3&lt;/strong&gt; or roughly &lt;strong&gt;33%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-take-away&#34;&gt;Key Take-away&lt;/h4&gt;
&lt;p&gt;When two events are &lt;strong&gt;independent&lt;/strong&gt;, their &lt;strong&gt;joint probability&lt;/strong&gt; is the product of each event:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(E,F) = P(E) * P(F)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Their &lt;strong&gt;conditional&lt;/strong&gt; probability is the &lt;strong&gt;joint probability&lt;/strong&gt; divided by the conditional (i.e., P(F)).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(E|F) = P(E,F) / P(F)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And so for our two challenge scenarios, we have:&lt;/p&gt;
&lt;p&gt;Challenge 1:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;B = probability that both children are girls&lt;/li&gt;
&lt;li&gt;G = probability that the &lt;em&gt;older&lt;/em&gt; children is a girl&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This can be stated as: P(B|G) = P(B,G) / P(G)&lt;/p&gt;
&lt;p&gt;Challenge 2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;B = probability that both children are girls&lt;/li&gt;
&lt;li&gt;L = probability that &lt;em&gt;at least one&lt;/em&gt; children is a girl&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This can be stated as: P(B|L) = P(B,L) / P(L)&lt;/p&gt;
&lt;h4 id=&#34;python-code&#34;&gt;Python Code&lt;/h4&gt;
&lt;p&gt;Now that we have an intuition and have worked out the problem on paper, we can use code to express conditional probability:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import enum, random
class Kid(enum.Enum):
    BOY = 0
    GIRL = 1
    
def random_kid() -&amp;gt; Kid:
    return random.choice([Kid.BOY, Kid.GIRL])
    
both_girls = 0
older_girl = 0
either_girl = 0

random.seed(0)
for _ in range(10000):
    younger = random_kid()
    older = random_kid()
    if older == Kid.GIRL:
        older_girl += 1
    if older == Kid.GIRL and younger == Kid.GIRL:
        both_girls += 1
    if older == Kid.GIRL or younger == Kid.GIRL:
        either_girl += 1
        
print(&amp;quot;P(both | older):&amp;quot;, both_girls / older_girl)   # 0.5007089325501317
print(&amp;quot;P(both | either):&amp;quot;, both_girls / either_girl) # 0.3311897106109325
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that code confirms our intuition.&lt;/p&gt;
&lt;p&gt;We use a &lt;code&gt;for-loop&lt;/code&gt; and &lt;code&gt;range(10000)&lt;/code&gt; to randomly simulate 10,000 scenarios. The &lt;code&gt;random_kid&lt;/code&gt; function randomly picks either a boy or girl (assumption #1). We set the following variables to start a 0, &lt;code&gt;both_girls&lt;/code&gt; (both children are girls); &lt;code&gt;older_girl&lt;/code&gt; (first child is a girl); and &lt;code&gt;either_girl&lt;/code&gt; (at least one child is a girl).&lt;/p&gt;
&lt;p&gt;Then, each of these variables are incremented by 1 through each of the 10,000 loops if it meets certain conditions. After we finish looping, we can call on each of the three variables to see if they match our calculations above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;either_girl #7,464 / 10,000 ~ roughly 75% or 3/4 probability that there is at least one girl
both_girls  #2,472 / 10,000 ~ roughly 25% or 1/4 probability that both children are girls
older_girl  #4,937 / 10,000 ~ roughly 50% or 1/2 probability that the first child is a girl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will look at Bayes Theorem next.&lt;/p&gt;
&lt;h2 id=&#34;bayes_theorem&#34;&gt;Bayes_Theorem&lt;/h2&gt;
&lt;p&gt;Previously, we established an understanding of &lt;strong&gt;conditional&lt;/strong&gt; probability, but building up with &lt;strong&gt;marginal&lt;/strong&gt; and &lt;strong&gt;joint&lt;/strong&gt; probabilities. We explored the conditional probabilities of two outcomes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 1: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;the older child is a girl&amp;rdquo; (G)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome one is roughly 50% or (1/2).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayes&amp;rsquo; Theorem&lt;/strong&gt; is simply &lt;em&gt;an alternate&lt;/em&gt; way of calculating conditional probability.&lt;/p&gt;
&lt;p&gt;Previously, we used the &lt;strong&gt;joint&lt;/strong&gt; probability to calculate the &lt;strong&gt;conditional&lt;/strong&gt; probability.&lt;/p&gt;
&lt;h3 id=&#34;outcome-1&#34;&gt;Outcome 1&lt;/h3&gt;
&lt;p&gt;Here&amp;rsquo;s the conditional probability for outcome 1, using a joint probability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(G) = &amp;lsquo;Probability that first child is a girl&amp;rsquo; (1/2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B) = &amp;lsquo;Probability that both children are girls&amp;rsquo; (1/4)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|G) = P(B,G) / P(G)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(B|G) =  (1/4) / (1/2) = &lt;strong&gt;1/2&lt;/strong&gt; or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Technically, we &lt;em&gt;can&amp;rsquo;t&lt;/em&gt; use joint probability because the two events are &lt;em&gt;not independent&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To clarify, the probability of the older child being a certain gender and the probability of the younger child being a certain gender &lt;em&gt;is&lt;/em&gt; independent, but &lt;code&gt;P(B|G)&lt;/code&gt; the &amp;lsquo;probability of &lt;em&gt;both&lt;/em&gt; child being a girl&amp;rsquo; and &amp;lsquo;the probability of the older child being a girl&amp;rsquo; are &lt;em&gt;not independent&lt;/em&gt;; and hence we express it as a &lt;em&gt;conditional&lt;/em&gt; probability.&lt;/p&gt;
&lt;p&gt;So, the joint probability of &lt;code&gt;P(B,G)&lt;/code&gt; is just event B,&lt;code&gt;P(B)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an alternate way to calculate the conditional probability (&lt;strong&gt;without&lt;/strong&gt; joint probability):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(B|G) = P(G|B) * P(B) / P(G)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;P(B|G) = 1 * (1/4) / (1/2)&lt;/li&gt;
&lt;li&gt;P(B|G) = (1/4) * (2/1)&lt;/li&gt;
&lt;li&gt;P(B|G) = 1/2 = &lt;strong&gt;50%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: P(G|B) is &amp;lsquo;the probability that the first child is a girl, given that &lt;strong&gt;both&lt;/strong&gt; children are girls is a certainty (1.0)&amp;rsquo;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reverse&lt;/strong&gt; conditional probability, can also be calculated, without joint probability:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability of the older child being a girl, given that both children are girls?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(G|B) = P(B|G) * P(G) / P(B)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem (reverse case)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;P(G|B) = (1/2) * (1/2) / (1/4)&lt;/li&gt;
&lt;li&gt;P(G|B) = (1/4) / (1/4)&lt;/li&gt;
&lt;li&gt;P(G|B) = 1 = &lt;strong&gt;100%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is consistent with what we already derived above, namely that P(G|B) is a &lt;strong&gt;certainty&lt;/strong&gt; (probability = 1.0), that the older child is a girl, &lt;strong&gt;given that&lt;/strong&gt; both children are girls.&lt;/p&gt;
&lt;p&gt;We can point out two additional observations / rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;While, joint probabilities are &lt;strong&gt;symmetrical&lt;/strong&gt;: P(B,G) == P(G,B),&lt;/li&gt;
&lt;li&gt;Conditional probabilities are &lt;strong&gt;not symmetrical&lt;/strong&gt;: P(B|G) != P(G|B)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bayes-theorem-alternative-expression&#34;&gt;Bayes&amp;rsquo; Theorem: Alternative Expression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bayes Theorem&lt;/strong&gt; is a way of calculating conditional probability &lt;em&gt;without&lt;/em&gt; the joint probability, summarized here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P(B|G) = P(G|B) * P(B) / P(G)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P(G|B) = P(B|G) * P(G) / P(B)&lt;/code&gt;  &lt;strong&gt;This is Bayes Theorem (reverse case)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You&amp;rsquo;ll note that &lt;code&gt;P(G)&lt;/code&gt; is the denominator in the former, and &lt;code&gt;P(B)&lt;/code&gt; is the denominator in the latter.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What if, for some reasons, we don&amp;rsquo;t have access to the denominator?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We could derive both &lt;code&gt;P(G)&lt;/code&gt; and &lt;code&gt;P(B)&lt;/code&gt; in another way using the &lt;code&gt;NOT&lt;/code&gt; operator:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(G) = P(G,B) + P(G,not B) = P(G|B) * P(B) + P(G|not B) * P(not B)&lt;/li&gt;
&lt;li&gt;P(B) = P(B,G) + P(B,not G) = P(B|G) * P(G) + P(B|not G) * P(not G)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the alternative expression of Bayes Theorem for the probability of &lt;em&gt;both&lt;/em&gt; children being girls, given that the first child is a girl ( P(B|G) ) is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|G) = P(G|B) * P(B) / ( P(G|B) * P(B) + P(G|not B) * P(not B) )&lt;/li&gt;
&lt;li&gt;P(B|G) =     1 * 1/4 / (1 * 1/4 + 1/3 * 3/4)&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/4  /  (1/4 + 3/12)&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/4  /  2/4  =  1/4 * 4/2&lt;/li&gt;
&lt;li&gt;P(B|G) =  1/2 or roughly &lt;strong&gt;50%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can check the result in code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bayes_theorem(p_b, p_g_given_b, p_g_given_not_b):
   # calculate P(not B)
   not_b = 1 - p_b
   # calculate P(G)
   p_g = p_g_given_b * p_b + p_g_given_not_b * not_b
   # calculate P(B|G)
   p_b_given_g = (p_g_given_b * p_b) / p_g
   return p_b_given_g
   
#P(B)
p_b = 1/4

# P(G|B)
p_g_given_b = 1

# P(G|notB)
p_g_given_not_b = 1/3

# calculate P(B|G)
result = bayes_theorem(p_b, p_g_given_b, p_g_given_not_b)

# print result
print(&#39;P(B|G) = %.2f%%&#39; % (result * 100))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the probability that the first child is a girl, given that &lt;em&gt;both&lt;/em&gt; children are girls ( P(G|B) ) is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(G|B) = P(B|G) * P(G) / ( P(G|B) * P(G) + P(B|not G) * P(not G) )&lt;/li&gt;
&lt;li&gt;P(G|B) =   1/2 * 1/2  / ((1/2 * 1/2) + (0 * 1/2))&lt;/li&gt;
&lt;li&gt;P(G|B) =  1/4  /  1/4&lt;/li&gt;
&lt;li&gt;P(G|B) = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s unpack Outcome 2.&lt;/p&gt;
&lt;h3 id=&#34;outcome-2&#34;&gt;Outcome 2&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Outcome 2: What is the probability of the event &amp;ldquo;both children are girls&amp;rdquo; (B) conditional on the event &amp;ldquo;at least one of the children is a girl&amp;rdquo; (L)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The probability for outcome two is roughly 33% or (1/3).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll go through the same process as above.&lt;/p&gt;
&lt;p&gt;We could use &lt;strong&gt;joint&lt;/strong&gt; probability to calculate the &lt;strong&gt;conditional&lt;/strong&gt; probability. As with the previous outcome, the joint probability of &lt;code&gt;P(B,G)&lt;/code&gt; is just event B,&lt;code&gt;P(B)&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(B,L) / P(L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Or, we could use Bayes&amp;rsquo; Theorem to figure out the &lt;strong&gt;conditional&lt;/strong&gt; probability &lt;strong&gt;without joint&lt;/strong&gt; probability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(L|B) * P(B) / P(L)&lt;/li&gt;
&lt;li&gt;P(B|L) =  (1 * 1/4) / (3/4)&lt;/li&gt;
&lt;li&gt;P(B|L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And, if there&amp;rsquo;s no &lt;code&gt;P(L)&lt;/code&gt;, we can calculate that indirectly, also using Bayes&amp;rsquo; Theorem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(L) = P(L|B) * P(B) + P(L|not B) * P(not B)&lt;/li&gt;
&lt;li&gt;P(L) =  1 * (1/4) + (2/3) * (3/4)&lt;/li&gt;
&lt;li&gt;P(L) =  (1/4) + (2/4)&lt;/li&gt;
&lt;li&gt;P(L) = 3/4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, we can use &lt;code&gt;P(L)&lt;/code&gt; in the way Bayes&amp;rsquo; Theorem is commonly expressed, when we don&amp;rsquo;t have the denominator:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B|L) = P(L|B) * P(B) / ( P(L|B) * P(B) + P(L|not B) * P(not B) )&lt;/li&gt;
&lt;li&gt;P(B|L) =  1 * (1/4) / (3/4)&lt;/li&gt;
&lt;li&gt;P(B|L) = 1/3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we&amp;rsquo;ve gone through the calculation for two conditional probabilities, &lt;code&gt;P(B|G)&lt;/code&gt; and &lt;code&gt;P(B|L)&lt;/code&gt;, using Bayes Theorem, and implemented code for one of the scenarios, let&amp;rsquo;s take a step back and assess what this &lt;em&gt;means&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bayesian-terminology&#34;&gt;Bayesian Terminology&lt;/h3&gt;
&lt;p&gt;I think its useful to understand that probability in general shines when we want to describe uncertainty and that Bayes&amp;rsquo; Theorem allows us to quantify how much the data we observe, should change our beliefs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes_table.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have two &lt;strong&gt;posteriors&lt;/strong&gt;, &lt;code&gt;P(B|G)&lt;/code&gt; and &lt;code&gt;P(B|L)&lt;/code&gt;, both with equal &lt;strong&gt;priors&lt;/strong&gt; and &lt;strong&gt;likelihood&lt;/strong&gt;, but with &lt;em&gt;different&lt;/em&gt; &lt;strong&gt;evidence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Said differently, we want to know the &amp;lsquo;probability that both children are girls`, given &lt;em&gt;different&lt;/em&gt; conditions.&lt;/p&gt;
&lt;p&gt;In the first case, our condition is &amp;lsquo;the first child is a girl&amp;rsquo; and in the second case, our condition is &amp;lsquo;&lt;em&gt;at least one&lt;/em&gt; of the child is a girl&amp;rsquo;. The question is which condition will increase the probability that &lt;strong&gt;both&lt;/strong&gt; children are girls?&lt;/p&gt;
&lt;p&gt;Bayes&amp;rsquo; Theorem allows us to update our belief about the probability in these two cases, as we incorporate varied data into our framework.&lt;/p&gt;
&lt;p&gt;What the calculations tell us is that the &lt;strong&gt;evidence&lt;/strong&gt; that &amp;lsquo;one child is a girl&amp;rsquo; increases the probability that &lt;strong&gt;both&lt;/strong&gt; children are girls &lt;em&gt;more than&lt;/em&gt; the other piece of &lt;strong&gt;evidence&lt;/strong&gt; that &amp;lsquo;at least one child is a girl&amp;rsquo; increases that probability.&lt;/p&gt;
&lt;p&gt;And our beliefs should be updated accordingly.&lt;/p&gt;
&lt;p&gt;At the end of the day, understanding conditional probability (and Bayes Theorem) comes down to &lt;strong&gt;counting&lt;/strong&gt;. For our hypothetical scenarios, we only need one hand:&lt;/p&gt;
&lt;p&gt;When we look at the probability table for outcome one, &lt;code&gt;P(B|G)&lt;/code&gt;, we can see how the posterior probability comes out to 1/2:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./outcome_one.png&#34; alt=&#34;outcome_one&#34;&gt;&lt;/p&gt;
&lt;p&gt;When we look at the probability table for outcome two, &lt;code&gt;P(B|L)&lt;/code&gt;, we can see how the posterior probability comes out to 1/3:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./outcome_two.png&#34; alt=&#34;outcome_two&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is part of an ongoing series documenting my progress through Data Science from Scratch by Joel Grus:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./conditional_prob_ch6.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;applying_bayes_theorem&#34;&gt;Applying_Bayes_Theorem&lt;/h2&gt;
&lt;p&gt;Now that we have a basic understanding of Bayes Theorem, let&amp;rsquo;s extend the application to a slightly more complex example. This section was inspired by this 
&lt;a href=&#34;https://twitter.com/3blue1brown/status/1333121058824613889?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tweet&lt;/a&gt; from Grant Sanderson (of 
&lt;a href=&#34;https://www.youtube.com/watch?v=HZGCoVF3YvM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown fame&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./grant_tweet.png&#34; alt=&#34;grant_tweet&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is a classic application of Bayes Theorem - the &lt;strong&gt;medical diagnostic scenario&lt;/strong&gt;. The above tweet can be re-stated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability of you &lt;em&gt;actually having the disease&lt;/em&gt;, given that you tested positive?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This happens to be even more relevant as we&amp;rsquo;re living through a generational pandemic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start off with a conceptual understanding, using the tools we learned previously. First, we have to keep in mind &lt;strong&gt;testing&lt;/strong&gt; and &lt;strong&gt;actually having the disease&lt;/strong&gt; are &lt;strong&gt;not independent&lt;/strong&gt; events. Therefore, we will use &lt;strong&gt;conditional probability&lt;/strong&gt; to express their joint outcomes.&lt;/p&gt;
&lt;p&gt;The intuitive visual to illustrate this is the &lt;strong&gt;tree diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./initial_tree.png&#34; alt=&#34;initial_tree&#34;&gt;&lt;/p&gt;
&lt;p&gt;The initial given information contains the information in the tree.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P(D): Probability of having the disease (covid-19)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P(P): Probability of testing positive&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;*P(D|P): Our objective is to find the probability of having the disease, given a positive test&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1 in 1,000 actively have covid-19, P(D), this implies&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;999 in 1,000 do &lt;strong&gt;not&lt;/strong&gt; actively have covid-19, P(not D)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1% or 0.01 false positive (given)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;10% or 0.1 false negative (given)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;false positive&lt;/strong&gt; is when you &lt;em&gt;don&amp;rsquo;t&lt;/em&gt; have the disease, but your test (in error) shows up positive. &lt;strong&gt;False negative&lt;/strong&gt; is when you &lt;em&gt;have&lt;/em&gt; the disease, but your test (in error) shows up negative. We are provided this information and have to calculate other values to fill in the tree.&lt;/p&gt;
&lt;p&gt;We know that all possible events have to add up to 1, so if 1 in 1,000 actively have the disease, we know that 999 in 1,000 do not have it. If the false negative is 10%, then the &lt;strong&gt;true positive&lt;/strong&gt; is 90%. If the false positive is 1%, then the &lt;strong&gt;true negative&lt;/strong&gt; is 99%. From our calculations, the tree can be updated:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./second_tree.png&#34; alt=&#34;second_tree&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we&amp;rsquo;ve filled out the tree, we can use &lt;strong&gt;Bayes&amp;rsquo; Theorem&lt;/strong&gt; to find &lt;code&gt;P(D|P)&lt;/code&gt;. Here&amp;rsquo;s Bayes&amp;rsquo; Theorem that we discussed in the previous section. We have Bayes&amp;rsquo; Theorem, the denominator, probability of testing positive &lt;code&gt;P(P)&lt;/code&gt; and the &lt;em&gt;second&lt;/em&gt; version of Bayes Theorem in cases were we &lt;em&gt;do not know&lt;/em&gt; the probability of testing positive (as in the present case):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes1.png&#34; alt=&#34;bayes1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then we can plug-in the denominator to get the alternative version of Bayes&amp;rsquo; Theorem:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bayes2.png&#34; alt=&#34;bayes2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how the numbers add up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(D|P) = P(P|D) * P(D) / P(P|D) * P(D) + P(P|not D) * P(not D)&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.9 * 0.001 / 0.9 * 0.001 + 0.01 * 0.999&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.0009 / 0.0009 + 0.00999&lt;/li&gt;
&lt;li&gt;P(D|P) = 0.0009 / 0.01089&lt;/li&gt;
&lt;li&gt;P(D|P) ~ 0.08264 or 8.26%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interestingly, 
&lt;a href=&#34;https://twitter.com/karpathy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy&lt;/a&gt; actually 
&lt;a href=&#34;https://twitter.com/karpathy/status/1333217287155847169?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;responded in the thread&lt;/a&gt; and provided an intuitive way to arrive at the same result using Python.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s his code (with added comments):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from random import random, seed
seed(0)

pop = 10000000 # 10M people
counts = {}

for i in range(pop):
    has_covid = i % 1000 == 0 # one in 1,000 people have covid (priors or prevalence of disease)
    # The major assumption is that every person gets tested regardless of any symptoms
    if has_covid:                  # Has disease
        tests_positive = True      # True positive
        if random() &amp;lt; 0.1:     
            tests_positive = False # False negative
    else:                          # Does not have disease
        tests_positive = False     # True negative
        if random() &amp;lt; 0.01:    
            tests_positive = True  # False positive
    outcome = (has_covid, tests_positive)
    counts[outcome] = counts.get(outcome, 0) + 1
    
for (has_covid, tested_positive), n in counts.items():
    print(&#39;has covid: %6s, tests positive: %6s, count: %d&#39; % (has_covid, tested_positive, n))
    
n_positive = counts[(True, True)] + counts[(False, True)]

print(&#39;number of people who tested positive:&#39;, n_positive)
print(&#39;probability that a test-positive person actually has covid: %.2f&#39; % (100.0 * counts[(True, True)] / n_positive), )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first build a hypothetical population of 10 million. If the &lt;strong&gt;prior&lt;/strong&gt; or &lt;strong&gt;prevalence&lt;/strong&gt; of disease is 1 in 1,000, a population of 10 million should find 10000 people with covid. You can see how this works with this short snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pop = 10000000
counts = 0

for i in range(pop):
    has_covid = i % 1000 == 0
    if has_covid:
        counts = counts + 1
print(counts, &amp;quot;people have the disease in a population of 10 million&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nested in the &lt;code&gt;for-loop&lt;/code&gt; are &lt;code&gt;if-statements&lt;/code&gt; that segment the population (10M) into one of four categories True Positive, False Negative, True Negative, False Positive. Each category is counted and stored in a &lt;code&gt;dict&lt;/code&gt; called &lt;code&gt;counts&lt;/code&gt;. Then another &lt;code&gt;for-loop&lt;/code&gt; is used to loop through this dictionary to print out all the categories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;has covid:   True, tests positive:   True, count: 9033
has covid:  False, tests positive:  False, count: 9890133
has covid:  False, tests positive:   True, count: 99867
has covid:   True, tests positive:  False, count: 967

number of people who tested positive: 108900
probability that a test-positive person actually has covid: 8.29
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we want the number of people who &lt;em&gt;have&lt;/em&gt; the disease &lt;em&gt;and&lt;/em&gt; tested positive (True Positive, 9033) divided by the number of people who tested positive, regardless of whether they actually have the disease (True Positive (9033) + False Positive (99867) = 108,900) and this comes out to approximately 8.29.&lt;/p&gt;
&lt;p&gt;Although the 
&lt;a href=&#34;https://twitter.com/karpathy/status/1333217287155847169?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; was billed as &amp;ldquo;simple code to build intuition&amp;rdquo;, I found that Bayes&amp;rsquo; Theorem &lt;em&gt;is&lt;/em&gt; the intuition.&lt;/p&gt;
&lt;h3 id=&#34;what-about-symptoms&#34;&gt;What about symptoms?&lt;/h3&gt;
&lt;p&gt;The key to Bayes&amp;rsquo; Theorem is that it encourages us to update our beliefs when presented with new evidence. But what if there&amp;rsquo;s evidence we missed in the first place?&lt;/p&gt;
&lt;p&gt;If you look back at the 
&lt;a href=&#34;https://twitter.com/3blue1brown/status/1333121058824613889?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original tweet&lt;/a&gt;, there are important details about symptoms that, if we wanted to be more realistic, should be accounted for.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You feel fatigued and have a slight sore throat.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, instead of assuming that prevalence of the disease (1 in 1,000 people have covid-19) is the prior, we might ask what probability that someone who is symptomatic has the disease?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s suppose we change from 1 in 1,000 to 1 in 100. We could change just one line of code (while everything else remains the same):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(pop):
    has_covid = i % 100 == 0 # update info: 1/1000 have covid, but 1/100 with symptoms have covid
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability that someone with a positive test actually has the disease jumps from 8.29% to 47.61%&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;has covid:   True, tests positive:   True, count: 180224
has covid:  False, tests positive:  False, count: 19601715
has covid:  False, tests positive:   True, count: 198285
has covid:   True, tests positive:  False, count: 19776
number of people who tested positive: 378509
probability that a test-positive person with symptoms actually has covid: 47.61
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, being symptomatic means our &lt;strong&gt;priors&lt;/strong&gt; should be adjusted and our &lt;strong&gt;beliefs&lt;/strong&gt; about the likelihood that a positive test means we have the disease (&lt;code&gt;P(D|P)&lt;/code&gt;) should be updated accordingly (in this case, it goes way up).&lt;/p&gt;
&lt;h3 id=&#34;take-aways&#34;&gt;Take Aways&lt;/h3&gt;
&lt;p&gt;Hypothetically, if we have family or friends living in an area where 1 in 1,000 people have covid-19 and they (god forbid) got tested and got a positive result, you could tell them that their probability of actually having the disease, given a positive test was around 8.26–8.29%.&lt;/p&gt;
&lt;p&gt;However, what’s useful about the Bayesian approach is that it encourages us to incorporate new information and update our beliefs accordingly. So if we find out our family or friend is also &lt;em&gt;symptomatic&lt;/em&gt;, we could advise them of the higher probability (~47.61%).&lt;/p&gt;
&lt;p&gt;Finally, we may also advise our family/friends to get tested &lt;strong&gt;again&lt;/strong&gt;, because as much as test-positive person would hope they got a ‘false positive’, chances are low. And even lower, is getting a false positive &lt;em&gt;twice&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./second_test.png&#34; alt=&#34;second_test&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;distributions&#34;&gt;Distributions&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll cover various distributions. This is a broad topic so we&amp;rsquo;ll sample a few concepts to get a feel for it. Borrowing from the previous post, we&amp;rsquo;ll chart our medical diagnostic outcomes.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll recall that each outcome is the combination of whether someone has a disease, &lt;code&gt;P(D)&lt;/code&gt;, or not, &lt;code&gt;P(not D)&lt;/code&gt;. Then, they&amp;rsquo;re given a diagnostic test that returns positive, &lt;code&gt;P(P)&lt;/code&gt; or negative, &lt;code&gt;P(not P)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;These are discrete outcomes so they can be represented with the &lt;strong&gt;probability mass function&lt;/strong&gt;, as opposed to a &lt;strong&gt;probability density function&lt;/strong&gt;, which represent a continuous distribution.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take another &lt;em&gt;hypothetical&lt;/em&gt; scenario of a city where 1 in 10 people have a disease and a diagnostic test has a True Positive of 95% and True Negative of 90%. The probability that a test-positive person &lt;em&gt;actually&lt;/em&gt; having the disease is 46.50%.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from random import random, seed

seed(0)
pop = 1000  # 1000 people
counts = {}
for i in range(pop):
    has_disease = i % 10 == 0  # one in 10 people have disease
    # assuming that every person gets tested regardless of any symptoms
    if has_disease:
        tests_positive = True       # True Positive  95%
        if random() &amp;lt; 0.05:
            tests_positive = False  # False Negative 5%
    else:
        tests_positive = False      # True Negative  90%
        if random() &amp;lt; 0.1:
            tests_positive = True   # False Positive 10%
    outcome = (has_disease, tests_positive)
    counts[outcome] = counts.get(outcome, 0) + 1

for (has_disease, tested_positive), n in counts.items():
    print(&#39;Has Disease: %6s, Test Positive: %6s, count: %d&#39; %
          (has_disease, tested_positive, n))

n_positive = counts[(True, True)] + counts[(False, True)]
print(&#39;Number of people who tested positive:&#39;, n_positive)
print(&#39;Probability that a test-positive person actually has disease: %.2f&#39; %
      (100.0 * counts[(True, True)] / n_positive),)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given the probability that someone has the disease (1 in 10), also called the &amp;lsquo;prior&amp;rsquo; in Bayesian terms. We modeled four scenarios where people were given a diagnostic test. Again, the big assumption here is that people get randomly tested. With the true positive and true negative rates stated above, here are the outcomes:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./hypothetical_outcome.png&#34; alt=&#34;hypothetical_outcome&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;probability-mass-function&#34;&gt;Probability Mass Function&lt;/h3&gt;
&lt;p&gt;Given these discrete events, we can chart a &lt;strong&gt;probability mass function&lt;/strong&gt;, also known as 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_mass_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discrete density function&lt;/a&gt;. We&amp;rsquo;ll import &lt;code&gt;pandas&lt;/code&gt; to help us create &lt;code&gt;DataFrames&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; to chart the &lt;strong&gt;probability mass function&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We first need to turn the counts of events into a &lt;code&gt;DataFrame&lt;/code&gt; and change the column to &lt;code&gt;item_counts&lt;/code&gt;. Then, we&amp;rsquo;ll calculate the probability of each event by dividing the count by the total number of people in our hypothetical city (i.e., population: 1000).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optional&lt;/strong&gt;: Create another column with abbreviations for test outcome (i.e., &amp;ldquo;True True&amp;rdquo; becomes &amp;ldquo;TT&amp;rdquo;). We&amp;rsquo;ll call this column &lt;code&gt;item2&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame.from_dict(counts, orient=&#39;index&#39;)
df = df.rename(columns={0: &#39;item_counts&#39;})
df[&#39;probability&#39;] = df[&#39;item_counts&#39;]/1000
df[&#39;item2&#39;] = [&#39;TT&#39;, &#39;FF&#39;, &#39;FT&#39;, &#39;TF&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the &lt;code&gt;DataFrame&lt;/code&gt; we have so far:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./pmf_df.png&#34; alt=&#34;pmf_df&#34;&gt;&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll note that the numbers in the &lt;code&gt;probability&lt;/code&gt; column adds up to 1.0 and that the &lt;code&gt;item_counts&lt;/code&gt; numbers are the same as the count above when we had calculated the probability of a test-positive person actually having the disease.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll use a simple bar chart to chart out the diagnostic probabilities and this is how we&amp;rsquo;d visually represent the probability mass function - probabilities of each discrete event; each &amp;lsquo;discrete event&amp;rsquo; is a conditional (e.g., probability that someone has a positive test, given that they &lt;em&gt;have&lt;/em&gt; the disease - TT or probability that someone has a negative test, given that they &lt;em&gt;don&amp;rsquo;t have&lt;/em&gt; the disease - FF, and so on).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./prob_mass_function.png&#34; alt=&#34;prob_mass_function.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.DataFrame.from_dict(counts, orient=&#39;index&#39;)
df = df.rename(columns={0: &#39;item_counts&#39;})
df[&#39;probability&#39;] = df[&#39;item_counts&#39;]/1000
df[&#39;item2&#39;] = [&#39;TT&#39;, &#39;FF&#39;, &#39;FT&#39;, &#39;TF&#39;]
plt.bar(df[&#39;item2&#39;], df[&#39;probability&#39;])
plt.title(&amp;quot;Probability Mass Function&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;cumulative-distribution-function&#34;&gt;Cumulative Distribution Function&lt;/h3&gt;
&lt;p&gt;While the probability mass function can tell us the probability of each discrete event (i.e., TT, FF, FT, and TF) we can also represent the same information as a &lt;strong&gt;cumulative distribution function&lt;/strong&gt; which allows us to see how the probability changes as we add events together.&lt;/p&gt;
&lt;p&gt;The cumulative distribution function simply adds the probability from the previous row in a &lt;code&gt;DataFrame&lt;/code&gt; in a cumulative fashion, like in the column &lt;code&gt;probability2&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cdf_df.png&#34; alt=&#34;cdf_df.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;cumsum()&lt;/code&gt; function to create the &lt;code&gt;cumsum&lt;/code&gt; column which is simply adding the &lt;code&gt;item_counts&lt;/code&gt;, with each successive row. When we create the corresponding probability column, &lt;code&gt;probability2&lt;/code&gt;, it gets larger until we reach 1.0.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cum_distri_function.png&#34; alt=&#34;cum_distri_function&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart tells us that the probability of getting both TT and FF (True, True = True Positive, and False, False = True Negative) is 88.6% which indicates that 11.4% (100 - 88.6) of the time, the diagnostic test will let us down.&lt;/p&gt;
&lt;h3 id=&#34;normal-distribution&#34;&gt;Normal Distribution&lt;/h3&gt;
&lt;p&gt;More often than not, you&amp;rsquo;ll be interested in &lt;em&gt;continuous&lt;/em&gt; distributions and you can see better see how the &lt;strong&gt;cumulative distribution function&lt;/strong&gt; works.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;re probably familiar with the bell shaped curve or the &lt;em&gt;normal distribution&lt;/em&gt;, defined solely by its mean (mu) and standard deviation (sigma). If you have a &lt;strong&gt;standard normal distribution&lt;/strong&gt; of probability values, the average would be 0 and the standard deviation would be 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./1_normal.png&#34; alt=&#34;1_normal&#34;&gt;&lt;/p&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math
SQRT_TWO_PI = math.sqrt(2 * math.pi)

def normal_pdf(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (SQRT_TWO_PI * sigma))
    
# plot
xs = [x / 10.0 for x in range(-50, 50)]
plt.plot(xs, [normal_pdf(x, sigma=1) for x in xs], &#39;-&#39;, label=&#39;mu=0, sigma=1&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;strong&gt;standard normal distribution&lt;/strong&gt; curve, you see the average probability is around 0.4. But if you add up the area under the curve (i.e., all probabilities of every possible outcome), you would get 1.0, just like with the medical diagnostic example.&lt;/p&gt;
&lt;p&gt;And if you split the bell in half, then flip over the left half, you&amp;rsquo;ll (visually) get the &lt;strong&gt;cumulative distribution function&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./1_cumu.png&#34; alt=&#34;1_cumu&#34;&gt;&lt;/p&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math

def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&amp;gt; float:
    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2
    
# plot
xs = [x / 10.0 for x in range(-50, 50)]
plt.plot(xs, [normal_cdf(x, sigma=1) for x in xs], &#39;-&#39;, label=&#39;mu=0,sigma=1&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In both cases, the area under the curve for the &lt;strong&gt;standard normal distribution&lt;/strong&gt; and the &lt;strong&gt;cumulative distribution function&lt;/strong&gt; is 1.0, thus summing the probabilities of all events is one.&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going beyond summary statistics</title>
      <link>/post/datasaurus/</link>
      <pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/datasaurus/</guid>
      <description>


&lt;div id=&#34;datasaurus-introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Datasaurus Introduction&lt;/h2&gt;
&lt;p&gt;I recently came across the &lt;strong&gt;Datasaurus&lt;/strong&gt; dataset by Alberto Cairo on &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-10-13/readme.md&#34;&gt;#TidyTuesday&lt;/a&gt; and wanted to create a series of charts illustrating the lessons associated with this dataset, primarily to: &lt;a href=&#34;http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html&#34;&gt;never trust summary statistics alone&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, some context. Here’s Alberto’s &lt;a href=&#34;https://twitter.com/albertocairo/status/765167969139765250&#34;&gt;original tweet&lt;/a&gt; from years ago when he created this dataset:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;png/alberto_cairo.png&#34; alt=&#34;png&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;png&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This tweet alone doesn’t communicate why we shouldn’t trust summary statistics alone, so let’s unpack this. First we’ll load the various packages and data we’ll use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;load-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Load Packages&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.3     ✓ dplyr   1.0.1
## ✓ tidyr   1.1.1     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggcorrplot)
library(ggridges)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Load Data&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt; : &lt;code&gt;datasaurus&lt;/code&gt; and &lt;code&gt;datasaurus_dozen&lt;/code&gt; are identical. The former is provided via #TidyTuesday, the latter from &lt;a href=&#34;https://www.autodesk.com/research/publications/same-stats-different-graphs&#34;&gt;this research paper&lt;/a&gt; discussing more advanced concepts beyond the scope of this document (i.e., simulated annealing).&lt;/p&gt;
&lt;p&gt;You’ll also note that &lt;code&gt;datasaurus_dozen&lt;/code&gt; and &lt;code&gt;datasaurus_wide&lt;/code&gt; are the same data, organized differently. The former in &lt;em&gt;long&lt;/em&gt; format and the latter, in &lt;em&gt;wide&lt;/em&gt; format - see here for &lt;a href=&#34;http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/&#34;&gt;details&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For the most part, we’ll use &lt;code&gt;datasaurus_dozen&lt;/code&gt; throughout this document. We’ll use &lt;code&gt;datasaurus_wide&lt;/code&gt; when we get to the correlation section.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus &amp;lt;- readr::read_csv(&amp;#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-13/datasaurus.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   dataset = col_character(),
##   x = col_double(),
##   y = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus_dozen &amp;lt;- read_tsv(&amp;#39;./data/DatasaurusDozen.tsv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   dataset = col_character(),
##   x = col_double(),
##   y = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus_wide &amp;lt;- read_tsv(&amp;#39;./data/DatasaurusDozen-wide.tsv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Duplicated column names deduplicated: &amp;#39;away&amp;#39; =&amp;gt; &amp;#39;away_1&amp;#39; [2],
## &amp;#39;bullseye&amp;#39; =&amp;gt; &amp;#39;bullseye_1&amp;#39; [4], &amp;#39;circle&amp;#39; =&amp;gt; &amp;#39;circle_1&amp;#39; [6], &amp;#39;dino&amp;#39; =&amp;gt;
## &amp;#39;dino_1&amp;#39; [8], &amp;#39;dots&amp;#39; =&amp;gt; &amp;#39;dots_1&amp;#39; [10], &amp;#39;h_lines&amp;#39; =&amp;gt; &amp;#39;h_lines_1&amp;#39; [12],
## &amp;#39;high_lines&amp;#39; =&amp;gt; &amp;#39;high_lines_1&amp;#39; [14], &amp;#39;slant_down&amp;#39; =&amp;gt; &amp;#39;slant_down_1&amp;#39; [16],
## &amp;#39;slant_up&amp;#39; =&amp;gt; &amp;#39;slant_up_1&amp;#39; [18], &amp;#39;star&amp;#39; =&amp;gt; &amp;#39;star_1&amp;#39; [20], &amp;#39;v_lines&amp;#39;
## =&amp;gt; &amp;#39;v_lines_1&amp;#39; [22], &amp;#39;wide_lines&amp;#39; =&amp;gt; &amp;#39;wide_lines_1&amp;#39; [24], &amp;#39;x_shape&amp;#39; =&amp;gt;
## &amp;#39;x_shape_1&amp;#39; [26]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   .default = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See spec(...) for full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;eyeballing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Eyeballing the data&lt;/h2&gt;
&lt;p&gt;Here are the first six rows of &lt;code&gt;datasaurus_dozen&lt;/code&gt; (long):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   dataset     x     y
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 dino     55.4  97.2
## 2 dino     51.5  96.0
## 3 dino     46.2  94.5
## 4 dino     42.8  91.4
## 5 dino     40.8  88.3
## 6 dino     38.7  84.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the first six rows of &lt;code&gt;datasaurus_wide&lt;/code&gt; (wide):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 26
##   away  away_1 bullseye bullseye_1 circle circle_1 dino  dino_1 dots  dots_1
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; 
## 1 x     y      x        y          x      y        x     y      x     y     
## 2 32.3… 61.41… 51.2038… 83.339776… 55.99… 79.2772… 55.3… 97.17… 51.1… 90.86…
## 3 53.4… 26.18… 58.9744… 85.499817… 50.03… 79.0130… 51.5… 96.02… 50.5… 89.10…
## 4 63.9… 30.83… 51.8720… 85.829737… 51.28… 82.4359… 46.1… 94.48… 50.2… 85.46…
## 5 70.2… 82.53… 48.1799… 85.045116… 51.17… 79.1652… 42.8… 91.41… 50.0… 83.05…
## 6 34.1… 45.73… 41.6832… 84.017940… 44.37… 78.1646… 40.7… 88.33… 50.5… 82.93…
## # … with 16 more variables: h_lines &amp;lt;chr&amp;gt;, h_lines_1 &amp;lt;chr&amp;gt;, high_lines &amp;lt;chr&amp;gt;,
## #   high_lines_1 &amp;lt;chr&amp;gt;, slant_down &amp;lt;chr&amp;gt;, slant_down_1 &amp;lt;chr&amp;gt;, slant_up &amp;lt;chr&amp;gt;,
## #   slant_up_1 &amp;lt;chr&amp;gt;, star &amp;lt;chr&amp;gt;, star_1 &amp;lt;chr&amp;gt;, v_lines &amp;lt;chr&amp;gt;, v_lines_1 &amp;lt;chr&amp;gt;,
## #   wide_lines &amp;lt;chr&amp;gt;, wide_lines_1 &amp;lt;chr&amp;gt;, x_shape &amp;lt;chr&amp;gt;, x_shape_1 &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 13 variables, each with X- and Y- axes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary Statistics&lt;/h2&gt;
&lt;p&gt;First, we’ll note that if we just look at summary statistics (i.e., &lt;strong&gt;mean&lt;/strong&gt; and &lt;strong&gt;standard deviation&lt;/strong&gt;), we might conclude that these variables are all the &lt;em&gt;same&lt;/em&gt;. Moreover, within each variable, &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values have very &lt;strong&gt;similarly low correlations&lt;/strong&gt; at ranging from -0.06 to -0.07.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus_dozen %&amp;gt;%
    group_by(dataset) %&amp;gt;%
    summarize(
        x_mean = mean(x),
        x_sd = sd(x),
        y_mean = mean(y),
        y_sd = sd(y),
        corr = cor(x,y)
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 x 6
##    dataset    x_mean  x_sd y_mean  y_sd    corr
##    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 away         54.3  16.8   47.8  26.9 -0.0641
##  2 bullseye     54.3  16.8   47.8  26.9 -0.0686
##  3 circle       54.3  16.8   47.8  26.9 -0.0683
##  4 dino         54.3  16.8   47.8  26.9 -0.0645
##  5 dots         54.3  16.8   47.8  26.9 -0.0603
##  6 h_lines      54.3  16.8   47.8  26.9 -0.0617
##  7 high_lines   54.3  16.8   47.8  26.9 -0.0685
##  8 slant_down   54.3  16.8   47.8  26.9 -0.0690
##  9 slant_up     54.3  16.8   47.8  26.9 -0.0686
## 10 star         54.3  16.8   47.8  26.9 -0.0630
## 11 v_lines      54.3  16.8   47.8  26.9 -0.0694
## 12 wide_lines   54.3  16.8   47.8  26.9 -0.0666
## 13 x_shape      54.3  16.8   47.8  26.9 -0.0656&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boxplots&lt;/h2&gt;
&lt;p&gt;You could use &lt;code&gt;boxplots&lt;/code&gt; to show &lt;em&gt;slight&lt;/em&gt; variation in the distribution and &lt;strong&gt;median&lt;/strong&gt; values of these 13 variables. However, the &lt;strong&gt;mean&lt;/strong&gt; values, indicated with the red circles, are identical.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus_dozen %&amp;gt;%
    ggplot(aes(x = dataset, y = x, fill = dataset)) +
    geom_boxplot(alpha = 0.6) +
    stat_summary(fun = mean, geom = &amp;quot;point&amp;quot;, shape = 20, size = 6, color = &amp;quot;red&amp;quot;, fill = &amp;quot;red&amp;quot;) +
    scale_fill_brewer(palette = &amp;quot;Set3&amp;quot;) +
    theme_classic() +
    theme(legend.position = &amp;#39;none&amp;#39;) +
    labs(
        y = &amp;#39;13 variables&amp;#39;,
        x = &amp;#39;X-values&amp;#39;,
        title = &amp;quot;Boxplots: Slight differences in the distribution and median values (X-axis)&amp;quot;,
        subtitle = &amp;quot;Identical mean values&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Set3 is 12
## Returning the palette you asked for with that many colors&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/boxplot_x-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s the same plot for &lt;code&gt;y&lt;/code&gt; values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus_dozen %&amp;gt;%
    ggplot(aes(x = dataset, y = y, fill = dataset)) +
    geom_boxplot(alpha = 0.6) +
    stat_summary(fun = mean, geom = &amp;quot;point&amp;quot;, shape = 20, size = 6, color = &amp;quot;red&amp;quot;, fill = &amp;quot;red&amp;quot;) +
    scale_fill_brewer(palette = &amp;quot;Paired&amp;quot;) +
    theme_classic() +
    theme(legend.position = &amp;#39;none&amp;#39;) +
    labs(
        y = &amp;#39;13 variables&amp;#39;,
        x = &amp;#39;Y-values&amp;#39;,
        title = &amp;quot;Boxplots: Slight differences in the distribution and median values (Y-axis)&amp;quot;,
        subtitle = &amp;quot;Identical mean values&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Paired is 12
## Returning the palette you asked for with that many colors&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/boxplot_y-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ridgeline-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ridgeline Plot&lt;/h2&gt;
&lt;p&gt;We can begin to get a sense for how these variables are different if we plot the distribution in different ways. The ridgeline plot begins to reveal aspects of the data that were hidden before.&lt;/p&gt;
&lt;p&gt;We can begin to see that certain variables have markedly different distribution shapes (i.e., &lt;code&gt;v_lines&lt;/code&gt;, &lt;code&gt;dots&lt;/code&gt;, &lt;code&gt;x_shape&lt;/code&gt;, &lt;code&gt;wide_lines&lt;/code&gt;), while having the same &lt;strong&gt;mean&lt;/strong&gt; value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus_dozen %&amp;gt;%
    ggplot(aes(x = x, y = dataset, fill = dataset)) +
    geom_density_ridges_gradient(scale = 3, quantile_lines = T, quantile_fun = mean) +
    scale_fill_manual(values = c(&amp;#39;#a6cee3&amp;#39;, &amp;#39;#1f78b4&amp;#39;, &amp;#39;#b2df8a&amp;#39;, &amp;#39;#33a02c&amp;#39;, &amp;#39;#fb9a99&amp;#39;, &amp;#39;#e31a1c&amp;#39;, &amp;#39;#fdbf6f&amp;#39;, &amp;#39;#ff7f00&amp;#39;, &amp;#39;#cab2d6&amp;#39;, &amp;#39;#6a3d9a&amp;#39;, &amp;#39;#ffff99&amp;#39;, &amp;#39;#b15928&amp;#39;, &amp;#39;grey&amp;#39;)) +
    theme_classic() +
    theme(legend.position = &amp;#39;none&amp;#39;) +
    labs(
        x = &amp;quot;X-values&amp;quot;,
        y = &amp;quot;13 variables&amp;quot;,
        title = &amp;quot;Ridgeline Plot: More variation in the distribution (X-axis)&amp;quot;,
        subtitle = &amp;quot;Identical mean values&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 5.46&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/ridgeline_x-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;y&lt;/code&gt; values, &lt;code&gt;high_lines&lt;/code&gt;, &lt;code&gt;dots&lt;/code&gt;, &lt;code&gt;circle&lt;/code&gt; and &lt;code&gt;star&lt;/code&gt; have obviously different distributions from the rest. Again, the &lt;strong&gt;mean&lt;/strong&gt; values are identical across variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus_dozen %&amp;gt;%
    ggplot(aes(x = y, y = dataset, fill = dataset)) +
    geom_density_ridges_gradient(scale = 3, quantile_lines = T, quantile_fun = mean) +
    scale_fill_manual(values = c(&amp;#39;#a6cee3&amp;#39;, &amp;#39;#1f78b4&amp;#39;, &amp;#39;#b2df8a&amp;#39;, &amp;#39;#33a02c&amp;#39;, &amp;#39;#fb9a99&amp;#39;, &amp;#39;#e31a1c&amp;#39;, &amp;#39;#fdbf6f&amp;#39;, &amp;#39;#ff7f00&amp;#39;, &amp;#39;#cab2d6&amp;#39;, &amp;#39;#6a3d9a&amp;#39;, &amp;#39;#ffff99&amp;#39;, &amp;#39;#b15928&amp;#39;, &amp;#39;grey&amp;#39;)) +
    theme_classic() +
    theme(legend.position = &amp;#39;none&amp;#39;) +
    labs(
        x = &amp;quot;Y-values&amp;quot;,
        y = &amp;quot;13 variables&amp;quot;,
        title = &amp;quot;Ridgeline Plot: More variation in the distribution (Y-axis)&amp;quot;,
        subtitle = &amp;quot;Identical mean values&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/ridgeline_y-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlations&lt;/h2&gt;
&lt;p&gt;If you skip visualizing the distribution and central tendencies and go straight to seeing how the variables correlate with each other, you could also miss some fundamental differences in the data.&lt;/p&gt;
&lt;p&gt;In particular, the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values across all 13 variables are &lt;em&gt;highlight correlated&lt;/em&gt;. With just knowledge of the summary statistics, one could be led to believe that these variables are &lt;em&gt;highly similar&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Below is an abbreviated &lt;strong&gt;correlation matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggcorrplot)

# X-values
# selecting rows 2-143 
# turning all values from character to numeric
datasaurus_wide_x &amp;lt;- datasaurus_wide %&amp;gt;%
    slice(2:143) %&amp;gt;%
    select(away, bullseye, circle, dino, dots, h_lines, high_lines, slant_down, slant_up, star, v_lines, wide_lines, x_shape) %&amp;gt;%
    mutate_if(is.character, as.numeric)
    
# Y-values
# selecting rows 2-143 
# turning all values from character to numeric
datasaurus_wide_y &amp;lt;- datasaurus_wide %&amp;gt;%
    slice(2:143) %&amp;gt;%
    select(away_1, bullseye_1, circle_1, dino_1, dots_1, h_lines_1, high_lines_1, slant_down_1, slant_up_1, star_1, v_lines_1, wide_lines_1, x_shape_1) %&amp;gt;%
    mutate_if(is.character, as.numeric)


# correlation matrix for X values
corr_x &amp;lt;- round(cor(datasaurus_wide_x), 1)

# correlation matrix for Y values
corr_y &amp;lt;- round(cor(datasaurus_wide_y), 1)

head(corr_x[, 1:6])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          away bullseye circle dino dots h_lines
## away      1.0     -0.3   -0.3 -0.3 -0.3    -0.3
## bullseye -0.3      1.0    0.9  0.9  0.9     0.9
## circle   -0.3      0.9    1.0  0.9  0.8     0.9
## dino     -0.3      0.9    0.9  1.0  0.9     1.0
## dots     -0.3      0.9    0.8  0.9  1.0     0.9
## h_lines  -0.3      0.9    0.9  1.0  0.9     1.0&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;visualizing-the-correlation-matrix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualizing the correlation matrix&lt;/h3&gt;
&lt;p&gt;Here is a correlation between the &lt;code&gt;x-values&lt;/code&gt; between all 13 variables. You can see that all variables, aside from &lt;code&gt;away&lt;/code&gt;, are highly correlated with each other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# correlation between X-values
ggcorrplot(corr_x, hc.order = TRUE, 
           type=&amp;quot;lower&amp;quot;, 
           outline.color = &amp;quot;white&amp;quot;,
           ggtheme = ggplot2::theme_gray,
           colors = c(&amp;quot;#d8b365&amp;quot;, &amp;quot;#f5f5f5&amp;quot;, &amp;quot;#5ab4ac&amp;quot;),
           lab = TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/corr_x_viz-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is a correlation between the ‘y-values’ between all 13 variables. Again, aside from &lt;code&gt;away&lt;/code&gt;, all the variables are highly correlated with each other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# correlation between Y-values
ggcorrplot(corr_y, hc.order = TRUE, 
           type=&amp;quot;lower&amp;quot;, 
           outline.color = &amp;quot;white&amp;quot;,
           ggtheme = ggplot2::theme_gray,
           colors = c(&amp;quot;#ef8a62&amp;quot;, &amp;quot;#f7f7f7&amp;quot;, &amp;quot;#67a9cf&amp;quot;),
           lab = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/corr_y_viz-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;facets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Facets&lt;/h2&gt;
&lt;p&gt;At this point, the &lt;strong&gt;boxplots&lt;/strong&gt; show us variables with &lt;em&gt;similar median&lt;/em&gt; and &lt;em&gt;identical mean&lt;/em&gt;; the &lt;strong&gt;ridgelines&lt;/strong&gt; begin to show us that some variables have different distributions. And the &lt;strong&gt;correlation matrix&lt;/strong&gt; suggests the variables are more similar than not.&lt;/p&gt;
&lt;p&gt;To really see their differences, we’ll need to use &lt;code&gt;facet_wrap&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here we’ll use &lt;code&gt;facet_wrap&lt;/code&gt; to examine the histogram for &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values of all 13 variables. We started to see the differences in distribution between variables from the &lt;code&gt;ridgeline&lt;/code&gt; plots, but overlapping histograms provide another perspective.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# facet histogram (both-values)
datasaurus_dozen %&amp;gt;%
    group_by(dataset) %&amp;gt;%
    ggplot() +
    geom_histogram(aes(x=x, fill=&amp;#39;red&amp;#39;), alpha = 0.5, bins = 30) +
    geom_histogram(aes(x=y, fill=&amp;#39;green&amp;#39;), alpha = 0.5, bins = 30) +
    facet_wrap(~dataset) +
    scale_fill_discrete(labels = c(&amp;#39;y&amp;#39;, &amp;#39;x&amp;#39;)) +
    theme_classic() +
    labs(
        fill = &amp;#39;Axes&amp;#39;,
        x = &amp;#39;&amp;#39;,
        y = &amp;#39;Count&amp;#39;,
        title = &amp;#39;Faceted Histogram: x- and y-values&amp;#39;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/facet_histo-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scatter-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scatter Plot&lt;/h2&gt;
&lt;p&gt;However, if there’s one thing this dataset is trying to communicate its that there’s no subtitute for plotting the actual data points. No amount of summary statistics, central tendency or distribution is going to replace &lt;strong&gt;plotting actually data points&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Once we create the scatter plot with &lt;code&gt;geom_point&lt;/code&gt;, we see the big reveal with this dataset. That despite the similarities in central measures, for the most part similar distributions and high correlations, the 13 variables are &lt;strong&gt;wildly different&lt;/strong&gt; from each other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasaurus_dozen %&amp;gt;%
    group_by(dataset) %&amp;gt;%
    ggplot(aes(x=x, y=y, color=dataset)) +
    geom_point(alpha = 0.5) +
    facet_wrap(~dataset) +
    scale_color_manual(values = c(&amp;#39;#a6cee3&amp;#39;, &amp;#39;#1f78b4&amp;#39;, &amp;#39;#b2df8a&amp;#39;, &amp;#39;#33a02c&amp;#39;, &amp;#39;#fb9a99&amp;#39;, &amp;#39;#e31a1c&amp;#39;, &amp;#39;#fdbf6f&amp;#39;, &amp;#39;#ff7f00&amp;#39;, &amp;#39;#cab2d6&amp;#39;, &amp;#39;#6a3d9a&amp;#39;, &amp;#39;#ffff99&amp;#39;, &amp;#39;#b15928&amp;#39;, &amp;#39;grey&amp;#39;)) +
    theme_classic() +
    theme(legend.position = &amp;quot;none&amp;quot;) +
    labs(
        x = &amp;#39;X-axis&amp;#39;,
        y = &amp;#39;Y-axis&amp;#39;,
        title = &amp;#39;Faceted Scatter Plot&amp;#39;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/facet_scatter-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are other less common alternatives to the &lt;strong&gt;scatter plot&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;geom-density-2d&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geom Density 2D&lt;/h2&gt;
&lt;p&gt;While not as clear as the &lt;strong&gt;scatter plot&lt;/strong&gt;, plotting the &lt;strong&gt;contours&lt;/strong&gt; of a 2D density estimate does show how very different the variables are from each other, despite similar summary statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# contours of a 2D Density estimate
datasaurus_dozen %&amp;gt;%
    ggplot(aes(x=x, y=y)) +
    geom_density_2d() +
    theme_classic() +
    facet_wrap(~dataset) +
    labs(
        x = &amp;#39;X-axis&amp;#39;,
        y = &amp;#39;Y-axis&amp;#39;,
        title = &amp;#39;Contours of a 2D density estimate&amp;#39;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/geom_density_contour-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a slight variation using &lt;code&gt;stat_density_2d&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# stat density 2d
datasaurus_dozen %&amp;gt;%
    ggplot(aes(x=x, y=y)) +
    stat_density_2d(aes(fill=y), geom = &amp;quot;polygon&amp;quot;, colour = &amp;#39;white&amp;#39;) +
    theme_classic() +
    facet_wrap(~dataset) +
    labs(
        x = &amp;#39;X-axis&amp;#39;,
        y = &amp;#39;Y-axis&amp;#39;,
        title = &amp;#39;Stat Density 2D estimate&amp;#39;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-11-16-datasaurus_files/figure-html/stat_density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;density_2d&lt;/code&gt; plots are quite effective in showing how different the variables are and serve as a nice alternative to the more familiar scatter plot.&lt;/p&gt;
&lt;p&gt;Hopefully this vignette illustrates the importance of never trusting summary statistics (alone). Moreover, when visualizing, we should go beyond simply visualizing the data’s distribution or central tendency, but plotting the actually data points.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
