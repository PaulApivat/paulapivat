<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ggplot2 | Paul Apivat</title>
    <link>/tag/ggplot2/</link>
      <atom:link href="/tag/ggplot2/index.xml" rel="self" type="application/rss+xml" />
    <description>ggplot2</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Paul Apivat Hanvongse. All Rights Reserved.</copyright><lastBuildDate>Thu, 18 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>ggplot2</title>
      <link>/tag/ggplot2/</link>
    </image>
    
    <item>
      <title>Pad Thai is a Terrible Choice</title>
      <link>/post/thai_dishes_project/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/post/thai_dishes_project/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#web_scraping&#34;&gt;Web Scraping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#data_cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#data_visualization&#34;&gt;Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#text_mining&#34;&gt;Text Mining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#exploratory_questions&#34;&gt;Exploratory Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;People need to know they have other choices aside from Pad Thai. In fact Pad Thai is one of 53 individual dishes and there are at least 201 shared dishes in Thai cuisine (source: 
&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Thai_dishes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wikipedia&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This project is an opportunity to build a dataset of Thai dishes by scrapping tables off Wikipedia. We will use Python for web scrapping and R for visualization. Web scrapping is done in &lt;code&gt;Beautiful Soup&lt;/code&gt; (Python) and pre-processed further with &lt;code&gt;dplyr&lt;/code&gt; and visualized with &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Aside from furthering our data skills, we also make an open source 
&lt;a href=&#34;https://github.com/holtzy/R-graph-gallery/pull/34&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;contribution&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The project repo is 
&lt;a href=&#34;https://github.com/PaulApivat/thai_dishes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;web_scraping&#34;&gt;Web_Scraping&lt;/h3&gt;
&lt;h4 id=&#34;first-round&#34;&gt;First Round&lt;/h4&gt;
&lt;p&gt;We scraped over 
&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Thai_dishes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;300 Thai dishes&lt;/a&gt;. For each dish, we got:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thai name&lt;/li&gt;
&lt;li&gt;Thai script&lt;/li&gt;
&lt;li&gt;English name&lt;/li&gt;
&lt;li&gt;Region&lt;/li&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, we&amp;rsquo;ll use the following libraries/modules:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests
from bs4 import BeautifulSoup
import urllib.request
import urllib.parse
import urllib.error
import ssl
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll use &lt;code&gt;requests&lt;/code&gt; to send an HTTP requests to the wikipedia url we need. We&amp;rsquo;ll access network sockets using &amp;lsquo;secure sockets layer&amp;rsquo; (SSL). Then we&amp;rsquo;ll read in the html data to parse it with &lt;strong&gt;Beautiful Soup&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To being using &lt;strong&gt;Beautiful Soup&lt;/strong&gt;, we want to understand the structure of the page (and tables) we want to scrape. We can see that we want the &lt;code&gt;table&lt;/code&gt; tag, along with class of &lt;code&gt;wikitable sortable&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./web_scrap.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The main function we&amp;rsquo;ll use from &lt;strong&gt;Beautiful Soup&lt;/strong&gt; is &lt;code&gt;findAll()&lt;/code&gt; and the main three parameters are &lt;code&gt;th&lt;/code&gt; (Header Cell in HTML table), &lt;code&gt;tr&lt;/code&gt; (Row in HTML table) and &lt;code&gt;td&lt;/code&gt; (Standard Data Cell).&lt;/p&gt;
&lt;p&gt;First, we&amp;rsquo;ll save the table headers in a list, which we&amp;rsquo;ll use when creating an empty &lt;code&gt;dictionary&lt;/code&gt; to store the data we need.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;header = [item.text.rstrip() for item in all_tables[0].findAll(&#39;th&#39;)]

table = dict([(x, 0) for x in header])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initially, we want to successfully scrape one table, knowing that we&amp;rsquo;ll need to repeat the process for all 16 tables. Therefore we&amp;rsquo;ll use a &lt;em&gt;nested loop&lt;/em&gt;. Because all tables have 6 columns, we&amp;rsquo;ll want to create 6 empty lists.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll scrape through all table rows &lt;code&gt;tr&lt;/code&gt; and check for 6 cells (which we should have for 6 columns), then we&amp;rsquo;ll &lt;em&gt;append&lt;/em&gt; the data to each empty list we create.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# loop through all 16 tables
a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

# empty list to store data
a1 = []
a2 = []
a3 = []
a4 = []
a5 = []
a6 = []

# nested loop for looping through all 16 tables, then all tables individually
for i in a:
    for row in all_tables[i].findAll(&#39;tr&#39;):
        cells = row.findAll(&#39;td&#39;)
        if len(cells) == 6:
            a1.append([string for string in cells[0].strings])
            a2.append(cells[1].find(text=True))
            a3.append(cells[2].find(text=True))
            a4.append(cells[3].find(text=True))
            a5.append(cells[4].find(text=True))
            a6.append([string for string in cells[5].strings])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You&amp;rsquo;ll note the code for &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a6&lt;/code&gt; are slightly different. In retrospect, I found that &lt;code&gt;cells[0].find(text=True)&lt;/code&gt; did &lt;strong&gt;not&lt;/strong&gt; yield certain texts, particularly if they were links, therefore a slight adjustment is made.&lt;/p&gt;
&lt;p&gt;The strings tag returns a &lt;code&gt;NavigableString&lt;/code&gt; type object while text returns a &lt;code&gt;unicode&lt;/code&gt; object (see 
&lt;a href=&#34;https://stackoverflow.com/questions/25327693/difference-between-string-and-text-beautifulsoup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stack overflow&lt;/a&gt; explanation).&lt;/p&gt;
&lt;p&gt;After we&amp;rsquo;ve scrapped the data, we&amp;rsquo;ll need to store the data in a &lt;code&gt;dictionary&lt;/code&gt; before converting to &lt;code&gt;data frame&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create dictionary
table = dict([(x, 0) for x in header])

# append dictionary with corresponding data list
table[&#39;Thai name&#39;] = a1
table[&#39;Thai script&#39;] = a2
table[&#39;English name&#39;] = a3
table[&#39;Image&#39;] = a4
table[&#39;Region&#39;] = a5
table[&#39;Description&#39;] = a6

# turn dict into dataframe
df_table = pd.DataFrame(table)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;code&gt;a1&lt;/code&gt; and &lt;code&gt;a6&lt;/code&gt;, we need to do an extra step of joining the strings together, so I&amp;rsquo;ve created two additional corresponding columns, &lt;code&gt;Thai name 2&lt;/code&gt; and &lt;code&gt;Description2&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Need to Flatten Two Columns: &#39;Thai name&#39; and &#39;Description&#39;
# Create two new columns
df_table[&#39;Thai name 2&#39;] = &amp;quot;&amp;quot;
df_table[&#39;Description2&#39;] = &amp;quot;&amp;quot;

# join all words in the list for each of 328 rows and set to thai_dishes[&#39;Description2&#39;] column
# automatically flatten the list
df_table[&#39;Description2&#39;] = [
    &#39; &#39;.join(cell) for cell in df_table[&#39;Description&#39;]]

df_table[&#39;Thai name 2&#39;] = [
    &#39; &#39;.join(cell) for cell in df_table[&#39;Thai name&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Exploring and wrangling data revealed issues in the web scraping process&lt;/p&gt;
&lt;p&gt;Second pass to rectify all issues&lt;/p&gt;
&lt;h3 id=&#34;data_cleaning&#34;&gt;Data_Cleaning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Changing column names (snake case)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remove newline escape sequence (\n)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add/Mutate new columns:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;major_groupings (individual, shared, savory, sweet, drinks)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;minor_groupings (rice, noodles, curries, soups, salads, grilled etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit rows for missing data in Thai_name column: 26, 110, 157, 234-238, 240, 241, 246&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;save to &amp;ldquo;edit_thai_dishes.csv&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lead to second-pass web scrappings&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data_visualization&#34;&gt;Data_Visualization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dividing into Individual vs Shared Dishes&lt;/li&gt;
&lt;li&gt;Dendrogram 1: Individual Dishes&lt;/li&gt;
&lt;li&gt;Dendrogram 2: Shared Dishes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;text_mining&#34;&gt;Text_Mining&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Word Frequency&lt;/li&gt;
&lt;li&gt;Term Document Inverse Document Frequency&lt;/li&gt;
&lt;li&gt;Network of Relationships&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exploratory_questions&#34;&gt;Exploratory_Questions&lt;/h3&gt;
&lt;p&gt;Here are some exploratory questions generated during this project:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How might we organized Thai dishes?&lt;/li&gt;
&lt;li&gt;What is the best way to organized the different dishes?&lt;/li&gt;
&lt;li&gt;Which raw material(s) are most popular?&lt;/li&gt;
&lt;li&gt;Which raw materials are most important?&lt;/li&gt;
&lt;li&gt;Could you learn about Thai food just from the names of the dishes?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The More a Network is Worth, the Harder it is to Attack</title>
      <link>/post/crypto_price_attack/</link>
      <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/post/crypto_price_attack/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#implication&#34;&gt;Implication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, I use Python and R to access, parse, manipulate, then visualize data from 
&lt;a href=&#34;https://www.crypto51.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Crypto51.app&lt;/a&gt; to show the strong relationship between Market Capitalization and Cost to Attack among public crypto networks.&lt;/p&gt;
&lt;p&gt;The more a network is thought to be worth, the more expensive it is to attack. An important, but often overlooked reason to celebrate price gains.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;In this post, I query an API endpoint setup at 
&lt;a href=&#34;https://www.crypto51.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Crypto51.app&lt;/a&gt; to get &lt;code&gt;JSON&lt;/code&gt; data. Then, I use Python to parse and convert to &lt;code&gt;dataframe&lt;/code&gt;. Finally, I use R to wrangle and visualize.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get it!&lt;/p&gt;
&lt;p&gt;Here is the Python code to read in &lt;code&gt;JSON&lt;/code&gt; and convert to a &lt;code&gt;dataframe&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import json
import requests

r = requests.get(&#39;https://api.crypto51.app/coins.json&#39;)
dct = dict()
dct = r.json()

# loop through:
# last_updated
# coins
for x, y in dct.items():
    print(x)

type(dct[&#39;coins&#39;]) # list
len(dct[&#39;coins&#39;])  # 57 dictionaries in side this list

# convert list of 57 dictionaries into a pandas dataframe
df = pd.DataFrame.from_dict(dct[&#39;coins&#39;])
df.head()
df.to_csv(&#39;crypto51.csv&#39;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After creating a CSV, I&amp;rsquo;m transition to R, out of preference for &lt;code&gt;dataframe&lt;/code&gt; manipulation and visualization with this tool (you could do the following in &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;seaborn&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll load the &lt;code&gt;tidyverse&lt;/code&gt; and read in the CSV file we created. Then we&amp;rsquo;ll use a series of &lt;code&gt;magrittr&lt;/code&gt; 
&lt;a href=&#34;https://magrittr.tidyverse.org/reference/pipe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pipes&lt;/a&gt; to sequence our data manipulation in one flow. We&amp;rsquo;ll remove projects with &lt;strong&gt;no market_cap&lt;/strong&gt; data. We&amp;rsquo;ll remove the Handshake project because of missing data for &lt;code&gt;attack_hourly_cost&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll change &lt;code&gt;attack_hourly_cost&lt;/code&gt; data type into numeric. Then we&amp;rsquo;ll use &lt;code&gt;ggplot2&lt;/code&gt; to visualize a scatter plot with both X and Y axes transformed with &lt;code&gt;scale_*_log10()&lt;/code&gt; to make the scatter plot more interpretable.&lt;/p&gt;
&lt;p&gt;Bitcoin and Ethereum are annotated as the two leading projects (see chart below).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;library(tidyverse)

df &amp;lt;- read_csv(&amp;quot;crypto51.csv&amp;quot;)

df %&amp;gt;%
    # remove projects with no market_cap
    slice(1:38) %&amp;gt;% 
    filter(attack_hourly_cost != &amp;quot;?&amp;quot;) %&amp;gt;% 
    # change character to numeric
    mutate(
        attack_hourly_cost = as.numeric(attack_hourly_cost)
    ) %&amp;gt;% 
    ggplot(aes(x=market_cap, y=attack_hourly_cost)) +
    geom_point(aes(size = log10(market_cap)), color = &amp;quot;white&amp;quot;, alpha = 0.8) +
    # use log10 transformation to make chart more interpretable
    scale_y_log10(label= scales::dollar) +
    scale_x_log10(label= scales::dollar) +
    theme_minimal() +
    theme(
        legend.position = &#39;none&#39;,
        panel.background = element_rect(fill = &amp;quot;dodger blue&amp;quot;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_rect(fill = &amp;quot;dodger blue&amp;quot;),
        plot.title = element_text(colour = &amp;quot;white&amp;quot;, face = &amp;quot;bold&amp;quot;, size = 30, 
                                  margin = margin(10,0,30,0)),
        plot.caption = element_text(color = &amp;quot;white&amp;quot;),
        axis.title = element_text(colour = &amp;quot;white&amp;quot;, face = &amp;quot;bold&amp;quot;),
        axis.title.x = element_text(margin = margin(30,0,10,0)),
        axis.text = element_text(colour = &amp;quot;white&amp;quot;, face = &amp;quot;bold&amp;quot;),
        axis.title.y = element_text(margin = margin(0,20,0,30), angle = 0)
    ) +
    labs(
        x = &amp;quot;Market Capitalization&amp;quot;,
        y = &amp;quot;Attack\nHourly\nCost&amp;quot;,
        title = &amp;quot;The More a Crypto Network is Worth,\n the Harder it is to Attack.&amp;quot;,
        caption = &amp;quot;Data: www.crypto51.app | Graphics: @paulapivat&amp;quot;
    ) +
    # annotate instead of geom_text
    annotate(&amp;quot;text&amp;quot;, x = 205174310335, y = 800000, label = &amp;quot;Bitcoin&amp;quot;, color = &amp;quot;white&amp;quot;) +
    annotate(&amp;quot;text&amp;quot;, x = 30762751140, y = 418437, label = &amp;quot;Ethereum&amp;quot;, color = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;implication&#34;&gt;Implication&lt;/h2&gt;
&lt;h3 id=&#34;the-more-a-crypto-network-is-worth-the-harder-it-is-to-attack&#34;&gt;The More a Crypto Network is Worth, the Harder it is to Attack&lt;/h3&gt;
&lt;p&gt;All time high.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;d be hard pressed to find three more delicious words than these.&lt;/p&gt;
&lt;p&gt;When it comes to crypto, everyone keeps an eye on their portfolio value.&lt;/p&gt;
&lt;p&gt;Your bags aside, there is &lt;em&gt;another&lt;/em&gt; reason to celebrate price gains.&lt;/p&gt;
&lt;p&gt;Bitcoin&amp;rsquo;s big innovation was making &lt;strong&gt;digital transaction difficult to replicate&lt;/strong&gt; (unlike most digital files that are easily duplicated).&lt;/p&gt;
&lt;p&gt;Nodes follow the longest chain as the &amp;ldquo;correct&amp;rdquo; chain. However, this opens things up for any node(s) with more than 51% of the network hashing power to pull &lt;em&gt;shenanigans&lt;/em&gt;, such as &lt;strong&gt;double-spending&lt;/strong&gt;. Sending funds to one address on the main chain and the same funds to another address on a different chain.&lt;/p&gt;
&lt;p&gt;More hardware and hash power allow a node to secretly mine a side chain, which they can later âfoolâ the rest of the network into accepting.&lt;/p&gt;
&lt;p&gt;Since their inception, Bitcoin and Ethereum have gotten more difficult to mine over time. And when price increases, the capital costs of buying new equipment goes up.&lt;/p&gt;
&lt;p&gt;This makes it more difficult for any one entity to accumulate too much hash power and pull shenanigans. &lt;strong&gt;As a result, the entire network is more secure&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In fact, the data provided by crypto51.app suggests a near perfect correlation between Market Capitalization and Cost to Attack, at r = 0.94.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./price_nb3.png&#34; alt=&#34;Crypto Price Attack&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The more a crypto network is worth, the more expensive it is to attack.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another reason to celebrate price gains.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.crypto51.app&#34;&gt;www.crypto51.app&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
