<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Paul Apivat</title>
    <link>/tag/r/</link>
      <atom:link href="/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Paul Apivat Hanvongse. All Rights Reserved.</copyright><lastBuildDate>Tue, 26 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>/tag/r/</link>
    </image>
    
    <item>
      <title>How Positive are Your Facebook Posts?</title>
      <link>/post/sentiment_analysis/</link>
      <pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/sentiment_analysis/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#getting_data&#34;&gt;Getting Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#tokenization&#34;&gt;Tokenization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#normalizing_sentences&#34;&gt;Normalizing Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#frequency&#34;&gt;Frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#sentiment_analysis&#34;&gt;Sentiment Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#data_transformation&#34;&gt;Data Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#visualization&#34;&gt;Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h4 id=&#34;why-sentiment-analysis&#34;&gt;Why Sentiment Analysis?&lt;/h4&gt;
&lt;p&gt;NLP is subfield of linguistic, computer science and artificial intelligence (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_language_processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wiki&lt;/a&gt;), and you could spend years studying it.&lt;/p&gt;
&lt;p&gt;However, I wanted a quick mini-dive into this domain, to a get an intuition for how NLP works.&lt;/p&gt;
&lt;p&gt;What better dataset than our own social media post as we can&amp;rsquo;t help but feel motivated to see insights about our &lt;em&gt;own&lt;/em&gt; content.&lt;/p&gt;
&lt;h4 id=&#34;how-well-does-facebook-know-us&#34;&gt;How well does Facebook know us?&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;How much is too much?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To find out, I downloaded 14 years of posts to apply &lt;strong&gt;text analysis&lt;/strong&gt; and &lt;strong&gt;pre-processing&lt;/strong&gt;. I  used &lt;code&gt;Python&lt;/code&gt; to read and download &lt;code&gt;json&lt;/code&gt; data from Facebook.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll perform tasks such as tokenization, normalization, stemming and lemmatization aided by Python&amp;rsquo;s &lt;strong&gt;Natural Language Toolkit&lt;/strong&gt;, &lt;code&gt;NLTK&lt;/code&gt;. We can also get a word/sentence frequency count of all posts. Finally, we&amp;rsquo;ll use the &lt;code&gt;VADER&lt;/code&gt; module (Hutto &amp;amp; Gilbert, 2014) for rule-based (lexicon) model of &lt;strong&gt;sentiment analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll transition our work flow to &lt;code&gt;R&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; for &lt;strong&gt;data manipulation&lt;/strong&gt; and &lt;strong&gt;visualization&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;getting_data&#34;&gt;Getting_Data&lt;/h2&gt;
&lt;p&gt;First, you&amp;rsquo;ll need to download your own Facebook data by following: Setting &amp;amp; Privacy &amp;gt; Setting &amp;gt; Your Facebook Information &amp;gt; Download Your Information &amp;gt; (select) Posts.&lt;/p&gt;
&lt;p&gt;Below, I named my file &lt;code&gt;your_posts_1.json&lt;/code&gt;, but you can change this.
We&amp;rsquo;ll use Python&amp;rsquo;s &lt;code&gt;json&lt;/code&gt; module read in data. We can get a feel for the data with &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;len&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json

# load json into python, assign to &#39;data&#39;
with open(&#39;your_posts_1.json&#39;) as file:
    data = json.load(file)

type(data)     # a list
type(data[0])  # first object in the list: a dictionary
len(data)      # my list contains 2166 dictionaries
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the Python libraries we use in this post:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.stem import LancasterStemmer, WordNetLemmatizer      # OPTIONAL (more relevant for individual words)
from nltk.corpus import stopwords
from nltk.probability import FreqDist
import re
import unicodedata
import nltk
import json
import inflect
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nltk.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Language Tookkit&lt;/a&gt; is a popular Python platform for work with human language data. While it has over 50 lexical resourcesm, we&amp;rsquo;ll use the 
&lt;a href=&#34;https://github.com/cjhutto/vaderSentiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vader Sentiment Lexicon&lt;/a&gt;, that is &lt;em&gt;specifically&lt;/em&gt; attuned to sentiments expressed in social media. Ideal for Facebook data. NLTK will also help with frequency count, stopwards, stemming and lemmatization.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.python.org/3/library/re.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regex&lt;/a&gt; (regular expressions) will be used to remove punctuation.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.python.org/3/library/unicodedata.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unicode Database&lt;/a&gt; will be used to remove non-ASCII characters.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.python.org/3/library/json.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JSON&lt;/a&gt; module helps us to read in json from Facebook.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://pypi.org/project/inflect/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inflect&lt;/a&gt; helps us to convert numbers to words.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pandas&lt;/a&gt; is a powerful data manipulation and data analysis tool for when we save our text data into a data frame and write to csv.&lt;/p&gt;
&lt;p&gt;After we have our data (in &lt;code&gt;json&lt;/code&gt; format), we&amp;rsquo;ll 
&lt;a href=&#34;https://twitter.com/paulapivat/status/1352893979897909251?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dig through&lt;/a&gt; to get actual &lt;strong&gt;text data&lt;/strong&gt; (our posts).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll store this text in a list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the &lt;code&gt;data&lt;/code&gt; key occassionally returns an empty array and we want to skip over those by checking &lt;code&gt;len(v) &amp;gt; 0&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create empty list
empty_lst = []

# multiple nested loops to store all post in empty list
for dct in data:
    for k, v in dct.items():
        if k == &#39;data&#39;:
            if len(v) &amp;gt; 0:
                for k_i, v_i in vee[0].items():  
                    if k_i == &#39;post&#39;:
                        empty_lst.append(v_i)

print(&amp;quot;This is the empty list: &amp;quot;, empty_lst)
print(&amp;quot;\nLength of list: &amp;quot;, len(empty_lst))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a list of strings.&lt;/p&gt;
&lt;h2 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll loop through our list of strings (empty_lst) to tokenize each &lt;em&gt;sentence&lt;/em&gt; with &lt;code&gt;nltk.sent_tokenize()&lt;/code&gt;. We want to split the text into either individual words or sentences. I think it&amp;rsquo;ll make more sense to try to find the sentiment of each sentence, so we&amp;rsquo;ll tokenize by sentence.&lt;/p&gt;
&lt;p&gt;This yields a list of list, we&amp;rsquo;ll need to flatten it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# - list of list, len: 1762 (each list contain sentences)
nested_sent_token = [nltk.sent_tokenize(lst) for lst in empty_lst]

# flatten list, len: 3241
flat_sent_token = [item for sublist in nested_sent_token for item in sublist]
print(&amp;quot;Flatten sentence token: &amp;quot;, len(flat_sent_token))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;normalizing_sentences&#34;&gt;Normalizing_Sentences&lt;/h2&gt;
&lt;p&gt;For context on the functions used in this section, check out this article by Matthew Mayo on 
&lt;a href=&#34;https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Text Data Preprocessing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, we&amp;rsquo;ll remove non-ASCII characters (&lt;code&gt;remove_non_ascii(words)&lt;/code&gt;) including: &lt;code&gt;#&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;&#39;&lt;/code&gt; and &lt;code&gt;?&lt;/code&gt;, among many others. Then we&amp;rsquo;ll lowercase (&lt;code&gt;to_lowercase(words)&lt;/code&gt;), remove punctuation (&lt;code&gt;remove_punctuation(words)&lt;/code&gt;), replace numbers (&lt;code&gt;replace_numbers(words)&lt;/code&gt;), and remove stopwords (&lt;code&gt;remove_stopwords(words)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Example stopwords are: your, yours, yourself, yourselves, he, him, his, himself etc.&lt;/p&gt;
&lt;p&gt;This allows us to have each sentence be on equal playing field.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: It turns out the &lt;code&gt;Vader&lt;/code&gt; module for sentiment analysis is fully capable of analyzing sentences with punctuation, word-shape (capitalization for emphasis), slang and even utf-8 encoded emojis, so we&amp;rsquo;ll run a parallel set of analyses at the end &lt;strong&gt;without normalization&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Remove Non-ASCII
def remove_non_ascii(words):
    &amp;quot;&amp;quot;&amp;quot;Remove non-ASCII character from List of tokenized words&amp;quot;&amp;quot;&amp;quot;
    new_words = []
    for word in words:
        new_word = unicodedata.normalize(&#39;NFKD&#39;, word).encode(
            &#39;ascii&#39;, &#39;ignore&#39;).decode(&#39;utf-8&#39;, &#39;ignore&#39;)
        new_words.append(new_word)
    return new_words


# To LowerCase
def to_lowercase(words):
    &amp;quot;&amp;quot;&amp;quot;Convert all characters to lowercase from List of tokenized words&amp;quot;&amp;quot;&amp;quot;
    new_words = []
    for word in words:
        new_word = word.lower()
        new_words.append(new_word)
    return new_words


# Remove Punctuation , then Re-Plot Frequency Graph
def remove_punctuation(words):
    &amp;quot;&amp;quot;&amp;quot;Remove punctuation from list of tokenized words&amp;quot;&amp;quot;&amp;quot;
    new_words = []
    for word in words:
        new_word = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, word)
        if new_word != &#39;&#39;:
            new_words.append(new_word)
    return new_words


# Replace Numbers with Textual Representations
def replace_numbers(words):
    &amp;quot;&amp;quot;&amp;quot;Replace all interger occurrences in list of tokenized words with textual representation&amp;quot;&amp;quot;&amp;quot;
    p = inflect.engine()
    new_words = []
    for word in words:
        if word.isdigit():
            new_word = p.number_to_words(word)
            new_words.append(new_word)
        else:
            new_words.append(word)
    return new_words

# Remove Stopwords
def remove_stopwords(words):
    &amp;quot;&amp;quot;&amp;quot;Remove stop words from list of tokenized words&amp;quot;&amp;quot;&amp;quot;
    new_words = []
    for word in words:
        if word not in stopwords.words(&#39;english&#39;):
            new_words.append(word)
    return new_words
    
# Combine all functions into Normalize() function
def normalize(words):
    words = remove_non_ascii(words)
    words = to_lowercase(words)
    words = remove_punctuation(words)
    words = replace_numbers(words)
    words = remove_stopwords(words)
    return words
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we&amp;rsquo;re normalizing &lt;em&gt;sentences&lt;/em&gt;, rather than &lt;em&gt;words&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sents = normalize(flat_sent_token)
print(&amp;quot;Length of sentences list: &amp;quot;, len(sents))   # 3194
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The process of stemming and lemmatization makes more sense for individuals words (over sentences), so we won&amp;rsquo;t use them here.&lt;/p&gt;
&lt;h2 id=&#34;frequency&#34;&gt;Frequency&lt;/h2&gt;
&lt;p&gt;You can use the &lt;code&gt;FreqDist()&lt;/code&gt; function to get the most common sentences. Then, you could plot a line chart for a visual comparison of the most frequent sentences.&lt;/p&gt;
&lt;p&gt;Although simple, counting frequencies can yield some 
&lt;a href=&#34;https://twitter.com/paulapivat/status/1353704114467729408?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;insights&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from nltk.probability import FreqDist

# Find frequency of sentence
fdist_sent = FreqDist(sents)
fdist_sent.most_common(10)   

# Plot
fdist_sent.plot(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sentiment_analysis&#34;&gt;Sentiment_Analysis&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ll use the &lt;code&gt;Vader&lt;/code&gt; module from &lt;code&gt;NLTK&lt;/code&gt;. Vader stands for:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Valence, Aware, Dictionary and sEntiment Reasoner.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are taking a &lt;strong&gt;Rule-based/Lexicon&lt;/strong&gt; approach to sentiment analysis because we have a fairly large dataset, but lack labeled data to build a robust training set. Thus, Machine Learning would &lt;strong&gt;not&lt;/strong&gt; be ideal for this task.&lt;/p&gt;
&lt;p&gt;To get an intuition for how the &lt;code&gt;Vader&lt;/code&gt; module works, we can visit the github repo to view &lt;code&gt;vader_lexicon.txt&lt;/code&gt; 
&lt;a href=&#34;https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;. This is a &lt;strong&gt;dictionary&lt;/strong&gt; that has been empirically validated by multiple independent human judges. Sentiment ratings are provided by 10 independent human raters (pre-screened, trained and checked for inter-rater reliability).&lt;/p&gt;
&lt;p&gt;Scores range from (-4) Extremely Negative to (4) Extremely Positive, with (0) as Neutral. For example, &amp;ldquo;die&amp;rdquo; is rated -2.9, while &amp;ldquo;dignified&amp;rdquo; has a 2.2 rating. For more details visit their 
&lt;a href=&#34;https://github.com/cjhutto/vaderSentiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll create two empty lists to store the sentences and the polarity scores, separately.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sentiment&lt;/code&gt; captures each sentence and &lt;code&gt;sent_scores&lt;/code&gt;, which initializes the &lt;code&gt;nltk.sentiment.vader.SentimentIntensityAnalyzer&lt;/code&gt;, calculates the &lt;strong&gt;polarity_score&lt;/strong&gt; of each sentence (i.e., negative, neutral, positive).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sentiment2&lt;/code&gt; captures each polarity and value in a list of tuples.&lt;/p&gt;
&lt;p&gt;The below screen-cap should give you a sense of what we have:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./sentiment_2.png&#34; alt=&#34;sentiment_2&#34;&gt;&lt;/p&gt;
&lt;p&gt;After we have appended each sentence (&lt;code&gt;sentiment&lt;/code&gt;) and their polarity scores (&lt;code&gt;sentiment2&lt;/code&gt;, negative, neutral, positive), we&amp;rsquo;ll &lt;strong&gt;create data frames&lt;/strong&gt; to store these values.&lt;/p&gt;
&lt;p&gt;Then, we&amp;rsquo;ll write the data frames to &lt;strong&gt;CSV&lt;/strong&gt; to transition to &lt;code&gt;R&lt;/code&gt;. Note that we set index to false when saving for CSV. Python starts counting at 0, while &lt;code&gt;R&lt;/code&gt; starts at 1, so we&amp;rsquo;re better off re-creating the index as a separate column in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: There are more efficient ways for what I&amp;rsquo;m doing here. My solution is to save two CSV files and move the work flow over to &lt;code&gt;R&lt;/code&gt; for further data manipulation and visualization. This is primarily a personal preference for handling data frames and visualizations in &lt;code&gt;R&lt;/code&gt;, but I should point out this &lt;em&gt;can&lt;/em&gt; be done with &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# nltk.download(&#39;vader_lexicon&#39;)

sid = SentimentIntensityAnalyzer()

sentiment = []
sentiment2 = []

for sent in sents:
    sent1 = sent
    sent_scores = sid.polarity_scores(sent1)
    for x, y in sent_scores.items():
        sentiment2.append((x, y))
    sentiment.append((sent1, sent_scores))
    # print(sentiment)

# sentiment
cols = [&#39;sentence&#39;, &#39;numbers&#39;]
result = pd.DataFrame(sentiment, columns=cols)
print(&amp;quot;First five rows of results: &amp;quot;, result.head())

# sentiment2
cols2 = [&#39;label&#39;, &#39;values&#39;]
result2 = pd.DataFrame(sentiment2, columns=cols2)
print(&amp;quot;First five rows of results2: &amp;quot;, result2.head())

# save to CSV
result.to_csv(&#39;sent_sentiment.csv&#39;, index=False)
result2.to_csv(&#39;sent_sentiment_2.csv&#39;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data_transformation&#34;&gt;Data_Transformation&lt;/h2&gt;
&lt;p&gt;From this point forward, we&amp;rsquo;ll be using &lt;code&gt;R&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; for data manipulation and visualization. &lt;code&gt;RStudio&lt;/code&gt; is the IDE of choice here. We&amp;rsquo;ll create an &lt;code&gt;R Script&lt;/code&gt; to store all our data transformation and visualization process. We should be in the same directory in which the above CSV files were created with &lt;code&gt;pandas&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll load the two CSV files we saved and the &lt;code&gt;tidyverse&lt;/code&gt; library:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)

# load data
df &amp;lt;- read_csv(&amp;quot;sent_sentiment.csv&amp;quot;)       
df2 &amp;lt;- read_csv(&#39;sent_sentiment_2.csv&#39;)    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll create another column that matches the index for the first data frame (sent_sentiment.csv). I save it as &lt;code&gt;df1&lt;/code&gt;, but you could overwrite the original &lt;code&gt;df&lt;/code&gt; if you wanted.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create a unique identifier for each sentence
df1 &amp;lt;- df %&amp;gt;%
    mutate(row = row_number())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, for the second data frame (sent_sentiment_2.csv), we&amp;rsquo;ll create another column matching the index, but also use &lt;code&gt;pivot_wider&lt;/code&gt; from the &lt;code&gt;tidyr&lt;/code&gt; package. &lt;strong&gt;NOTE&lt;/strong&gt;: You&amp;rsquo;ll want to &lt;code&gt;group_by&lt;/code&gt; label first, then use &lt;code&gt;mutate&lt;/code&gt; to create a unique identifier.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll then use &lt;code&gt;pivot_wider&lt;/code&gt; to ensure that all polarity values (negative, neutral, positive) have their own columns.&lt;/p&gt;
&lt;p&gt;By creating a unique identifier using &lt;code&gt;mutate&lt;/code&gt; and &lt;code&gt;row_number()&lt;/code&gt;, we&amp;rsquo;ll be able to join (&lt;code&gt;left_join&lt;/code&gt;) by row.&lt;/p&gt;
&lt;p&gt;Finally, I save the operation to &lt;code&gt;df3&lt;/code&gt; which allows me to work off a fresh new data frame for visualization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# long-to-wide for df2
# note: first, group by label; then, create a unique identifier for each label then use pivot_wider

df3 &amp;lt;- df2 %&amp;gt;%
    group_by(label) %&amp;gt;%
    mutate(row = row_number()) %&amp;gt;%
    pivot_wider(names_from = label, values_from = values) %&amp;gt;%
    left_join(df1, by = &#39;row&#39;) %&amp;gt;%
    select(row, sentence, neg:compound, numbers) 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;visualization&#34;&gt;Visualization&lt;/h2&gt;
&lt;p&gt;First, we&amp;rsquo;ll visualize the positive and negative polarity scores separately, across all 3194 sentences (your numbers will vary).&lt;/p&gt;
&lt;p&gt;Here are positivity scores:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./positivity_line.png&#34; alt=&#34;positivity_line&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here are negativity scores:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./negativity_line.png&#34; alt=&#34;negativity_line&#34;&gt;&lt;/p&gt;
&lt;p&gt;When I sum positivity and negativity scores to get a ratio, it&amp;rsquo;s approximately 568:97 or  5.8x more positive than negative according to the &lt;code&gt;Vader&lt;/code&gt; (Valance Aware Dictionary and Sentiment Reasoner).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Vader&lt;/code&gt; module will take in every sentence and assign a valence score from -1 (most negative) to 1 (most positive). We can classify sentences as &lt;code&gt;pos&lt;/code&gt; (positive), &lt;code&gt;neu&lt;/code&gt; (neutral) and &lt;code&gt;neg&lt;/code&gt;(negative) or as a composite (&lt;code&gt;compound&lt;/code&gt;) score (i.e., normalized, weighted composite score). For more details, see 
&lt;a href=&#34;https://pypi.org/project/vader-sentiment/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vader-sentiment documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To see both positive and negative scores together (positive = blue, negative = red, neutral = black).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./sentiment2.png&#34; alt=&#34;sentiment2.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, we can also use &lt;code&gt;histograms&lt;/code&gt; to see the distribution of negative and positive sentiment among the sentences:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./patch_histo.png&#34; alt=&#34;patch_histo&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;
&lt;p&gt;I downloaded 14 years worth of Facebook posts to run a rule-based sentiment analysis and visualize the results, using a combination of &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I enjoyed using both for this project and sought to play to their strengths. I found parsing JSON straight-forward with Python, but once we transition to data frames, I was itching to get back to R.&lt;/p&gt;
&lt;p&gt;Because we lacked labeled data, using a rule-based/lexicon-approach to sentiment analysis made sense. Now that we have a label for valence scores, it may be possible to take a machine learning approach to predict the valence of future posts.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Hutto, C.J. &amp;amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Your Twitter Data</title>
      <link>/post/twitter_analytics/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/twitter_analytics/</guid>
      <description>&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#exploring_relationships&#34;&gt;Exploring Relationships&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview--setup&#34;&gt;Overview &amp;amp; Setup&lt;/h2&gt;
&lt;p&gt;This post uses various R libraries and functions to help you explore your Twitter Analytics Data. The first thing to do is download data from 
&lt;a href=&#34;https://analytics.twitter.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analytics.twitter.com&lt;/a&gt;. The assumption here is that you&amp;rsquo;re already a Twitter user and have been using for at least 6 months.&lt;/p&gt;
&lt;p&gt;Once there, you&amp;rsquo;ll click on the &lt;code&gt;Tweets&lt;/code&gt; tab, which should bring you to your Tweet activity with the option to &lt;strong&gt;Export data&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./twitter_analytics.png&#34; alt=&#34;twitter_analytics&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once you click on &lt;strong&gt;Export data&lt;/strong&gt;, you&amp;rsquo;ll choose &amp;ldquo;By day&amp;rdquo;, which provides your impressions and engagements metrics for everyday (you&amp;rsquo;ll also select the time period, in the drop down menu right next to Export data - the default is &amp;ldquo;Last 28 Days&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The other option is to choose &amp;ldquo;By Tweet&amp;rdquo; and that will download the text of each Tweet along with associated metrics. We could potentially do fun text analysis with this, but we&amp;rsquo;ll save that for another post.&lt;/p&gt;
&lt;p&gt;For this post, I downloaded all &lt;em&gt;available&lt;/em&gt; data, which goes five months back.&lt;/p&gt;
&lt;p&gt;After downloading, you&amp;rsquo;ll want to &lt;strong&gt;read&lt;/strong&gt; in the data and, in our case, &lt;strong&gt;combine&lt;/strong&gt; all five months into one data frame, we&amp;rsquo;ll use the &lt;code&gt;readr&lt;/code&gt; package and &lt;code&gt;read_csv()&lt;/code&gt; function contained in &lt;code&gt;tidyverse&lt;/code&gt;. Then we&amp;rsquo;ll use &lt;code&gt;rbind()&lt;/code&gt; to combine five data frames by rows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)

# load data from September to mid-January
df1 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20200901_20201001_en.csv&amp;quot;)
df2 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201001_20201101_en.csv&amp;quot;)
df3 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201101_20201201_en.csv&amp;quot;)
df4 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20201201_20210101_en.csv&amp;quot;)
df5 &amp;lt;- read_csv(&amp;quot;./daily_tweet_activity/daily_tweet_activity_metrics_paulapivat_20210101_20210112_en.csv&amp;quot;)

# combining ALL five dataframes into ONE, by rows
df &amp;lt;- rbind(df1, df2, df3, df4, df5)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;exploring_relationships&#34;&gt;Exploring_Relationships&lt;/h2&gt;
&lt;p&gt;Twitter analytics tracks several metric that are broadly grouped under Engagements, including: retweets, replies, likes, user profile clicks, url clicks, hashtag clicks, detail expands, media views and media engagements.&lt;/p&gt;
&lt;p&gt;There are other metrics like &amp;ldquo;app opens&amp;rdquo; and &amp;ldquo;promoted engagements&amp;rdquo;, which are services I have not used and so do not have any data available.&lt;/p&gt;
&lt;h4 id=&#34;a-guiding-question&#34;&gt;A Guiding Question&lt;/h4&gt;
&lt;p&gt;It&amp;rsquo;s useful to have a guiding question as it helps focus your exploration. Let&amp;rsquo;s say, I was interested in whether one of my tweets prompted a reader to click on my profile. The metric for this is &lt;code&gt;user profile clicks&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;My initial guiding question for this post is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Which metrics are most strongly correlated with User Profile Clicks?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You could simply use the &lt;code&gt;cor.test()&lt;/code&gt; function, which comes with base R, to go one by one between &lt;em&gt;each&lt;/em&gt; metric and &lt;code&gt;User Profile Click&lt;/code&gt;. For example, below we calculate the correlation between three pairs of variables, &lt;code&gt;User Profile Clicks&lt;/code&gt; and &lt;code&gt;retweets&lt;/code&gt;, &lt;code&gt;replies&lt;/code&gt; and &lt;code&gt;likes&lt;/code&gt;, separately. After awhile, this can get tedious.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cor.test(x = df$`user profile clicks`, y = df$retweets)
cor.test(x = df$`user profile clicks`, y = df$replies)
cor.test(x = df$`user profile clicks`, y = df$likes)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quicker way to explore the relationship between pairs of metrics throughout a dataset is to use a &lt;strong&gt;correlelogram&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll start with base R. You&amp;rsquo;ll want to limit the number of variables you visualize so the correlelogram doesn&amp;rsquo;t become too cluttered. Here are four variables that correlate the highest with &lt;code&gt;User Profile Clicks&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# four columns are selected along with user profile clicks to plot
df %&amp;gt;%
    select(8, 12, 19:20, `user profile clicks`) %&amp;gt;%
    plot(pch = 20, cex = 1.5, col=&amp;quot;#69b3a2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s a visual:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./plot_strongest.png&#34; alt=&#34;plot_strongest&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here are another four metrics with &lt;em&gt;moderate&lt;/em&gt; relationships:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df %&amp;gt;%
    select(6:7, 10:11, `user profile clicks`) %&amp;gt;%
    plot(pch = 20, cex = 1.5, col=&amp;quot;#69b3a2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./plot_moderate.png&#34; alt=&#34;plot_moderate&#34;&gt;&lt;/p&gt;
&lt;p&gt;Visually, you can see the moderate relationship scatter plots are more dispersed, with a less identifiable direction.&lt;/p&gt;
&lt;p&gt;While base R is dependable, we can get more informative plots with the &lt;code&gt;GGally&lt;/code&gt; package. Here are the four highly correlated variables with &lt;code&gt;User Profile Clicks&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(GGally)

# GGally, Strongest Related
df %&amp;gt;%
    select(8, 12, 19:20, `user profile clicks`) %&amp;gt;%
    ggpairs(
        diag = NULL,
        title = &amp;quot;Strongest Relationships with User Profile Clicks: Sep 2020 - Jan 2021&amp;quot;,
        axisLabels = c(&amp;quot;internal&amp;quot;),
        xlab = &amp;quot;Value&amp;quot;
    )

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s the correlelogram between the four most highly correlated variables with &lt;code&gt;user profile clicks&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./strongest.png&#34; alt=&#34;strongest&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here are the moderately correlated variables with &lt;code&gt;User Profile Clicks&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./moderate.png&#34; alt=&#34;moderate&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see, not only do these provide scatter plots, but they also show the numerical values of the correlation between each pair of variables, which is much more informative than base R.&lt;/p&gt;
&lt;p&gt;Now, its entirely possible that the pattern of correlation in your data is different as the initial patterns we&amp;rsquo;re seeing here are not meant to generalize to a different dataset.&lt;/p&gt;
&lt;p&gt;For more content on data science, machine learning, R, Python, SQL and more, 
&lt;a href=&#34;https://twitter.com/paulapivat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;find me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
